# LeaPR: Learned Programmatic Representation models

**This is a fork of the original [LeaPR repository](https://github.com/neurosymbolic-learning/leapr) with extensions.**

This repository contains the implementation corresponding to the following paper:

[Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)

LeaPR models are neural network-free machine learning models: they combine *feature functions* represented as (LLM-synthesized) Python functions with *decision tree* predictors learned on top of those features. These models can be trained for any supervised learning task.

## Key Additions in This Fork

- **Text Regression Domain**: New domain for regression tasks on text data, with support for training on the [Anthropic HH-RLHF dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf) for reward model debugging experiments
- **Pairwise Text Classification Domain**: Support for pairwise comparison tasks
- **SHAP Analysis**: Added `--interpret` flag and `interpret.py` script for computing SHAP values to analyze feature importance in trained models
- **Evaluation Scripts**: New scripts in `rm_debugger/` for evaluating models on the HH-RLHF test set and comparing against neural reward models

![Overview of Learned Programmatic Representation Models: training can take any supervised learning dataset and output a set of features implemented as Python functions.](img/fig1.png)

LeaPR models can contain hundreds or thousands of automatically generated features, modularly combined by decision trees. Because features are LLM-generated code, they tend to be highly readable and interpretable, and tools such as SHAP values, used to interpret and explain decisions from classical ML models, can be easily applied to LeaPR models.

Here is an example of a feature learned for the *text classification* task of classifying human vs ChatGPT-generated text (which we evaluated using the [Ghostbuster dataset](https://github.com/vivek3141/ghostbuster-data/) from NAACL 2024):

```python
def feature(text: str) -> float:
    'Fraction of quotation marks that are curly/typographic quotes (e.g., \u2018 \u2019 \u201c \u201d) vs plain ASCII quotes, indicating published/edited text'
    import re
    if not text:
        return 0.0
    # count curly quote characters and total quote-like characters
    curly = sum(text.count(ch) for ch in ('\u2018','\u2019','\u201a','\u201c','\u201d','\u201e'))
    plain = sum(text.count(ch) for ch in (\"'\", '\"'))
    total = curly + plain
    if total == 0:
        return 0.0
    return float(curly) / total
```

This feature was generated by gpt-5-mini. It turns out that, for human-generated text, this feature is either at 0 (meaning all quotes, if there are any, are "curly" quotes) or at 1 (meaning the text contains only plain ASCII quotes). However, many ChatGPT-generated essays are in the middle, with the model mixing up both kinds of characters in the same essay. On Ghostbuster, LeaPR essentially matches the state-of-the-art (neural) models except that the final model only relies on a large collection of simple features like the one above. Check out the paper for more details on these results.

![Top-2 Ghostbuster features by SHAP value.](img/fig3.png)

Here, each data point is a text sample from the Ghostbuster dataset, either written by a human or by ChatGPT, and each axis corresponds to the value of one of the top-2 most influential features by SHAP value (small Gaussian jitter to aid visualization). The quote-related feature described above is the top-1 by SHAP value in our best learned model on this dataset. The second one is the fraction of text inside parenthesis. As the image shows, a large fraction of text in parenthesis generally indicates ChatGPT-generated text in the dataset. The idea of LeaPR, of course, is that we did not have to think of such features by ourselves, nor the hundreds of others that, together, nearly saturate performance in the dataset just with a random forest. Yet, we can readily understand them.

## Setup

Use `requirements.txt` to start a virtual environment and install dependencies:

```sh
[leapr/] $ python -m venv venv
[leapr/] $ source venv/bin/activate
[leapr/] (venv) $ pip install -r requirements.txt
...
```

All LLM calls use provider APIs, and you should have your API keys in the appropriate environment variables. Currently, the implementation only supports OpenAI and Anthropic models, but we use langchain to run models, so it should be very easy to add other providers. If you want to run with other models, install the appropriate langchain vendor package and add a case for your new provider in `llm_generator.py`.

### Datasets

This repository supports five domains:

**1. Chess** - Board evaluation and move prediction
- Download data from Lichess using `python download.py all` (80+ GB)
- Also downloads Stockfish engine for ground-truth move evaluation

**2. Image Classification** - Image categorization tasks
- Datasets downloaded automatically from HuggingFace when needed

**3. Text Classification** - Binary/multi-class text classification
- **Ghostbuster**: Clone from [ghostbuster-data](https://github.com/vivek3141/ghostbuster-data/), then run `python create_ghostbuster_datasets.py`
- **AI vs Human Text**: Download from [Kaggle](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text/data) and place `AI_Human.csv` in `data/`

**4. Text Regression** - Predicting continuous values from text
- **Anthropic HH-RLHF** (for reward model debugging): Download using `python rm_debugger/download_hh_rlhf.py`
- Creates `data/hh_rlhf_helpful_base_test.csv` with conversation-response pairs and reward scores
- Use domain config: `text_regression` with dataset: `rm_helpful`

**5. Pairwise Text Classification** - Comparing two text samples
- **Anthropic HH-RLHF** (pairwise format): Same download as above
- Use domain config: `pairwise_text_classification` with dataset: `hh_rlhf_pairwise`

## Running LeaPR

The main LeaPR algorithms are implemented in `representation/did3.py` and `representation/f2.py`. We use Hydra for configuration, so all experiments read from files in `config/`.

### Basic Workflow

**1. Learn features** using representation learning algorithms (DID3 or FunSearch):

```sh
python launch.py --leapr --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This generates a JSON file in `results/features/` containing the learned feature functions.

**2. Train a random forest** on the learned features:

```sh
python launch.py --train --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This trains a model and saves it to `results/models/`, with evaluation metrics in `results/evals/`.

**3. Analyze feature importance** using SHAP:

```sh
python launch.py --interpret --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This computes SHAP values and saves a detailed report to `results/shap/` showing which features are most important for predictions.

### Reward Model Debugging Experiment

For the HH-RLHF reward model debugging experiment:

```sh
# 1. Download dataset
python rm_debugger/download_hh_rlhf.py

# 2. Learn features and train model
python launch.py --leapr --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
python launch.py --train --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini

# 3. Evaluate on test set
python rm_debugger/eval_checkpoint.py  # Update MODEL_CHECKPOINT path in script

# 4. Analyze feature importance
python launch.py --interpret --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

The evaluation script compares the decision tree model's accuracy against a neural reward model baseline on the HH-RLHF test set.

## Available Domains

This fork includes the following domains (see `domain/` and `config/domain/`):

- `chess` - Board position evaluation (regression)
- `image_classification` - Image categorization tasks
- `text_classification` - Text categorization (e.g., AI vs human text detection)
- `text_regression` - Continuous value prediction from text (e.g., reward modeling)
- `pairwise_text_classification` - Pairwise text comparison tasks

Each domain defines:
- Data loading and preprocessing
- Feature execution namespace (available Python libraries/functions)
- Prompt templates for LLM feature generation
- Evaluation metrics appropriate for the task

## Looking at Learned Features

See `results/features/` for feature sets from various experiments. Each JSON file contains Python functions that can be inspected to understand what patterns the model learned.

## Running in Custom Domains

To add a new domain:
1. Create a domain class in `domain/` implementing the `Domain` interface
2. Add a config file in `config/domain/`
3. Create prompt templates in `prompts/`
4. Update data loaders as needed

See existing domains for examples.
