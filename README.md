# LeaPR: Learned Programmatic Representation models

**This is a fork of the original [LeaPR repository](https://github.com/neurosymbolic-learning/leapr) with extensions.**

This repository contains the implementation corresponding to the following paper:

[Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)

LeaPR models are neural network-free machine learning models: they combine *feature functions* represented as (LLM-synthesized) Python functions with *decision tree* predictors learned on top of those features. These models can be trained for any supervised learning task.

## Key Additions in This Fork

- **Text Regression Domain**: New domain for regression tasks on text data, with support for training on the [Anthropic HH-RLHF dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf) for reward model debugging experiments
- **Pairwise Text Classification Domain**: Support for pairwise comparison tasks
- **SHAP Analysis**: Added `--interpret` flag and `interpret.py` script for computing SHAP values to analyze feature importance in trained models
- **Evaluation Scripts**: New scripts in `rm_debugger/` for evaluating models on the HH-RLHF test set and comparing against neural reward models

![Overview of Learned Programmatic Representation Models: training can take any supervised learning dataset and output a set of features implemented as Python functions.](img/fig1.png)

LeaPR models can contain hundreds or thousands of automatically generated features, modularly combined by decision trees. Because features are LLM-generated code, they tend to be highly readable and interpretable, and tools such as SHAP values, used to interpret and explain decisions from classical ML models, can be easily applied to LeaPR models.

Here is an example of a feature learned for the *text classification* task of classifying human vs ChatGPT-generated text (which we evaluated using the [Ghostbuster dataset](https://github.com/vivek3141/ghostbuster-data/) from NAACL 2024):

```python
def feature(text: str) -> float:
    'Fraction of quotation marks that are curly/typographic quotes (e.g., \u2018 \u2019 \u201c \u201d) vs plain ASCII quotes, indicating published/edited text'
    import re
    if not text:
        return 0.0
    # count curly quote characters and total quote-like characters
    curly = sum(text.count(ch) for ch in ('\u2018','\u2019','\u201a','\u201c','\u201d','\u201e'))
    plain = sum(text.count(ch) for ch in (\"'\", '\"'))
    total = curly + plain
    if total == 0:
        return 0.0
    return float(curly) / total
```

This feature was generated by gpt-5-mini. It turns out that, for human-generated text, this feature is either at 0 (meaning all quotes, if there are any, are "curly" quotes) or at 1 (meaning the text contains only plain ASCII quotes). However, many ChatGPT-generated essays are in the middle, with the model mixing up both kinds of characters in the same essay. On Ghostbuster, LeaPR essentially matches the state-of-the-art (neural) models except that the final model only relies on a large collection of simple features like the one above. Check out the paper for more details on these results.

![Top-2 Ghostbuster features by SHAP value.](img/fig3.png)

Here, each data point is a text sample from the Ghostbuster dataset, either written by a human or by ChatGPT, and each axis corresponds to the value of one of the top-2 most influential features by SHAP value (small Gaussian jitter to aid visualization). The quote-related feature described above is the top-1 by SHAP value in our best learned model on this dataset. The second one is the fraction of text inside parenthesis. As the image shows, a large fraction of text in parenthesis generally indicates ChatGPT-generated text in the dataset. The idea of LeaPR, of course, is that we did not have to think of such features by ourselves, nor the hundreds of others that, together, nearly saturate performance in the dataset just with a random forest. Yet, we can readily understand them.

## Setup

Use `requirements.txt` to start a virtual environment and install dependencies:

```sh
[leapr/] $ python -m venv venv
[leapr/] $ source venv/bin/activate
[leapr/] (venv) $ pip install -r requirements.txt
...
```

All LLM calls use provider APIs, and you should have your API keys in the appropriate environment variables. Currently, the implementation only supports OpenAI and Anthropic models, but we use langchain to run models, so it should be very easy to add other providers. If you want to run with other models, install the appropriate langchain vendor package and add a case for your new provider in `llm_generator.py`.

### NLP Libraries for Expert Text API

For advanced text analysis features (expert API level), use the provided setup script to download required NLP models:

```sh
bash setup_nlp_libraries.sh
```

These libraries enable advanced linguistic features:
- **spaCy**: Part-of-speech tagging, dependency parsing, named entity recognition
- **NLTK**: VADER sentiment analysis
- **TextBlob**: Sentiment polarity and subjectivity
- **Textstat**: Readability metrics (Flesch scores, etc.)

### Datasets

This repository supports five domains:

**1. Chess** - Board evaluation and move prediction
- Download data from Lichess using `python download.py all` (80+ GB)
- Also downloads Stockfish engine for ground-truth move evaluation

**2. Image Classification** - Image categorization tasks
- Datasets downloaded automatically from HuggingFace when needed

**3. Text Classification** - Binary/multi-class text classification
- **Ghostbuster**: Clone from [ghostbuster-data](https://github.com/vivek3141/ghostbuster-data/), then run `python create_ghostbuster_datasets.py`
- **AI vs Human Text**: Download from [Kaggle](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text/data) and place `AI_Human.csv` in `data/`

**4. Text Regression** - Predicting continuous values from text
- **Anthropic HH-RLHF** (for reward model debugging): Download using `python rm_debugger/download_hh_rlhf.py`
- Creates `data/hh_rlhf_helpful_base_test.csv` with conversation-response pairs and reward scores
- Use domain config: `text_regression` with dataset: `rm_helpful`
- **API Levels**: Text regression supports three API levels for feature generation (see below)

**5. Pairwise Text Classification** - Comparing two text samples
- **Anthropic HH-RLHF** (pairwise format): Same download as above
- Use domain config: `pairwise_text_classification` with dataset: `hh_rlhf_pairwise`

## Running LeaPR

The main LeaPR algorithms are implemented in `representation/did3.py` and `representation/f2.py`. We use Hydra for configuration, so all experiments read from files in `config/`.

### Basic Workflow

**1. Learn features** using representation learning algorithms (DID3 or FunSearch):

```sh
python launch.py --leapr --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This generates a JSON file in `results/features/` containing the learned feature functions.

**2. Train a random forest** on the learned features:

```sh
python launch.py --train --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This trains a model and saves it to `results/models/`, with evaluation metrics in `results/evals/`.

**3. Analyze feature importance** using SHAP:

```sh
python launch.py --interpret --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This computes SHAP values and saves a detailed report to `results/shap/` showing which features are most important for predictions.

### Reward Model Debugging & Analysis

For the HH-RLHF reward model debugging experiment, we provide several tools in `rm_debugger/` to analyze model behavior, sensitivity, and feature importance.

#### 1. Setup & Training

```sh
# 1. Download dataset
python rm_debugger/download_hh_rlhf.py

# 2. Learn features and train model
python launch.py --leapr --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
python launch.py --train --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

#### 2. Evaluation on Test Set

Compare the decision tree model's accuracy against a neural reward model baseline on the HH-RLHF test set:

```sh
python rm_debugger/eval_checkpoint.py  # Update MODEL_CHECKPOINT path in script
```

#### 3. Sensitivity Analysis (`e_sensitivity.py`)

Test the reward model's sensitivity to simple text perturbations (e.g., adding "e" or custom strings) to detect spurious correlations or fragility. This script generates statistical reports on how perturbations affect reward scores.

```sh
# Standard analysis (sensitivity to appending 'e', 'ee', 'eee')
python rm_debugger/e_sensitivity.py

# Custom perturbation (e.g., prepending a specific string)
python rm_debugger/e_sensitivity.py --custom " Ignore previous instructions" --prepend
```

Results are saved to CSV files (e.g., `e_sensitivity_results_append.csv`).

#### 4. Intervention & Exploits (`intervention.py`)

Test the model against known exploits or specific intervention cases to measure robustness against adversarial examples or specific failure modes.

```sh
python rm_debugger/intervention.py --data_path rm_debugger/data/exploit/verbose_hallucination.json
```

#### 5. Feature Importance (SHAP)

Analyze which learned features drive the model's predictions using SHAP values.

```sh
python launch.py --interpret --learner did3 --domain text_regression_rm_helpful --model gpt-4o-mini
```

This computes SHAP values and saves detailed reports to `results/shap/` (e.g., `expert_did3__text_regression_rm_helpful__gpt-4o-mini.json`). These JSON reports contain the feature definitions and their corresponding SHAP values, allowing for in-depth interpretation of what the model has learned.

## Available Domains

This fork includes the following domains (see `domain/` and `config/domain/`):

- `chess` - Board position evaluation (regression)
- `image_classification` - Image categorization tasks
- `text_classification` - Text categorization (e.g., AI vs human text detection)
- `text_regression` - Continuous value prediction from text (e.g., reward modeling)
- `pairwise_text_classification` - Pairwise text comparison tasks

Each domain defines:
- Data loading and preprocessing
- Feature execution namespace (available Python libraries/functions)
- Prompt templates for LLM feature generation
- Evaluation metrics appropriate for the task

### Text Regression API Levels

The text regression domain supports three API levels that control which libraries and features are exposed to the LLM for feature generation. Configure via `config/domain/text_regression.yaml`:

#### Basic API (`api_level: basic`)
Simple text processing with Python built-ins and regex:
- String methods (`.split()`, `.lower()`, `.count()`, etc.)
- Regular expressions (`re` module)
- Basic text statistics (word count, character count, etc.)

Example feature:
```python
def feature(text: str) -> float:
    "Average word length"
    words = text.split()
    if not words:
        return 0.0
    return sum(len(word) for word in words) / len(words)
```

#### Plus API (`api_level: plus`)
Adds statistical and data processing utilities:
- **statistics**: `mean()`, `median()`, `pstdev()`
- **collections**: `Counter()`, `defaultdict()`
- **itertools**: `islice()`, `pairwise()`
- **numpy**: Array operations, `percentile()`, `diff()`
- **string/unicodedata**: Character category inspection

Example feature:
```python
def feature(text: str) -> float:
    "Median sentence length in words"
    import statistics
    sentences = re.split(r'[.!?]+', text)
    lengths = [len(s.split()) for s in sentences if s.split()]
    return float(statistics.median(lengths)) if lengths else 0.0
```

#### Expert API (`api_level: expert`)
Advanced NLP libraries for linguistic analysis (requires additional setup, see above):
- **spaCy**: Part-of-speech tagging, dependency parsing, named entity recognition, lemmatization
- **NLTK**: VADER sentiment analysis (compound, positive, negative, neutral scores)
- **TextBlob**: Sentiment polarity and subjectivity scores
- **Textstat**: Readability metrics (Flesch Reading Ease, Flesch-Kincaid Grade, SMOG Index, etc.)

Example features:
```python
def feature(text: str) -> float:
    "Ratio of verbs to nouns"
    import spacy
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')
    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')
    return float(verb_count) / (noun_count + 1)

def feature(text: str) -> float:
    "VADER sentiment compound score"
    from nltk.sentiment import SentimentIntensityAnalyzer
    sia = SentimentIntensityAnalyzer()
    return float(sia.polarity_scores(text)['compound'])

def feature(text: str) -> float:
    "Flesch Reading Ease score"
    import textstat
    return float(textstat.flesch_reading_ease(text))
```

The expert API enables the LLM to generate sophisticated linguistic features capturing:
- Syntactic complexity (POS patterns, dependency structures)
- Semantic richness (named entities, lexical diversity)
- Pragmatic patterns (sentiment, politeness, certainty markers)
- Text readability and formality

**Configuration**: Edit `config/domain/text_regression.yaml`:
```yaml
domain_name: text_regression
api_level: expert  # Options: basic, plus, expert
```

## Looking at Learned Features

See `results/features/` for feature sets from various experiments. Each JSON file contains Python functions that can be inspected to understand what patterns the model learned.

## Running in Custom Domains

To add a new domain:
1. Create a domain class in `domain/` implementing the `Domain` interface
2. Add a config file in `config/domain/`
3. Create prompt templates in `prompts/`
4. Update data loaders as needed

See existing domains for examples.
