{
  "model_info": {
    "learner": "combo",
    "domain": "text_regression_rm_helpful",
    "model": "gpt-4o-mini",
    "api_level": "expert",
    "checkpoint_path": "results/models/combo__text_regression_rm_helpful__gpt-4o-mini.pkl",
    "features_path": "results/features/combo__text_regression_rm_helpful__gpt-4o-mini.json",
    "analysis_date": "2025-11-21T11:37:21.239934"
  },
  "dataset_info": {
    "split": "validation",
    "num_samples": 236,
    "feature_matrix_shape": [
      236,
      400
    ]
  },
  "feature_importance": [
    {
      "feature_index": 0,
      "feature_name": "feature_0",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of words spoken per turn in the text'\n    turns = text.split('Assistant:')\n    total_words = sum(len(turn.split()) for turn in turns)\n    return total_words / len(turns) if turns else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.26462566399502935,
        "mean_shap": 0.04288571959511954,
        "std_shap": 0.28651063942329846,
        "min_shap": -0.5845663122582269,
        "max_shap": 0.40592123181791057
      },
      "rank": 1
    },
    {
      "feature_index": 1,
      "feature_name": "feature_1",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique nouns in the text'\n    doc = nlp(text)\n    unique_nouns = {token.text.lower() for token in doc if token.pos_ == 'NOUN'}\n    return float(len(unique_nouns))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.12598087272741193,
        "mean_shap": 0.005047623202186383,
        "std_shap": 0.1538237367445065,
        "min_shap": -0.22339925779517544,
        "max_shap": 0.3671613116651799
      },
      "rank": 2
    },
    {
      "feature_index": 2,
      "feature_name": "feature_2",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of personal pronouns to total words in the text\"\n    doc = nlp(text)\n    personal_pronouns = sum(1 for token in doc if token.text.lower() in ['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'])\n    total_words = len([token for token in doc if token.is_alpha])\n    return personal_pronouns / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.09032738199732383,
        "mean_shap": 0.014062106388855725,
        "std_shap": 0.10918708721649945,
        "min_shap": -0.18353309068279955,
        "max_shap": 0.3348753386046564
      },
      "rank": 3
    },
    {
      "feature_index": 3,
      "feature_name": "feature_3",
      "feature_code": "def feature(text: str) -> float:\n    'Average word length in characters within questions.'\n    if text.strip().endswith('?'):\n        words = text.split()\n        return float(sum(len(word) for word in words)) / len(words) if words else 0.0\n    return 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.054088080872463334,
        "mean_shap": -0.0003043128811752054,
        "std_shap": 0.07781750330095376,
        "min_shap": -0.3672180593905961,
        "max_shap": 0.07851384836819868
      },
      "rank": 4
    },
    {
      "feature_index": 4,
      "feature_name": "feature_4",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of occurrences of key thematic words divided by total words\"\n    thematic_words = ['human', 'social', 'pandemic', 'learn', 'people']\n    count = sum(text.lower().count(word) for word in thematic_words)\n    total_words = len(text.split())\n    return count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.04126275037606872,
        "mean_shap": 0.01211325729611488,
        "std_shap": 0.049934278949192966,
        "min_shap": -0.12518560922889804,
        "max_shap": 0.10963122967711901
      },
      "rank": 5
    },
    {
      "feature_index": 200,
      "feature_name": "feature_200",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of non-stopwords in the text\"\n    doc = nlp(text)\n    non_stopword_count = sum(1 for token in doc if not token.is_stop and token.is_alpha)\n    return float(non_stopword_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.03642419171401528,
        "mean_shap": 0.00900043464674234,
        "std_shap": 0.04406936114041401,
        "min_shap": -0.16371223537165216,
        "max_shap": 0.09500249376815337
      },
      "rank": 6
    },
    {
      "feature_index": 201,
      "feature_name": "feature_201",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique lemmas in the text\"\n    doc = nlp(text)\n    lemmas = {token.lemma_.lower() for token in doc if token.is_alpha}\n    return float(len(lemmas))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.032653984230041885,
        "mean_shap": 0.00013110807684995138,
        "std_shap": 0.04076264472470579,
        "min_shap": -0.049706990501433665,
        "max_shap": 0.14295530423237432
      },
      "rank": 7
    },
    {
      "feature_index": 5,
      "feature_name": "feature_5",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of stopwords to total words in the text'\n    doc = nlp(text)\n    total_words = len([token for token in doc if token.is_alpha])\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.028716947236135538,
        "mean_shap": 0.0009912521194034762,
        "std_shap": 0.03576891296781819,
        "min_shap": -0.09483984248530729,
        "max_shap": 0.11261846468417008
      },
      "rank": 8
    },
    {
      "feature_index": 8,
      "feature_name": "feature_8",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of pronouns to total words in the text\"\n    doc = nlp(text)\n    pronoun_count = sum(1 for token in doc if token.pos_ == 'PRON')\n    return pronoun_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.027043497778199563,
        "mean_shap": 0.001283546117104483,
        "std_shap": 0.030828292876307364,
        "min_shap": -0.06896946304218785,
        "max_shap": 0.052199106071251215
      },
      "rank": 9
    },
    {
      "feature_index": 10,
      "feature_name": "feature_10",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.02670832937937523,
        "mean_shap": 0.00047826545853856466,
        "std_shap": 0.03552437863693901,
        "min_shap": -0.04522059805918121,
        "max_shap": 0.10071924410688221
      },
      "rank": 10
    },
    {
      "feature_index": 11,
      "feature_name": "feature_11",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.02574263121542102,
        "mean_shap": 0.0005568252020781613,
        "std_shap": 0.03387184125248157,
        "min_shap": -0.04799231002943522,
        "max_shap": 0.08801447152849294
      },
      "rank": 11
    },
    {
      "feature_index": 14,
      "feature_name": "feature_14",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.02536428682794386,
        "mean_shap": 0.00031070586700704996,
        "std_shap": 0.03339379298797523,
        "min_shap": -0.04131178105453584,
        "max_shap": 0.09195591833100232
      },
      "rank": 12
    },
    {
      "feature_index": 9,
      "feature_name": "feature_9",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.02401380078267979,
        "mean_shap": 0.0004357632202683462,
        "std_shap": 0.032204642249313555,
        "min_shap": -0.039069000894320846,
        "max_shap": 0.0877662958412035
      },
      "rank": 13
    },
    {
      "feature_index": 202,
      "feature_name": "feature_202",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of commas in the text\"\n    return float(text.count(','))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.02218152877799578,
        "mean_shap": 0.003047979082901042,
        "std_shap": 0.02373404990883595,
        "min_shap": -0.03832517015591489,
        "max_shap": 0.04999809869791688
      },
      "rank": 14
    },
    {
      "feature_index": 6,
      "feature_name": "feature_6",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.021121692342717286,
        "mean_shap": 2.8174283820654287e-05,
        "std_shap": 0.029091331643221613,
        "min_shap": -0.03296511627273269,
        "max_shap": 0.084055341363112
      },
      "rank": 15
    },
    {
      "feature_index": 12,
      "feature_name": "feature_12",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.019996101807068095,
        "mean_shap": 0.00018006931514773857,
        "std_shap": 0.027044492380204607,
        "min_shap": -0.029998099471581618,
        "max_shap": 0.08037470683010417
      },
      "rank": 16
    },
    {
      "feature_index": 19,
      "feature_name": "feature_19",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical density of the text'\n    doc = nlp(text)\n    total_tokens = len(doc)\n    content_tokens = sum(1 for token in doc if token.pos_ not in ['DET', 'ADP', 'CONJ', 'PRON', 'PUNCT', 'NUM'])\n    return content_tokens / total_tokens if total_tokens > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.019550889164333438,
        "mean_shap": -0.0037862620107118926,
        "std_shap": 0.023753973135607218,
        "min_shap": -0.05755581205723494,
        "max_shap": 0.052094657835424635
      },
      "rank": 17
    },
    {
      "feature_index": 15,
      "feature_name": "feature_15",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of words per question in the text'\n    questions = re.findall(r'\\?\\s*', text)\n    if not questions:\n        return 0.0\n    return float(len(text.split())) / len(questions)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.019417525735551867,
        "mean_shap": -0.002644712781777748,
        "std_shap": 0.026919336062445304,
        "min_shap": -0.14908453502623534,
        "max_shap": 0.03745789611061975
      },
      "rank": 18
    },
    {
      "feature_index": 16,
      "feature_name": "feature_16",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of words per question in the text'\n    questions = re.findall(r'\\?\\s*', text)\n    if not questions:\n        return 0.0\n    return float(len(text.split())) / len(questions)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.018599881959781465,
        "mean_shap": -0.0025192611057593636,
        "std_shap": 0.026483436842145897,
        "min_shap": -0.1520582632929519,
        "max_shap": 0.03627518912574514
      },
      "rank": 19
    },
    {
      "feature_index": 211,
      "feature_name": "feature_211",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentence fragments (absence of punctuation)\"\n    sentences = re.split(r'[.!?]+', text)\n    fragment_count = sum(1 for s in sentences if len(s.split()) < 3)\n    return float(fragment_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.017113236667395445,
        "mean_shap": 0.0014774690052472368,
        "std_shap": 0.03946883776528038,
        "min_shap": -0.29190509976678636,
        "max_shap": 0.020263483239279945
      },
      "rank": 20
    },
    {
      "feature_index": 17,
      "feature_name": "feature_17",
      "feature_code": "def feature(text: str) -> float:\n    'Presence of personal pronouns in the text (I, me, my)'\n    personal_pronouns = {'i', 'me', 'my', 'you', 'your'}\n    words = text.split()\n    if not words:\n        return 0.0\n    pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n    return float(pronoun_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01698384547515254,
        "mean_shap": 0.0026307429514492793,
        "std_shap": 0.02259919128555091,
        "min_shap": -0.044144255978075364,
        "max_shap": 0.09366819731740807
      },
      "rank": 21
    },
    {
      "feature_index": 23,
      "feature_name": "feature_23",
      "feature_code": "def feature(text: str) -> float:\n    'Presence of personal pronouns in the text (I, me, my)'\n    personal_pronouns = {'i', 'me', 'my', 'you', 'your'}\n    words = text.split()\n    if not words:\n        return 0.0\n    pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n    return float(pronoun_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01693285685459065,
        "mean_shap": 0.0026479941872341634,
        "std_shap": 0.02284283408975289,
        "min_shap": -0.04354215102666589,
        "max_shap": 0.0973854789384704
      },
      "rank": 22
    },
    {
      "feature_index": 25,
      "feature_name": "feature_25",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of non-stopwords to total words'\n    doc = nlp(text)\n    non_stopword_count = sum(1 for token in doc if not token.is_stop and token.is_alpha)\n    total_word_count = len(doc)\n    return float(non_stopword_count) / total_word_count if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01655121348448579,
        "mean_shap": 0.00021173933255920369,
        "std_shap": 0.021672408872904586,
        "min_shap": -0.04957301435451613,
        "max_shap": 0.06274819752418334
      },
      "rank": 23
    },
    {
      "feature_index": 18,
      "feature_name": "feature_18",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of personal pronouns to total words indicating personal engagement'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    personal_pronouns = sum(1 for token in doc if token.tag_ in ['PRP', 'PRP$', 'WP', 'WP$'])\n    return personal_pronouns / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01654018276529864,
        "mean_shap": 0.0019551624275642642,
        "std_shap": 0.01883941511764921,
        "min_shap": -0.03296140404499112,
        "max_shap": 0.051142762768841554
      },
      "rank": 24
    },
    {
      "feature_index": 22,
      "feature_name": "feature_22",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    stopword_count = sum(1 for token in nlp(text) if token.is_stop)\n    return float(stopword_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.016038181364484344,
        "mean_shap": -0.0008180823326567441,
        "std_shap": 0.0197800547800922,
        "min_shap": -0.04048325655454332,
        "max_shap": 0.07348683536930248
      },
      "rank": 25
    },
    {
      "feature_index": 20,
      "feature_name": "feature_20",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of sentences to paragraphs in the text'\n    paragraphs = text.split('\\n')\n    sentence_count = sum(text.count(s) for s in ['.', '!', '?'])\n    return sentence_count / len(paragraphs) if len(paragraphs) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.015588353702630712,
        "mean_shap": -0.00037989664111424655,
        "std_shap": 0.020420406091442935,
        "min_shap": -0.06855817757971322,
        "max_shap": 0.04239758712856927
      },
      "rank": 26
    },
    {
      "feature_index": 13,
      "feature_name": "feature_13",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words longer than 6 characters\"\n    long_words = sum(1 for word in text.split() if len(word) > 6)\n    return float(long_words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.015505403304405382,
        "mean_shap": 0.0009016938672274283,
        "std_shap": 0.01707671967370487,
        "min_shap": -0.034472463204492654,
        "max_shap": 0.03556970770680319
      },
      "rank": 27
    },
    {
      "feature_index": 206,
      "feature_name": "feature_206",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of content words (nouns, verbs, adjectives, adverbs) to total words\"\n    doc = nlp(text)\n    content_word_count = sum(1 for token in doc if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'})\n    return float(content_word_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.015433778436194897,
        "mean_shap": -0.0005723110824133483,
        "std_shap": 0.020342486723173656,
        "min_shap": -0.048147929087168174,
        "max_shap": 0.08037501204175193
      },
      "rank": 28
    },
    {
      "feature_index": 7,
      "feature_name": "feature_7",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical density measuring ratio of content words to total words'\n    doc = nlp(text)\n    total_words = len(doc)\n    if total_words == 0:\n        return 0.0\n    content_word_count = sum(1 for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV'])\n    return float(content_word_count) / total_words\n",
      "shap_statistics": {
        "mean_abs_shap": 0.014626286691228984,
        "mean_shap": -0.0005782664123727565,
        "std_shap": 0.018759465733455504,
        "min_shap": -0.04496208253176308,
        "max_shap": 0.07388859349075742
      },
      "rank": 29
    },
    {
      "feature_index": 29,
      "feature_name": "feature_29",
      "feature_code": "def feature(text: str) -> float:\n    \"Unique stopword ratio in the text\"\n    doc = nlp(text)\n    stopwords = [token for token in doc if token.is_stop]\n    unique_stopwords = len(set(token.lemma_.lower() for token in stopwords))\n    return float(unique_stopwords) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.012824618457793107,
        "mean_shap": 0.0031202242408284266,
        "std_shap": 0.016096045083565293,
        "min_shap": -0.02874729309240448,
        "max_shap": 0.07055044651737796
      },
      "rank": 30
    },
    {
      "feature_index": 21,
      "feature_name": "feature_21",
      "feature_code": "def feature(text: str) -> float:\n    'Count of words longer than six characters'\n    long_words = sum(1 for word in text.split() if len(word) > 6)\n    return float(long_words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01241255356773272,
        "mean_shap": 0.0015139851696883676,
        "std_shap": 0.014259118022741176,
        "min_shap": -0.029531009048909884,
        "max_shap": 0.03707007613259379
      },
      "rank": 31
    },
    {
      "feature_index": 203,
      "feature_name": "feature_203",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words with 5 or more characters\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) >= 5)\n    return float(long_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.011984136937350263,
        "mean_shap": 1.145949468350441e-05,
        "std_shap": 0.014099825539395087,
        "min_shap": -0.028023873562237657,
        "max_shap": 0.03850335861295226
      },
      "rank": 32
    },
    {
      "feature_index": 34,
      "feature_name": "feature_34",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of passages'\n    passage_sentiments = []\n    passages = text.split('\\n')\n    for passage in passages:\n        blob = TextBlob(passage)\n        passage_sentiments.append(blob.sentiment.polarity)\n    return float(statistics.mean(passage_sentiments)) if passage_sentiments else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.011514418869469138,
        "mean_shap": -0.0008558385259085781,
        "std_shap": 0.013824368801968223,
        "min_shap": -0.03723135734016633,
        "max_shap": 0.05562005111179776
      },
      "rank": 33
    },
    {
      "feature_index": 31,
      "feature_name": "feature_31",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of punctuation sequences in the text'\n    punctuation_sequences = re.findall(r'[{0}]+'.format(re.escape(string.punctuation)), text)\n    if not punctuation_sequences:\n        return 0.0\n    return float(sum(len(seq) for seq in punctuation_sequences)) / len(punctuation_sequences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.011231888564233294,
        "mean_shap": 0.0019582911025370063,
        "std_shap": 0.018661397448420904,
        "min_shap": -0.09448076954714986,
        "max_shap": 0.01726040471457677
      },
      "rank": 34
    },
    {
      "feature_index": 33,
      "feature_name": "feature_33",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of specific parts of speech (e.g., nouns)\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.009156117440839422,
        "mean_shap": -0.0005835801311536907,
        "std_shap": 0.012450009129518685,
        "min_shap": -0.01375266321046287,
        "max_shap": 0.04201084620561434
      },
      "rank": 35
    },
    {
      "feature_index": 32,
      "feature_name": "feature_32",
      "feature_code": "def feature(text: str) -> float:\n    'Average position of named entities in the text'\n    doc = nlp(text)\n    entity_positions = [ent.start_char for ent in doc.ents]\n    if not entity_positions:\n        return 0.0\n    return float(sum(entity_positions)) / len(entity_positions)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.009071453690199158,
        "mean_shap": 0.0024998878191660433,
        "std_shap": 0.010991989646859341,
        "min_shap": -0.01891766969677638,
        "max_shap": 0.030314694922338386
      },
      "rank": 36
    },
    {
      "feature_index": 212,
      "feature_name": "feature_212",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of negations in the text\"\n    negations = ['not', 'no', 'never', 'none']\n    return float(sum(text.count(neg) for neg in negations))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00906774396161199,
        "mean_shap": -0.0023524916127565087,
        "std_shap": 0.011057568956545671,
        "min_shap": -0.0382922808073442,
        "max_shap": 0.012749911703989321
      },
      "rank": 37
    },
    {
      "feature_index": 53,
      "feature_name": "feature_53",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase letters to total letters in the text'\n    upper_count = sum(1 for c in text if c.isupper())\n    total_count = sum(1 for c in text if c.isalpha())\n    if total_count == 0:\n        return 0.0\n    return upper_count / total_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008978068766691013,
        "mean_shap": 0.001176835125511613,
        "std_shap": 0.01071108112406478,
        "min_shap": -0.01630871052256196,
        "max_shap": 0.0355119820626367
      },
      "rank": 38
    },
    {
      "feature_index": 36,
      "feature_name": "feature_36",
      "feature_code": "def feature(text: str) -> float:\n    'Personal pronoun ratio in the text'\n    doc = nlp(text)\n    personal_pronoun_count = sum(1 for token in doc if token.tag_ in ['PRP', 'PRP$'])\n    return personal_pronoun_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008848402839042079,
        "mean_shap": -1.860558096217054e-05,
        "std_shap": 0.01109949658852978,
        "min_shap": -0.01506472478026029,
        "max_shap": 0.043462761300668964
      },
      "rank": 39
    },
    {
      "feature_index": 37,
      "feature_name": "feature_37",
      "feature_code": "def feature(text: str) -> float:\n    \"Variance of sentiment scores in the text using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(statistics.pstdev([scores['pos'], scores['neg'], scores['neu']])) if any([scores['pos'], scores['neg'], scores['neu']]) else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008654197019227214,
        "mean_shap": -0.00025078164856437186,
        "std_shap": 0.010349599071860451,
        "min_shap": -0.027419938564330726,
        "max_shap": 0.02151792703302065
      },
      "rank": 40
    },
    {
      "feature_index": 41,
      "feature_name": "feature_41",
      "feature_code": "def feature(text: str) -> float:\n    'Count of sentences containing the word \"I\" or \"me\"'\n    sentences = re.split(r'[.!?]+', text)\n    i_count = sum(1 for s in sentences if ' I ' in s or ' me ' in s)\n    return float(i_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00856845760170591,
        "mean_shap": -0.0003180970723294505,
        "std_shap": 0.010466915944939441,
        "min_shap": -0.02685839872352332,
        "max_shap": 0.025354224317801648
      },
      "rank": 41
    },
    {
      "feature_index": 28,
      "feature_name": "feature_28",
      "feature_code": "def feature(text: str) -> float:\n    'Number of nouns in the text'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008197922221845812,
        "mean_shap": -0.00047189561579237906,
        "std_shap": 0.010929677760448408,
        "min_shap": -0.011854290187610578,
        "max_shap": 0.035397474553928526
      },
      "rank": 42
    },
    {
      "feature_index": 43,
      "feature_name": "feature_43",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of the longest word length to average word length'\n    words = text.split()\n    if not words:\n        return 0.0\n    longest_word_length = max(len(word) for word in words)\n    average_word_length = sum(len(word) for word in words) / len(words)\n    return longest_word_length / average_word_length if average_word_length > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008103670472449422,
        "mean_shap": 0.0002550301093901002,
        "std_shap": 0.011858866667959718,
        "min_shap": -0.07377572184412595,
        "max_shap": 0.01884636596624854
      },
      "rank": 43
    },
    {
      "feature_index": 39,
      "feature_name": "feature_39",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of complex words in the text (more than two syllables)\"\n    complexity_count = len(re.findall(r'\\b\\w*[aeiou]{1}\\w*[aeiou]{1}\\w*\\b', text))\n    word_count = len(text.split())\n    return (complexity_count / word_count) if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00807382681060542,
        "mean_shap": -0.00029983131376859094,
        "std_shap": 0.009599466438448915,
        "min_shap": -0.026191797838063805,
        "max_shap": 0.01860082825389823
      },
      "rank": 44
    },
    {
      "feature_index": 40,
      "feature_name": "feature_40",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of conjunctions to total words in the text'\n    doc = nlp(text)\n    conjunctions = sum(1 for token in doc if token.dep_ == 'cc')\n    total_words = len(doc)\n    return float(conjunctions) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007920782842785742,
        "mean_shap": -0.0014682879919753619,
        "std_shap": 0.009773085032967711,
        "min_shap": -0.022369382808340697,
        "max_shap": 0.02909552399688306
      },
      "rank": 45
    },
    {
      "feature_index": 35,
      "feature_name": "feature_35",
      "feature_code": "def feature(text: str) -> float:\n    \"Weighted count of nouns considering their frequency in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007881088075641772,
        "mean_shap": -0.00042068439218809266,
        "std_shap": 0.011267930247859902,
        "min_shap": -0.011092559222157452,
        "max_shap": 0.040272852832258725
      },
      "rank": 46
    },
    {
      "feature_index": 42,
      "feature_name": "feature_42",
      "feature_code": "def feature(text: str) -> float:\n    'Average character length between punctuations'\n    segments = re.split(r'[,.!?]', text)\n    lengths = [len(segment.strip()) for segment in segments if segment.strip()]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007382011308444065,
        "mean_shap": -0.00010255323842621167,
        "std_shap": 0.009108377394697926,
        "min_shap": -0.016822100434900202,
        "max_shap": 0.027696657675787464
      },
      "rank": 47
    },
    {
      "feature_index": 44,
      "feature_name": "feature_44",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase letters to total letters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    total_letters = sum(1 for c in text if c.isalpha())\n    return (uppercase_count / total_letters) if total_letters else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007374670697793376,
        "mean_shap": 0.0012575928853355962,
        "std_shap": 0.008964068236629102,
        "min_shap": -0.012842136583001327,
        "max_shap": 0.031562553440727496
      },
      "rank": 48
    },
    {
      "feature_index": 48,
      "feature_name": "feature_48",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of conjunctions to total words in the text\"\n    doc = nlp(text)\n    conjunction_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conjunction_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00734227797142974,
        "mean_shap": -0.00048792530391714015,
        "std_shap": 0.009079944530713764,
        "min_shap": -0.019097635862980168,
        "max_shap": 0.029941156756178994
      },
      "rank": 49
    },
    {
      "feature_index": 45,
      "feature_name": "feature_45",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of sentence fragments per sentence\"\n    sentences = re.split(r'[.!?]+', text)\n    fragments = sum(len(s.split(',')) - 1 for s in sentences)  # Count commas as fragments\n    avg_fragments = fragments / len(sentences) if sentences else 0.0\n    return avg_fragments\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006854214492485792,
        "mean_shap": 0.0006945144238567082,
        "std_shap": 0.007863449495774515,
        "min_shap": -0.01731553507184542,
        "max_shap": 0.015852023706057157
      },
      "rank": 50
    },
    {
      "feature_index": 46,
      "feature_name": "feature_46",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences with multiple clauses (using commas)\"\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    complex_sentences = sum(1 for s in sentences if s.count(',') > 1)\n    return float(complex_sentences) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006852428180129523,
        "mean_shap": 0.0005184011438837473,
        "std_shap": 0.00863379618050064,
        "min_shap": -0.022337814353695144,
        "max_shap": 0.05039959100283935
      },
      "rank": 51
    },
    {
      "feature_index": 51,
      "feature_name": "feature_51",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of question marks to total punctuation in the text\"\n    question_marks = text.count('?')\n    total_punctuation = sum(1 for c in text if c in string.punctuation)\n    return question_marks / total_punctuation if total_punctuation > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0067859981782302315,
        "mean_shap": -0.0031467433473916927,
        "std_shap": 0.007883189757921046,
        "min_shap": -0.029234736373376275,
        "max_shap": 0.010520265819561326
      },
      "rank": 52
    },
    {
      "feature_index": 50,
      "feature_name": "feature_50",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of personal pronouns to indicate personal engagement\"\n    personal_pronouns = {'i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'}\n    words = text.lower().split()\n    personal_count = sum(1 for word in words if word in personal_pronouns)\n    return float(personal_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006785436000990607,
        "mean_shap": -0.00044212663580366676,
        "std_shap": 0.00835556479779464,
        "min_shap": -0.013227742725334524,
        "max_shap": 0.027169638477487976
      },
      "rank": 53
    },
    {
      "feature_index": 54,
      "feature_name": "feature_54",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of noun phrases to total words in the text\"\n    doc = nlp(text)\n    noun_phrases = sum(1 for chunk in doc.noun_chunks)\n    word_count = len(doc)\n    return noun_phrases / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006757596756568201,
        "mean_shap": -0.0005174450097191268,
        "std_shap": 0.008463463079205789,
        "min_shap": -0.019072348388886304,
        "max_shap": 0.027051791347355754
      },
      "rank": 54
    },
    {
      "feature_index": 49,
      "feature_name": "feature_49",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words to total words, indicating lexical difficulty\"\n    words = text.split()\n    complex_words_count = sum(1 for word in words if len(word) > 2 and sum(c.isalpha() for c in word) > 2)\n    return float(complex_words_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006517400698814639,
        "mean_shap": 0.0003214076517646817,
        "std_shap": 0.00769515445075502,
        "min_shap": -0.017961980883583036,
        "max_shap": 0.015704697837820056
      },
      "rank": 55
    },
    {
      "feature_index": 52,
      "feature_name": "feature_52",
      "feature_code": "def feature(text: str) -> float:\n    'Sentence complexity measured by average number of clauses (using conjunctions)'\n    clauses = text.count(',') + text.count('and') + text.count('but') + text.count(';')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count == 0:\n        return 0.0\n    return float(clauses) / sentence_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006263878492558191,
        "mean_shap": 0.0015336713583207046,
        "std_shap": 0.010581750802820358,
        "min_shap": -0.06958555855403784,
        "max_shap": 0.028435331521892454
      },
      "rank": 56
    },
    {
      "feature_index": 217,
      "feature_name": "feature_217",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with negation words\"\n    negation_words = {'not', 'no', 'never', 'none', 'nobody', 'nothing'}\n    sentences = re.split(r'[.!?]+', text)\n    negation_sentence_count = sum(1 for s in sentences if any(neg in s.lower() for neg in negation_words))\n    return float(negation_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006191798942078083,
        "mean_shap": -0.0009042136832714914,
        "std_shap": 0.0072347214281143464,
        "min_shap": -0.01820018788941792,
        "max_shap": 0.012839558658103606
      },
      "rank": 57
    },
    {
      "feature_index": 58,
      "feature_name": "feature_58",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of unique verbs relative to total verbs'\n    doc = nlp(text)\n    total_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB'))\n    return unique_verbs / total_verbs if total_verbs > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00602104676302404,
        "mean_shap": 0.0002688895446175092,
        "std_shap": 0.00680745049104418,
        "min_shap": -0.015179264531374326,
        "max_shap": 0.015242517275736903
      },
      "rank": 58
    },
    {
      "feature_index": 56,
      "feature_name": "feature_56",
      "feature_code": "def feature(text: str) -> float:\n    'Frequency of definite articles (\"the\") in the text'\n    return float(text.lower().count('the'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005965801130712025,
        "mean_shap": 5.861380401015468e-05,
        "std_shap": 0.006706551051421034,
        "min_shap": -0.012103355953373023,
        "max_shap": 0.016137240498073
      },
      "rank": 59
    },
    {
      "feature_index": 60,
      "feature_name": "feature_60",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words in the text (words with three or more syllables)\"\n    complex_words = len(re.findall(r'\\b\\w{3,}\\b', text))\n    total_words = len(text.split())\n    if total_words == 0:\n        return 0.0\n    return float(complex_words) / total_words\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005857247061999766,
        "mean_shap": 0.0004556765318640237,
        "std_shap": 0.007047395778128994,
        "min_shap": -0.02417424994574433,
        "max_shap": 0.011992458899195407
      },
      "rank": 60
    },
    {
      "feature_index": 57,
      "feature_name": "feature_57",
      "feature_code": "def feature(text: str) -> float:\n    'Count of emotional words using VADER'\n    scores = sia.polarity_scores(text)\n    emotional_words_count = scores['pos'] + scores['neg']\n    return float(emotional_words_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005788284181792155,
        "mean_shap": -0.0002503219852232549,
        "std_shap": 0.0068525358640124665,
        "min_shap": -0.015858591264650602,
        "max_shap": 0.014149695479059792
      },
      "rank": 61
    },
    {
      "feature_index": 237,
      "feature_name": "feature_237",
      "feature_code": "def feature(text: str) -> float:\n    \"Named entity density considering unique entity types\"\n    doc = nlp(text)\n    unique_entities = len(set(ent.label_ for ent in doc.ents))\n    return float(unique_entities) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005735319379837874,
        "mean_shap": -0.0005530764186452565,
        "std_shap": 0.00744951397181612,
        "min_shap": -0.028213885325431464,
        "max_shap": 0.02415133449872243
      },
      "rank": 62
    },
    {
      "feature_index": 26,
      "feature_name": "feature_26",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text as a measure of complexity\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return punctuation_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005690210310918793,
        "mean_shap": 0.000902938718832661,
        "std_shap": 0.007978896186401711,
        "min_shap": -0.02948089220339858,
        "max_shap": 0.01753642107760743
      },
      "rank": 63
    },
    {
      "feature_index": 213,
      "feature_name": "feature_213",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return punctuation_count / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00564828500448634,
        "mean_shap": 0.001072132658678968,
        "std_shap": 0.00795464423225992,
        "min_shap": -0.0281254855075791,
        "max_shap": 0.020470597235386606
      },
      "rank": 64
    },
    {
      "feature_index": 27,
      "feature_name": "feature_27",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text as a measure of complexity\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return punctuation_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00553931803013919,
        "mean_shap": 0.0007575869336946702,
        "std_shap": 0.007503524031800667,
        "min_shap": -0.023669797598519775,
        "max_shap": 0.01575536979552398
      },
      "rank": 65
    },
    {
      "feature_index": 69,
      "feature_name": "feature_69",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of words that are adverbs in the text'\n    doc = nlp(text)\n    word_count = len(doc)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count) / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005527486813090823,
        "mean_shap": -0.0004807146816324595,
        "std_shap": 0.006952016742010918,
        "min_shap": -0.015880538607700372,
        "max_shap": 0.0171216956086213
      },
      "rank": 66
    },
    {
      "feature_index": 61,
      "feature_name": "feature_61",
      "feature_code": "def feature(text: str) -> float:\n    'Punctuation-to-word ratio in the text'\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words_count = len(text.split())\n    if words_count == 0:\n        return 0.0\n    return float(punctuation_count) / words_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00548165669187777,
        "mean_shap": -0.0002478488970714163,
        "std_shap": 0.007108611597906767,
        "min_shap": -0.022769138667073942,
        "max_shap": 0.012280856601218054
      },
      "rank": 67
    },
    {
      "feature_index": 67,
      "feature_name": "feature_67",
      "feature_code": "def feature(text: str) -> float:\n    'Punctuation-to-word ratio in the text'\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words_count = len(text.split())\n    if words_count == 0:\n        return 0.0\n    return float(punctuation_count) / words_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005472630934421295,
        "mean_shap": -0.00019396030176481482,
        "std_shap": 0.00666782031113657,
        "min_shap": -0.021293159355022774,
        "max_shap": 0.010885647053610378
      },
      "rank": 68
    },
    {
      "feature_index": 65,
      "feature_name": "feature_65",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of uppercase words in the text'\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return uppercase_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005423416847144252,
        "mean_shap": 0.00017626146843467995,
        "std_shap": 0.006636818177902045,
        "min_shap": -0.01810305320788791,
        "max_shap": 0.012065612995804097
      },
      "rank": 69
    },
    {
      "feature_index": 64,
      "feature_name": "feature_64",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation counts per sentence.\"\n    sentences = re.split(r'[.!?]+', text)\n    punctuation_counts = [sum(1 for c in s if c in string.punctuation) for s in sentences]\n    return float(sum(punctuation_counts)) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005355090879176687,
        "mean_shap": -6.682710852580064e-05,
        "std_shap": 0.006654025286354779,
        "min_shap": -0.021126009383885876,
        "max_shap": 0.009161741503752987
      },
      "rank": 70
    },
    {
      "feature_index": 204,
      "feature_name": "feature_204",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of nouns in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'NOUN'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0053198931214957085,
        "mean_shap": 0.00024375835003080215,
        "std_shap": 0.009007125740604164,
        "min_shap": -0.007478543582628621,
        "max_shap": 0.034731195357232715
      },
      "rank": 71
    },
    {
      "feature_index": 68,
      "feature_name": "feature_68",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words in the text\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0051409817464263645,
        "mean_shap": 0.00010299297191871423,
        "std_shap": 0.0063095013301892175,
        "min_shap": -0.019419208419845548,
        "max_shap": 0.010582282493949229
      },
      "rank": 72
    },
    {
      "feature_index": 47,
      "feature_name": "feature_47",
      "feature_code": "def feature(text: str) -> float:\n    'Sum of unique word lengths in the text'\n    words = set(text.split())\n    if not words:\n        return 0.0\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005077320878264447,
        "mean_shap": 0.000798782300742255,
        "std_shap": 0.005626160490298653,
        "min_shap": -0.01673261285232335,
        "max_shap": 0.012408371164317866
      },
      "rank": 73
    },
    {
      "feature_index": 218,
      "feature_name": "feature_218",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation to total characters\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return (punctuation_count / len(text)) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004941294793342706,
        "mean_shap": 0.0008932002993548191,
        "std_shap": 0.006737642105739166,
        "min_shap": -0.022392010542036794,
        "max_shap": 0.01481282359922034
      },
      "rank": 74
    },
    {
      "feature_index": 224,
      "feature_name": "feature_224",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation characters to total characters\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punctuation_count) / len(text) if text else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004887085389092892,
        "mean_shap": 0.0008927123226887572,
        "std_shap": 0.006398651653302173,
        "min_shap": -0.020064072491895374,
        "max_shap": 0.015295688429164579
      },
      "rank": 75
    },
    {
      "feature_index": 72,
      "feature_name": "feature_72",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of uppercase words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0048028104785207805,
        "mean_shap": 0.00017224526332030855,
        "std_shap": 0.005781440770007463,
        "min_shap": -0.015782669989569086,
        "max_shap": 0.009015624372262564
      },
      "rank": 76
    },
    {
      "feature_index": 219,
      "feature_name": "feature_219",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation to total characters\"\n    if len(text) == 0:\n        return 0.0\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punctuation_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004789108373010617,
        "mean_shap": 0.0008946118468218934,
        "std_shap": 0.00663506972743949,
        "min_shap": -0.02425462623048909,
        "max_shap": 0.014883943048348796
      },
      "rank": 77
    },
    {
      "feature_index": 221,
      "feature_name": "feature_221",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of punctuation in the text\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punctuation_count) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004738105576227594,
        "mean_shap": 0.000843407319276924,
        "std_shap": 0.0064017890761687785,
        "min_shap": -0.021679480151173647,
        "max_shap": 0.016759598542058263
      },
      "rank": 78
    },
    {
      "feature_index": 75,
      "feature_name": "feature_75",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of abstract nouns to total nouns in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    abstract_noun_count = sum(1 for token in doc if token.pos_ == 'NOUN' and token.dep_ in ['nsubj', 'dobj'])\n    return abstract_noun_count / noun_count if noun_count > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004683789777010632,
        "mean_shap": -0.00034097765385545124,
        "std_shap": 0.006108701946629438,
        "min_shap": -0.016079194439976532,
        "max_shap": 0.01842259069577731
      },
      "rank": 79
    },
    {
      "feature_index": 76,
      "feature_name": "feature_76",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words\"\n    words = text.split()\n    upper_count = sum(1 for word in words if word.isupper())\n    return upper_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00466076919297513,
        "mean_shap": 0.0001969222152291194,
        "std_shap": 0.005750099326367774,
        "min_shap": -0.01804015830445375,
        "max_shap": 0.010072040308027734
      },
      "rank": 80
    },
    {
      "feature_index": 215,
      "feature_name": "feature_215",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text\"\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punctuation_count) / total_chars\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004641669861756366,
        "mean_shap": 0.0006879738347004647,
        "std_shap": 0.006399123733353014,
        "min_shap": -0.020481933034472023,
        "max_shap": 0.013890013340745024
      },
      "rank": 81
    },
    {
      "feature_index": 78,
      "feature_name": "feature_78",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of questions in the text\"\n    doc = nlp(text)\n    question_count = sum(1 for sent in doc.sents if sent.text.strip().endswith('?'))\n    return float(question_count) / len(list(doc.sents)) if doc.sents else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004633819274746167,
        "mean_shap": -0.000890294250576918,
        "std_shap": 0.005626192686719253,
        "min_shap": -0.015919577163359483,
        "max_shap": 0.009045720354512874
      },
      "rank": 82
    },
    {
      "feature_index": 216,
      "feature_name": "feature_216",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation characters to total characters\"\n    punctuation_count = sum(1 for char in text if not char.isalnum() and not char.isspace())\n    return float(punctuation_count) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004624545878076705,
        "mean_shap": 0.0008310758684061602,
        "std_shap": 0.006321035658578692,
        "min_shap": -0.022497335073741388,
        "max_shap": 0.015182686070630235
      },
      "rank": 83
    },
    {
      "feature_index": 190,
      "feature_name": "feature_190",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths)) if len(lengths) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004555627954006618,
        "mean_shap": 0.0011461925158823892,
        "std_shap": 0.008645713037412213,
        "min_shap": -0.0717206657861326,
        "max_shap": 0.013036748047367536
      },
      "rank": 84
    },
    {
      "feature_index": 195,
      "feature_name": "feature_195",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004543252870730345,
        "mean_shap": 0.0008649473742706537,
        "std_shap": 0.009265637667600158,
        "min_shap": -0.0807416672661801,
        "max_shap": 0.013736132906584738
      },
      "rank": 85
    },
    {
      "feature_index": 71,
      "feature_name": "feature_71",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words in the text\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0045176568418211415,
        "mean_shap": 6.51212719027045e-05,
        "std_shap": 0.005653472466173625,
        "min_shap": -0.017820785914119813,
        "max_shap": 0.00932863651146083
      },
      "rank": 86
    },
    {
      "feature_index": 80,
      "feature_name": "feature_80",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical diversity based on character n-grams'\n    n_grams = set(text[i:i+2] for i in range(len(text)-1))\n    return float(len(n_grams)) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004500103025545618,
        "mean_shap": 0.0005267902830712604,
        "std_shap": 0.006012192865070224,
        "min_shap": -0.014248043692247532,
        "max_shap": 0.015292916015332843
      },
      "rank": 87
    },
    {
      "feature_index": 24,
      "feature_name": "feature_24",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation marks to total characters in the text\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    total_chars = len(text)\n    return float(punctuation_count) / total_chars if total_chars > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004450632654914258,
        "mean_shap": 0.0006405510073368438,
        "std_shap": 0.006281860892593768,
        "min_shap": -0.021413715190424842,
        "max_shap": 0.014021464738064458
      },
      "rank": 88
    },
    {
      "feature_index": 74,
      "feature_name": "feature_74",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of uppercase words in the text'\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return uppercase_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004448919168727353,
        "mean_shap": 9.544698667923875e-05,
        "std_shap": 0.0054750328091910825,
        "min_shap": -0.016243583272195494,
        "max_shap": 0.008739429121363084
      },
      "rank": 89
    },
    {
      "feature_index": 59,
      "feature_name": "feature_59",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of uppercase words in the text'\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return uppercase_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004323419497611972,
        "mean_shap": 2.392663331835342e-05,
        "std_shap": 0.005301968422037727,
        "min_shap": -0.017676299327204235,
        "max_shap": 0.01048815906057485
      },
      "rank": 90
    },
    {
      "feature_index": 62,
      "feature_name": "feature_62",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words in the text\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004279099959075057,
        "mean_shap": 0.00018519210321961527,
        "std_shap": 0.005302061398273949,
        "min_shap": -0.014768680900619963,
        "max_shap": 0.01099093828322884
      },
      "rank": 91
    },
    {
      "feature_index": 229,
      "feature_name": "feature_229",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of quotes in the text\"\n    quote_count = text.count('\"') + text.count(\"'\")\n    return float(quote_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004210429833437769,
        "mean_shap": 0.00018310596693898505,
        "std_shap": 0.005526826033501275,
        "min_shap": -0.028533533419340246,
        "max_shap": 0.008803575339817576
      },
      "rank": 92
    },
    {
      "feature_index": 63,
      "feature_name": "feature_63",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase to total words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00415452024857636,
        "mean_shap": 0.00016753532403438536,
        "std_shap": 0.005104671854709871,
        "min_shap": -0.015891599378589357,
        "max_shap": 0.00934655107651221
      },
      "rank": 93
    },
    {
      "feature_index": 79,
      "feature_name": "feature_79",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical diversity based on unique bigrams in the text'\n    doc = nlp(text)\n    bigrams = set((doc[i].text, doc[i+1].text) for i in range(len(doc)-1))\n    return float(len(bigrams)) / (len(doc) - 1) if len(doc) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004118309890131486,
        "mean_shap": 0.00029533490593626287,
        "std_shap": 0.0063034056921188394,
        "min_shap": -0.013824181174835875,
        "max_shap": 0.03165418065329105
      },
      "rank": 94
    },
    {
      "feature_index": 70,
      "feature_name": "feature_70",
      "feature_code": "def feature(text: str) -> float:\n    \"Total length of text excluding punctuation\"\n    text_no_punct = re.sub(r'[{}]+'.format(string.punctuation), '', text)\n    return float(len(text_no_punct))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004072318078412044,
        "mean_shap": -4.07877385008608e-05,
        "std_shap": 0.005318363517407216,
        "min_shap": -0.0049159400614942166,
        "max_shap": 0.018292583477680317
      },
      "rank": 95
    },
    {
      "feature_index": 207,
      "feature_name": "feature_207",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of uppercase words to total words\"\n    words = text.split()\n    if not words:\n        return 0.0\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004070400386098602,
        "mean_shap": 0.00020509170407638546,
        "std_shap": 0.004981399010957328,
        "min_shap": -0.014578833086587195,
        "max_shap": 0.009618962277103148
      },
      "rank": 96
    },
    {
      "feature_index": 77,
      "feature_name": "feature_77",
      "feature_code": "def feature(text: str) -> float:\n    'Named entity type diversity as a ratio'\n    doc = nlp(text)\n    unique_entity_types = len(set(ent.label_ for ent in doc.ents))\n    return float(unique_entity_types) / (len(doc.ents) if len(doc.ents) > 0 else 1)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003996107376988003,
        "mean_shap": 0.0004267024543040917,
        "std_shap": 0.00436443863364263,
        "min_shap": -0.008486720591441713,
        "max_shap": 0.008582834635112628
      },
      "rank": 97
    },
    {
      "feature_index": 66,
      "feature_name": "feature_66",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of uppercase words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003979857164612996,
        "mean_shap": 0.000246454723935438,
        "std_shap": 0.004832394583302326,
        "min_shap": -0.01529480965604118,
        "max_shap": 0.00948399046216789
      },
      "rank": 98
    },
    {
      "feature_index": 208,
      "feature_name": "feature_208",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of uppercase words to total words\"\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003868295330681067,
        "mean_shap": 0.0001115637012729746,
        "std_shap": 0.004783369514490625,
        "min_shap": -0.013644621145098816,
        "max_shap": 0.010369942805372751
      },
      "rank": 99
    },
    {
      "feature_index": 84,
      "feature_name": "feature_84",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words (longer than 3 letters) to total words in the text\"\n    words = text.split()\n    complex_word_count = sum(1 for word in words if len(word) > 3)\n    return float(complex_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003782402378454463,
        "mean_shap": -6.31578156168758e-05,
        "std_shap": 0.004895059152958638,
        "min_shap": -0.016769358669495,
        "max_shap": 0.008995138511635893
      },
      "rank": 100
    },
    {
      "feature_index": 30,
      "feature_name": "feature_30",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all unique words in the text\"\n    words = set(text.split())\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0037632473120416403,
        "mean_shap": 0.00020955220292613515,
        "std_shap": 0.004170783546800724,
        "min_shap": -0.014628400780488153,
        "max_shap": 0.008401853282834775
      },
      "rank": 101
    },
    {
      "feature_index": 88,
      "feature_name": "feature_88",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique adjectives in the text'\n    doc = nlp(text)\n    unique_adjectives = len(set(token.text for token in doc if token.pos_ == 'ADJ'))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003706821573024824,
        "mean_shap": -0.0006001819320950128,
        "std_shap": 0.004287292764325271,
        "min_shap": -0.007529810610965571,
        "max_shap": 0.011779492010140297
      },
      "rank": 102
    },
    {
      "feature_index": 230,
      "feature_name": "feature_230",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability index using the Dale-Chall formula\"\n    import textstat\n    return float(textstat.dale_chall_readability_score(text))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0036351156386438305,
        "mean_shap": -0.0007945337039078214,
        "std_shap": 0.00423927576222625,
        "min_shap": -0.009939424235209849,
        "max_shap": 0.011541772882462697
      },
      "rank": 103
    },
    {
      "feature_index": 83,
      "feature_name": "feature_83",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences that contain modal verbs in the text\"\n    doc = nlp(text)\n    modal_verbs = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would'}\n    modal_sentences = sum(1 for sent in doc.sents if any(token.lemma_.lower() in modal_verbs for token in sent))\n    return float(modal_sentences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0036202415216477777,
        "mean_shap": -0.00037631950398803415,
        "std_shap": 0.004776050515183869,
        "min_shap": -0.009945616812185235,
        "max_shap": 0.02126694012999687
      },
      "rank": 104
    },
    {
      "feature_index": 73,
      "feature_name": "feature_73",
      "feature_code": "def feature(text: str) -> float:\n    \"Cohesion measure based on repeated phrases (n-grams)\"\n    ngrams = {text[i:i+2] for i in range(len(text)-1)}  # bi-grams\n    return float(len(ngrams)) / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00361088972566443,
        "mean_shap": 0.0005605355725073312,
        "std_shap": 0.004754944658054987,
        "min_shap": -0.009494821764040255,
        "max_shap": 0.01207488231267232
      },
      "rank": 105
    },
    {
      "feature_index": 82,
      "feature_name": "feature_82",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique named entities in the text divided by total words'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    unique_entities = len(set(ent.text for ent in doc.ents))\n    return unique_entities / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0033372309161706815,
        "mean_shap": -0.0008743730660831797,
        "std_shap": 0.005078332341540057,
        "min_shap": -0.007164113618515399,
        "max_shap": 0.029927695925720234
      },
      "rank": 106
    },
    {
      "feature_index": 235,
      "feature_name": "feature_235",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text using TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003274811076935618,
        "mean_shap": -6.262371917124586e-05,
        "std_shap": 0.0038667657012159892,
        "min_shap": -0.010800644206667978,
        "max_shap": 0.01251628530766254
      },
      "rank": 107
    },
    {
      "feature_index": 236,
      "feature_name": "feature_236",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of words that are verbs\"\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0032726741732193097,
        "mean_shap": -0.00019586256562961814,
        "std_shap": 0.004157443199075273,
        "min_shap": -0.017440857426458182,
        "max_shap": 0.010932463195792792
      },
      "rank": 108
    },
    {
      "feature_index": 243,
      "feature_name": "feature_243",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of numeric tokens in the text\"\n    doc = nlp(text)\n    total_tokens = len(doc)\n    numeric_count = sum(1 for token in doc if token.like_num)\n    return float(numeric_count) / total_tokens if total_tokens > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003113266173567831,
        "mean_shap": -0.00029045736167304296,
        "std_shap": 0.003932037369389113,
        "min_shap": -0.006592508536524289,
        "max_shap": 0.01636239884052239
      },
      "rank": 109
    },
    {
      "feature_index": 91,
      "feature_name": "feature_91",
      "feature_code": "def feature(text: str) -> float:\n    \"Frequency of modal verbs in the text\"\n    doc = nlp(text)\n    modal_verbs = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for token in doc if token.text.lower() in modal_verbs)\n    return float(count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003091671979952389,
        "mean_shap": -0.00016687271009742589,
        "std_shap": 0.004716156651534005,
        "min_shap": -0.005492676981701546,
        "max_shap": 0.02618409268240819
      },
      "rank": 110
    },
    {
      "feature_index": 90,
      "feature_name": "feature_90",
      "feature_code": "def feature(text: str) -> float:\n    'Count of phrases with modal verbs (e.g., can, should, might)'\n    modal_verbs = ['can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would']\n    return float(sum(1 for word in text.lower().split() if word in modal_verbs))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003068684829128589,
        "mean_shap": -0.00019232664295953052,
        "std_shap": 0.0041940145579779,
        "min_shap": -0.006314508082093252,
        "max_shap": 0.016467085507630416
      },
      "rank": 111
    },
    {
      "feature_index": 55,
      "feature_name": "feature_55",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are stopwords in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    stopword_count = sum(1 for word in words if nlp(word)[0].is_stop)\n    return stopword_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0030564248223311527,
        "mean_shap": 0.00013438673393826965,
        "std_shap": 0.004356614214823006,
        "min_shap": -0.005514853970889069,
        "max_shap": 0.020405071897383314
      },
      "rank": 112
    },
    {
      "feature_index": 95,
      "feature_name": "feature_95",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment variance in the text using TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    scores = [sentence.sentiment.polarity for sentence in blob.sentences]\n    return float(statistics.pstdev(scores)) if scores else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0030130156052664194,
        "mean_shap": -0.00015413611502531447,
        "std_shap": 0.003999913585113171,
        "min_shap": -0.011807343200238692,
        "max_shap": 0.015002105800739235
      },
      "rank": 113
    },
    {
      "feature_index": 98,
      "feature_name": "feature_98",
      "feature_code": "def feature(text: str) -> float:\n    \"Density of rare words based on frequency (using a simple threshold)\"\n    rare_word_count = sum(1 for token in nlp(text) if token.is_alpha and len(token) > 6 and not token.is_stop)\n    total_word_count = len(text.split())\n    return (rare_word_count / total_word_count) if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003005305284442464,
        "mean_shap": -0.0002897795925455217,
        "std_shap": 0.004544515901697855,
        "min_shap": -0.008002100130917898,
        "max_shap": 0.0301372017387889
      },
      "rank": 114
    },
    {
      "feature_index": 102,
      "feature_name": "feature_102",
      "feature_code": "def feature(text: str) -> float:\n    'Average syllables per word in the text'\n    words = text.split()\n    syllable_count = sum(textstat.syllable_count(word) for word in words if word.isalpha())\n    return float(syllable_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0029962481762216813,
        "mean_shap": 0.0005385883421692065,
        "std_shap": 0.0043591694234555115,
        "min_shap": -0.02465484755335265,
        "max_shap": 0.012301150329632249
      },
      "rank": 115
    },
    {
      "feature_index": 103,
      "feature_name": "feature_103",
      "feature_code": "def feature(text: str) -> float:\n    'Count of dependent clauses in the text'\n    doc = nlp(text)\n    dependents = sum(1 for token in doc if token.dep_ in {'ccomp', 'acl'})\n    return float(dependents)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00291821040792924,
        "mean_shap": -0.00045288613414624354,
        "std_shap": 0.003503648685153683,
        "min_shap": -0.010274808412409096,
        "max_shap": 0.005022230157574107
      },
      "rank": 116
    },
    {
      "feature_index": 99,
      "feature_name": "feature_99",
      "feature_code": "def feature(text: str) -> float:\n    'Count of dependent clauses in the text'\n    doc = nlp(text)\n    dependents = sum(1 for token in doc if token.dep_ in {'ccomp', 'acl'})\n    return float(dependents)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002898191727702943,
        "mean_shap": -0.0004073732136896235,
        "std_shap": 0.0034566018767496374,
        "min_shap": -0.011690329414904605,
        "max_shap": 0.005579677524576119
      },
      "rank": 117
    },
    {
      "feature_index": 241,
      "feature_name": "feature_241",
      "feature_code": "def feature(text: str) -> float:\n    \"Frequency of the most common word in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    most_common_word_count = Counter(words).most_common(1)[0][1]\n    return float(most_common_word_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0028490301097284956,
        "mean_shap": -0.0004181392283919148,
        "std_shap": 0.0036989346999140187,
        "min_shap": -0.013075841052978664,
        "max_shap": 0.015024761558518543
      },
      "rank": 118
    },
    {
      "feature_index": 210,
      "feature_name": "feature_210",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of words that are stopwords\"\n    words = text.split()\n    if not words:\n        return 0.0\n    stopword_count = sum(1 for word in words if nlp(word)[0].is_stop)\n    return float(stopword_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0028293168457257578,
        "mean_shap": 3.0867546581767854e-06,
        "std_shap": 0.0040600060803692405,
        "min_shap": -0.006329690977637291,
        "max_shap": 0.020371373665093815
      },
      "rank": 119
    },
    {
      "feature_index": 93,
      "feature_name": "feature_93",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of question marks to total sentences in the text\"\n    sentences = text.count('?') + text.count('.') + text.count('!')  \n    if sentences == 0:\n        return 0.0\n    return float(text.count('?')) / sentences\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027852538973478155,
        "mean_shap": -0.0005971462042573994,
        "std_shap": 0.003206715354792382,
        "min_shap": -0.010369242571892975,
        "max_shap": 0.005058669395911437
      },
      "rank": 120
    },
    {
      "feature_index": 231,
      "feature_name": "feature_231",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment score from TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002772209218474703,
        "mean_shap": 1.3661467225983104e-05,
        "std_shap": 0.0033174826719436583,
        "min_shap": -0.009619616204093705,
        "max_shap": 0.010715470717139521
      },
      "rank": 121
    },
    {
      "feature_index": 114,
      "feature_name": "feature_114",
      "feature_code": "def feature(text: str) -> float:\n    \"Density of complex words (3+ syllables) in the text\"\n    complex_words = len([word for word in text.split() if textstat.syllable_count(word) > 3])\n    return float(complex_words) / len(text.split()) if len(text.split()) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002766799677655231,
        "mean_shap": 2.4807581321215606e-05,
        "std_shap": 0.0034210129160484395,
        "min_shap": -0.008769876514650197,
        "max_shap": 0.007006236732937868
      },
      "rank": 122
    },
    {
      "feature_index": 106,
      "feature_name": "feature_106",
      "feature_code": "def feature(text: str) -> float:\n    \"Average character length of nouns in the text\"\n    doc = nlp(text)\n    noun_lengths = [len(token.text) for token in doc if token.pos_ == 'NOUN']\n    return float(sum(noun_lengths)) / len(noun_lengths) if noun_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027484681651878994,
        "mean_shap": 0.0004032862268722051,
        "std_shap": 0.003710833661796914,
        "min_shap": -0.015095182920031477,
        "max_shap": 0.0064132077305064416
      },
      "rank": 123
    },
    {
      "feature_index": 234,
      "feature_name": "feature_234",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text using TextBlob\"\n    from textblob import TextBlob\n    sentiment = TextBlob(text).sentiment\n    return float(sentiment.polarity)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027240472170548774,
        "mean_shap": -0.00011379295527437094,
        "std_shap": 0.0032100602005700374,
        "min_shap": -0.009486550779002043,
        "max_shap": 0.009303012809105058
      },
      "rank": 124
    },
    {
      "feature_index": 87,
      "feature_name": "feature_87",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of all lowercase words in the text\"\n    lowercase_count = sum(1 for word in text.split() if word.islower())\n    return float(lowercase_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002639877871002661,
        "mean_shap": 0.00039861673518090614,
        "std_shap": 0.0032223392985094097,
        "min_shap": -0.007330226079801511,
        "max_shap": 0.012280667430465547
      },
      "rank": 125
    },
    {
      "feature_index": 38,
      "feature_name": "feature_38",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are longer than the average word length\"\n    words = text.split()\n    average_length = sum(len(word) for word in words) / len(words) if words else 0.0\n    long_words_count = sum(1 for word in words if len(word) > average_length)\n    return float(long_words_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0026266483509000535,
        "mean_shap": 0.000581336090924047,
        "std_shap": 0.004730400243654956,
        "min_shap": -0.03307993765078205,
        "max_shap": 0.011290973525577872
      },
      "rank": 126
    },
    {
      "feature_index": 116,
      "feature_name": "feature_116",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of nouns in the text\"\n    doc = nlp(text)\n    noun_lengths = [len(token.text) for token in doc if token.pos_ == 'NOUN']\n    return float(sum(noun_lengths)) / len(noun_lengths) if noun_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002601747546357498,
        "mean_shap": 0.0005099794941735616,
        "std_shap": 0.003528171508903051,
        "min_shap": -0.015518005235092787,
        "max_shap": 0.007793465968481227
      },
      "rank": 127
    },
    {
      "feature_index": 92,
      "feature_name": "feature_92",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase to total characters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    if len(text) == 0:\n        return 0.0\n    return float(uppercase_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002575324914817677,
        "mean_shap": 0.00036133157015539924,
        "std_shap": 0.003181550067380175,
        "min_shap": -0.005018004176807457,
        "max_shap": 0.011240974136466398
      },
      "rank": 128
    },
    {
      "feature_index": 119,
      "feature_name": "feature_119",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of unique lemmas in text\"\n    doc = nlp(text)\n    unique_lemmas = set(token.lemma_.lower() for token in doc if token.is_alpha)\n    if not unique_lemmas:\n        return 0.0\n    return float(sum(len(lemma) for lemma in unique_lemmas)) / len(unique_lemmas)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0025280318670532383,
        "mean_shap": -0.00018822885271507626,
        "std_shap": 0.0035751198538213256,
        "min_shap": -0.011487794147403372,
        "max_shap": 0.012201211279797201
      },
      "rank": 129
    },
    {
      "feature_index": 94,
      "feature_name": "feature_94",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of negations in the text to assess negative framing'\n    neg_words = {'no', 'not', 'never', 'none', 'neither', 'nobody'}\n    doc = nlp(text)\n    neg_count = sum(1 for token in doc if token.text.lower() in neg_words)\n    return float(neg_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0025257561574280455,
        "mean_shap": -0.0003125291663201589,
        "std_shap": 0.0030177110981401234,
        "min_shap": -0.011435747036901821,
        "max_shap": 0.003435679448733544
      },
      "rank": 130
    },
    {
      "feature_index": 118,
      "feature_name": "feature_118",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase letters to total characters'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    total_count = len(text)\n    return float(uppercase_count) / total_count if total_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0024864973625699964,
        "mean_shap": 0.0001231486693776948,
        "std_shap": 0.002918769719899302,
        "min_shap": -0.005360832426436405,
        "max_shap": 0.00928250531357121
      },
      "rank": 131
    },
    {
      "feature_index": 205,
      "feature_name": "feature_205",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of uppercase letters in the text'\n    if len(text) == 0:\n        return 0.0\n    uppercase_count = sum(1 for c in text if c.isupper())\n    return float(uppercase_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0024801509746607713,
        "mean_shap": 0.00035625938291657523,
        "std_shap": 0.003128064129648159,
        "min_shap": -0.004900478787391402,
        "max_shap": 0.013009841493692458
      },
      "rank": 132
    },
    {
      "feature_index": 223,
      "feature_name": "feature_223",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are longer than the average word length\"\n    words = text.split()\n    average_length = sum(len(word) for word in words) / len(words) if words else 0\n    long_word_count = sum(1 for word in words if len(word) > average_length)\n    return float(long_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0024308006499354625,
        "mean_shap": 0.0003228954876163216,
        "std_shap": 0.003675764684255971,
        "min_shap": -0.022666529074182738,
        "max_shap": 0.009275468448624834
      },
      "rank": 133
    },
    {
      "feature_index": 97,
      "feature_name": "feature_97",
      "feature_code": "def feature(text: str) -> float:\n    'Number of distinct words used in the text'\n    words = text.split()\n    unique_word_count = len(set(word.lower() for word in words if word.isalpha()))\n    return float(unique_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0024164712693234653,
        "mean_shap": 9.30307365613841e-05,
        "std_shap": 0.004072687028434474,
        "min_shap": -0.007676304462624057,
        "max_shap": 0.02092013295177102
      },
      "rank": 134
    },
    {
      "feature_index": 117,
      "feature_name": "feature_117",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of unique lemmas in text\"\n    doc = nlp(text)\n    unique_lemmas = set(token.lemma_.lower() for token in doc if token.is_alpha)\n    if not unique_lemmas:\n        return 0.0\n    return float(sum(len(lemma) for lemma in unique_lemmas)) / len(unique_lemmas)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0024030493359560932,
        "mean_shap": -0.0001685339814009055,
        "std_shap": 0.0036041858816164363,
        "min_shap": -0.013657537845548828,
        "max_shap": 0.010925563938135317
      },
      "rank": 135
    },
    {
      "feature_index": 104,
      "feature_name": "feature_104",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of words that are adjacent and match the same part of speech'\n    doc = nlp(text)\n    pos_pairs = sum(1 for token1, token2 in itertools.pairwise(doc) if token1.pos_ == token2.pos_)\n    return float(pos_pairs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0023514863874712967,
        "mean_shap": 0.0003986386337807734,
        "std_shap": 0.002829048316628695,
        "min_shap": -0.00957788620736945,
        "max_shap": 0.007595380864534034
      },
      "rank": 136
    },
    {
      "feature_index": 81,
      "feature_name": "feature_81",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of questions in the text based on question marks\"\n    question_count = text.count('?')\n    total_sentences = text.count('.') + text.count('!') + question_count\n    return float(question_count) / total_sentences if total_sentences > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002350098570481955,
        "mean_shap": -0.0008271643220760516,
        "std_shap": 0.002739641472786205,
        "min_shap": -0.009298512699839384,
        "max_shap": 0.004008962203969523
      },
      "rank": 137
    },
    {
      "feature_index": 253,
      "feature_name": "feature_253",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of uppercase words in the text\"\n    words = text.split()\n    uppercase_word_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002341099443970212,
        "mean_shap": -3.696727462124192e-05,
        "std_shap": 0.002804970164970982,
        "min_shap": -0.008493964914478925,
        "max_shap": 0.007375837577375669
      },
      "rank": 138
    },
    {
      "feature_index": 101,
      "feature_name": "feature_101",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of capital letters in the text'\n    capital_letters = sum(1 for c in text if c.isupper())\n    return float(capital_letters) / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0023189798189149242,
        "mean_shap": 0.0002357884775495952,
        "std_shap": 0.0028377920089941357,
        "min_shap": -0.004348142825001042,
        "max_shap": 0.010604028900529371
      },
      "rank": 139
    },
    {
      "feature_index": 107,
      "feature_name": "feature_107",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of clauses per sentence'\n    sentences = re.split(r'[.!?]+', text)\n    clause_count = sum(len(re.findall(r',', s)) + 1 for s in sentences if s.strip())\n    return clause_count / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0023063917535274606,
        "mean_shap": 0.0001371179240464829,
        "std_shap": 0.0028125080548123955,
        "min_shap": -0.006294918557144468,
        "max_shap": 0.009126411775751752
      },
      "rank": 140
    },
    {
      "feature_index": 239,
      "feature_name": "feature_239",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of numerals in the text\"\n    doc = nlp(text)\n    numeral_count = sum(1 for token in doc if token.is_digit)\n    return float(numeral_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022786528204515343,
        "mean_shap": -0.00040361672964576105,
        "std_shap": 0.0033325622212818733,
        "min_shap": -0.004309364191899202,
        "max_shap": 0.014511776681339636
      },
      "rank": 141
    },
    {
      "feature_index": 209,
      "feature_name": "feature_209",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences that are questions\"\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    question_count = text.count('?')\n    return float(question_count) / sentence_count if sentence_count > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002266704436023958,
        "mean_shap": -0.0005626794427722146,
        "std_shap": 0.0025801376174111237,
        "min_shap": -0.007175325023304064,
        "max_shap": 0.004597100109629785
      },
      "rank": 142
    },
    {
      "feature_index": 259,
      "feature_name": "feature_259",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of three-letter words to total words\"\n    words = text.split()\n    three_letter_count = sum(1 for word in words if len(word) == 3)\n    return float(three_letter_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022450102250380476,
        "mean_shap": -2.9634026706638153e-05,
        "std_shap": 0.0028504472615772554,
        "min_shap": -0.012680594777535447,
        "max_shap": 0.005901720684235697
      },
      "rank": 143
    },
    {
      "feature_index": 113,
      "feature_name": "feature_113",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are recognized named entities in the text\"\n    words = text.split()\n    doc = nlp(text)\n    entity_words = {ent.text.lower() for ent in doc.ents}\n    count_of_entity_words = sum(1 for word in words if word.lower() in entity_words)\n    return float(count_of_entity_words) / len(words) if len(words) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002244342147672673,
        "mean_shap": 0.0001496400250182217,
        "std_shap": 0.0029221709977053445,
        "min_shap": -0.01103369038475977,
        "max_shap": 0.010928186545373711
      },
      "rank": 144
    },
    {
      "feature_index": 109,
      "feature_name": "feature_109",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of unique lemmas in text\"\n    doc = nlp(text)\n    unique_lemmas = set(token.lemma_.lower() for token in doc if token.is_alpha)\n    if not unique_lemmas:\n        return 0.0\n    return float(sum(len(lemma) for lemma in unique_lemmas)) / len(unique_lemmas)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022414758466132415,
        "mean_shap": -0.00012387125446209008,
        "std_shap": 0.0032239718680982334,
        "min_shap": -0.011997336465530145,
        "max_shap": 0.00913702943952672
      },
      "rank": 145
    },
    {
      "feature_index": 275,
      "feature_name": "feature_275",
      "feature_code": "def feature(text: str) -> float:\n    \"Average syllable count per sentence\"\n    sentences = text.split('.')\n    syllable_counts = [textstat.syllable_count(sentence) for sentence in sentences if sentence.strip()]\n    return float(sum(syllable_counts)) / len(syllable_counts) if syllable_counts else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0021848417442528285,
        "mean_shap": 0.0002090306768733408,
        "std_shap": 0.0031622498477104372,
        "min_shap": -0.016784765212466333,
        "max_shap": 0.00854766168242343
      },
      "rank": 146
    },
    {
      "feature_index": 112,
      "feature_name": "feature_112",
      "feature_code": "def feature(text: str) -> float:\n    'Fraction of uppercase letters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    return uppercase_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002155788963327825,
        "mean_shap": 0.00015720230824995627,
        "std_shap": 0.0025625347234112515,
        "min_shap": -0.004228569253663746,
        "max_shap": 0.008259323433745948
      },
      "rank": 147
    },
    {
      "feature_index": 232,
      "feature_name": "feature_232",
      "feature_code": "def feature(text: str) -> float:\n    \"Frequency of the most common character in the text\"\n    if not text:\n        return 0.0\n    most_common_char_count = Counter(text).most_common(1)[0][1]\n    return float(most_common_char_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0021534672622556175,
        "mean_shap": -3.6279382763564086e-05,
        "std_shap": 0.002962296714970035,
        "min_shap": -0.01147199059050914,
        "max_shap": 0.007263019894628736
      },
      "rank": 148
    },
    {
      "feature_index": 105,
      "feature_name": "feature_105",
      "feature_code": "def feature(text: str) -> float:\n    'Count of conjunctions in the text'\n    doc = nlp(text)\n    conjunction_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conjunction_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002118214969168491,
        "mean_shap": -0.0005308733499332466,
        "std_shap": 0.002320250874018406,
        "min_shap": -0.005405819913134088,
        "max_shap": 0.006909577363329807
      },
      "rank": 149
    },
    {
      "feature_index": 124,
      "feature_name": "feature_124",
      "feature_code": "def feature(text: str) -> float:\n    'Count of coordinating conjunctions in the text'\n    doc = nlp(text)\n    conj_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0020850147440129455,
        "mean_shap": -0.0005552899773555818,
        "std_shap": 0.0022898904559309698,
        "min_shap": -0.00542612299198309,
        "max_shap": 0.005069163825455863
      },
      "rank": 150
    },
    {
      "feature_index": 128,
      "feature_name": "feature_128",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of noun phrases to total sentences in the text\"\n    doc = nlp(text)\n    noun_phrases = sum(1 for chunk in doc.noun_chunks)\n    sentence_count = len(list(doc.sents))\n    return float(noun_phrases) / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0020831871629747125,
        "mean_shap": -0.00043068338469065694,
        "std_shap": 0.0025757304545269122,
        "min_shap": -0.007212351437162023,
        "max_shap": 0.005862160946218127
      },
      "rank": 151
    },
    {
      "feature_index": 110,
      "feature_name": "feature_110",
      "feature_code": "def feature(text: str) -> float:\n    'Count of negation words in the text'\n    negation_words = {'not', 'no', 'never', 'nobody', 'nothing', 'nowhere'}\n    words = text.lower().split()\n    negation_count = sum(1 for word in words if word in negation_words)\n    return float(negation_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002075244065014946,
        "mean_shap": -0.00015131977493895767,
        "std_shap": 0.0023783735795495837,
        "min_shap": -0.007837375272537452,
        "max_shap": 0.0031067302252543545
      },
      "rank": 152
    },
    {
      "feature_index": 220,
      "feature_name": "feature_220",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical richness calculated as unique noun ratio\"\n    doc = nlp(text)\n    nouns = [token.lemma_.lower() for token in doc if token.pos_ == 'NOUN']\n    unique_nouns = len(set(nouns))\n    return float(unique_nouns) / len(nouns) if nouns else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0020372003719869194,
        "mean_shap": 4.083850622700481e-05,
        "std_shap": 0.0027147048004126976,
        "min_shap": -0.007248778357386032,
        "max_shap": 0.011502966283252794
      },
      "rank": 153
    },
    {
      "feature_index": 249,
      "feature_name": "feature_249",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of periods in the text\"\n    return float(text.count('.'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001969576840809687,
        "mean_shap": 0.0003282815672873919,
        "std_shap": 0.002386596731863171,
        "min_shap": -0.00445119148345593,
        "max_shap": 0.008185411550047551
      },
      "rank": 154
    },
    {
      "feature_index": 358,
      "feature_name": "feature_358",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences in the text\"\n    return float(len(re.split(r'[.!?]+', text)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019466822326273617,
        "mean_shap": 1.0601590577295725e-05,
        "std_shap": 0.002377892203538945,
        "min_shap": -0.0033420248455689255,
        "max_shap": 0.006844569056793188
      },
      "rank": 155
    },
    {
      "feature_index": 123,
      "feature_name": "feature_123",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words, but only considering sentences longer than a given threshold\"\n    sentences = re.split(r'(?<=[.!?]) +', text)\n    lengths = [len(s.split()) for s in sentences if len(s.split()) > 5]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019157714129875247,
        "mean_shap": 0.00019471118772301953,
        "std_shap": 0.002465193087095341,
        "min_shap": -0.011058391838217475,
        "max_shap": 0.005245441405532261
      },
      "rank": 156
    },
    {
      "feature_index": 350,
      "feature_name": "feature_350",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of uppercase letters in the text\"\n    uppercase_count = sum(1 for c in text if c.isupper())\n    return float(uppercase_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019053217962451382,
        "mean_shap": 0.0004680493953450114,
        "std_shap": 0.00236060109271931,
        "min_shap": -0.006807144435758845,
        "max_shap": 0.012945032375548502
      },
      "rank": 157
    },
    {
      "feature_index": 131,
      "feature_name": "feature_131",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique adjectives in the text\"\n    doc = nlp(text)\n    unique_adjectives = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'ADJ' and token.is_alpha))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0018780521685732342,
        "mean_shap": -0.0003155424007966066,
        "std_shap": 0.0021760290574835968,
        "min_shap": -0.0029636914674162434,
        "max_shap": 0.006576777012543197
      },
      "rank": 158
    },
    {
      "feature_index": 125,
      "feature_name": "feature_125",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns divided by total words\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    total_words = len(doc)\n    return float(unique_nouns) / total_words if total_words > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00180784569450202,
        "mean_shap": -0.00012074734378840058,
        "std_shap": 0.0024399415874457324,
        "min_shap": -0.006113548351652659,
        "max_shap": 0.011117681524214062
      },
      "rank": 159
    },
    {
      "feature_index": 228,
      "feature_name": "feature_228",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words)) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0017655964852215937,
        "mean_shap": -4.932143484167734e-05,
        "std_shap": 0.002587854317616521,
        "min_shap": -0.005884391235802864,
        "max_shap": 0.010849047641784805
      },
      "rank": 160
    },
    {
      "feature_index": 86,
      "feature_name": "feature_86",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique capitalization forms in the text\"\n    words = text.split()\n    unique_capitalized = len(set(word for word in words if word[0].isupper()))\n    return float(unique_capitalized)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001724386164373389,
        "mean_shap": 0.00047273001650444243,
        "std_shap": 0.0025050031979739823,
        "min_shap": -0.004127606536168982,
        "max_shap": 0.018726652202986607
      },
      "rank": 161
    },
    {
      "feature_index": 121,
      "feature_name": "feature_121",
      "feature_code": "def feature(text: str) -> float:\n    'Fraction of uppercase letters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    return uppercase_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016879766292510444,
        "mean_shap": 0.00021549107530947703,
        "std_shap": 0.001991333902157813,
        "min_shap": -0.004926525713845798,
        "max_shap": 0.005371755928726312
      },
      "rank": 162
    },
    {
      "feature_index": 261,
      "feature_name": "feature_261",
      "feature_code": "def feature(text: str) -> float:\n    'Readability score using Flesch-Kincaid Grade Level'\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016823794782047355,
        "mean_shap": -0.0004992065645023528,
        "std_shap": 0.0022433494716341037,
        "min_shap": -0.011698477427158365,
        "max_shap": 0.004648091615258649
      },
      "rank": 163
    },
    {
      "feature_index": 132,
      "feature_name": "feature_132",
      "feature_code": "def feature(text: str) -> float:\n    'Count of conjunctions in the text'\n    conjunctions = ['and', 'but', 'or', 'so', 'for', 'nor', 'yet']\n    count = sum(text.lower().count(conj) for conj in conjunctions)\n    return float(count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016816811595691595,
        "mean_shap": -0.00010690455289870577,
        "std_shap": 0.0021723081871978266,
        "min_shap": -0.005529749978264582,
        "max_shap": 0.009899600405129688
      },
      "rank": 164
    },
    {
      "feature_index": 179,
      "feature_name": "feature_179",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016716814660255985,
        "mean_shap": -8.914040114921595e-05,
        "std_shap": 0.002024870673659166,
        "min_shap": -0.004215285654331898,
        "max_shap": 0.005997157788800768
      },
      "rank": 165
    },
    {
      "feature_index": 126,
      "feature_name": "feature_126",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of sentences that contain a question mark\"\n    sentence_count = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n    question_count = text.count('?')\n    return question_count / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016703995322168825,
        "mean_shap": -0.0010668622738948476,
        "std_shap": 0.0024305800720768912,
        "min_shap": -0.01527353143553192,
        "max_shap": 0.002860012653562808
      },
      "rank": 166
    },
    {
      "feature_index": 191,
      "feature_name": "feature_191",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of paragraphs in the text'\n    paragraphs = text.split('\\n')\n    return float(len(paragraphs))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016695939345517212,
        "mean_shap": -0.000603586921029103,
        "std_shap": 0.002112982560860167,
        "min_shap": -0.005814256480729489,
        "max_shap": 0.00705532886550607
      },
      "rank": 167
    },
    {
      "feature_index": 320,
      "feature_name": "feature_320",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences that contain at least one noun\"\n    doc = nlp(text)\n    sentence_with_noun_count = sum(1 for sent in doc.sents if any(token.pos_ == 'NOUN' for token in sent))\n    return float(sentence_with_noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016540680560947017,
        "mean_shap": 3.499920124662954e-05,
        "std_shap": 0.002054528051188351,
        "min_shap": -0.006602553671560785,
        "max_shap": 0.006446405144897708
      },
      "rank": 168
    },
    {
      "feature_index": 188,
      "feature_name": "feature_188",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001640789450342738,
        "mean_shap": 5.977634388929476e-05,
        "std_shap": 0.0019659388723095757,
        "min_shap": -0.0029050654287010556,
        "max_shap": 0.0055632562529517
      },
      "rank": 169
    },
    {
      "feature_index": 180,
      "feature_name": "feature_180",
      "feature_code": "def feature(text: str) -> float:\n    \"Positive sentiment proportion from VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00163768918592974,
        "mean_shap": -0.0001136459828745873,
        "std_shap": 0.0019506452428098996,
        "min_shap": -0.005386749338356166,
        "max_shap": 0.0038948695250028776
      },
      "rank": 170
    },
    {
      "feature_index": 133,
      "feature_name": "feature_133",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of nouns that are not stopwords'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN' and not token.is_stop)\n    total_noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return noun_count / total_noun_count if total_noun_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001636907180646836,
        "mean_shap": -0.0001434370618151302,
        "std_shap": 0.002624191657161033,
        "min_shap": -0.01637295828555671,
        "max_shap": 0.004824350843790632
      },
      "rank": 171
    },
    {
      "feature_index": 199,
      "feature_name": "feature_199",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016203403917457327,
        "mean_shap": -0.00014567865650628585,
        "std_shap": 0.0019200226795444801,
        "min_shap": -0.004689164027527522,
        "max_shap": 0.00500510113202921
      },
      "rank": 172
    },
    {
      "feature_index": 265,
      "feature_name": "feature_265",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of punctuation characters in the text\"\n    return float(sum(1 for c in text if not c.isalnum() and not c.isspace()))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015982625431424308,
        "mean_shap": -0.0009292399585153116,
        "std_shap": 0.0018436179623974953,
        "min_shap": -0.005709043011694925,
        "max_shap": 0.002597397030326318
      },
      "rank": 173
    },
    {
      "feature_index": 111,
      "feature_name": "feature_111",
      "feature_code": "def feature(text: str) -> float:\n    'Count of conjunctions in the text'\n    doc = nlp(text)\n    conjunction_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conjunction_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015945772985537204,
        "mean_shap": -0.0003839642702942928,
        "std_shap": 0.0018649056369984495,
        "min_shap": -0.005264505052800282,
        "max_shap": 0.004464766028553946
      },
      "rank": 174
    },
    {
      "feature_index": 155,
      "feature_name": "feature_155",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of unique words in the text'\n    unique_words = set(text.split())\n    if not unique_words:\n        return 0.0\n    return float(sum(len(word) for word in unique_words)) / len(unique_words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015867104564334345,
        "mean_shap": 0.00023940694721017133,
        "std_shap": 0.0020503139058534654,
        "min_shap": -0.006925284605633715,
        "max_shap": 0.008271607523302917
      },
      "rank": 175
    },
    {
      "feature_index": 245,
      "feature_name": "feature_245",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique capitalized words\"\n    words = set(word for word in text.split() if word[0].isupper())\n    return float(len(words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015478734461266206,
        "mean_shap": 0.00047416167230388527,
        "std_shap": 0.0026022634152125675,
        "min_shap": -0.0029307564379468546,
        "max_shap": 0.023555238976842835
      },
      "rank": 176
    },
    {
      "feature_index": 222,
      "feature_name": "feature_222",
      "feature_code": "def feature(text: str) -> float:\n    \"Named entity density in the text\"\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    return entity_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015223431260236852,
        "mean_shap": -9.920778472402955e-05,
        "std_shap": 0.0023604242898337844,
        "min_shap": -0.004361961731162726,
        "max_shap": 0.010759298655124807
      },
      "rank": 177
    },
    {
      "feature_index": 225,
      "feature_name": "feature_225",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    if not sentences:\n        return 0.0\n    total_length = sum(len(s) for s in sentences)\n    return float(total_length) / len(sentences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001521778770930734,
        "mean_shap": -0.0007564682538559865,
        "std_shap": 0.0019278786247964364,
        "min_shap": -0.010536986021347797,
        "max_shap": 0.003070372744394586
      },
      "rank": 178
    },
    {
      "feature_index": 238,
      "feature_name": "feature_238",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of nouns to total words\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    total_word_count = len(doc)\n    return float(noun_count) / total_word_count if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014997382539499785,
        "mean_shap": -4.7358041824597086e-05,
        "std_shap": 0.0018938531855951285,
        "min_shap": -0.0059507666226812144,
        "max_shap": 0.004446239597131601
      },
      "rank": 179
    },
    {
      "feature_index": 240,
      "feature_name": "feature_240",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of nouns to total words in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014850360320028668,
        "mean_shap": -3.7883416032135273e-06,
        "std_shap": 0.0018218741692002211,
        "min_shap": -0.005504233506495315,
        "max_shap": 0.003960717623916667
      },
      "rank": 180
    },
    {
      "feature_index": 182,
      "feature_name": "feature_182",
      "feature_code": "def feature(text: str) -> float:\n    'Positive sentiment score using VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001469542304078777,
        "mean_shap": -9.006245088756065e-05,
        "std_shap": 0.001777393080944251,
        "min_shap": -0.00473612274071344,
        "max_shap": 0.003889457064646628
      },
      "rank": 181
    },
    {
      "feature_index": 145,
      "feature_name": "feature_145",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of syllables per word in the text\"\n    def syllable_count(word):\n        return len(re.findall(r'[aeiou]', word.lower()))\n    \n    words = text.split()\n    if not words:\n        return 0.0\n    return sum(syllable_count(word) for word in words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001468295831982667,
        "mean_shap": -1.8472768280010762e-05,
        "std_shap": 0.0020091221272339555,
        "min_shap": -0.004864258014821172,
        "max_shap": 0.008954984565489987
      },
      "rank": 182
    },
    {
      "feature_index": 173,
      "feature_name": "feature_173",
      "feature_code": "def feature(text: str) -> float:\n    \"Positive sentiment proportion from VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014658234335617017,
        "mean_shap": -5.544265153500305e-05,
        "std_shap": 0.0018209217276460837,
        "min_shap": -0.004653552588929432,
        "max_shap": 0.005519091512590218
      },
      "rank": 183
    },
    {
      "feature_index": 100,
      "feature_name": "feature_100",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are longer than 5 characters'\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 5)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014651109873119596,
        "mean_shap": -0.00015598944765843486,
        "std_shap": 0.002073372545702589,
        "min_shap": -0.00403135983658652,
        "max_shap": 0.013033728008679411
      },
      "rank": 184
    },
    {
      "feature_index": 149,
      "feature_name": "feature_149",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of unique words in the text'\n    words = set(text.split())\n    if not words:\n        return 0.0\n    avg_length = sum(len(word) for word in words) / len(words)\n    return float(avg_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001454668294905474,
        "mean_shap": 0.0001267488372240111,
        "std_shap": 0.00204457604146465,
        "min_shap": -0.007710665033172368,
        "max_shap": 0.0069539188994791785
      },
      "rank": 185
    },
    {
      "feature_index": 189,
      "feature_name": "feature_189",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of adjectives to total words in the text'\n    doc = nlp(text)\n    total_words = len([token for token in doc if token.is_alpha])\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adjective_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014200828756808965,
        "mean_shap": -0.00020674234111420158,
        "std_shap": 0.0018990169408071192,
        "min_shap": -0.008212782518964484,
        "max_shap": 0.006061082240744239
      },
      "rank": 186
    },
    {
      "feature_index": 250,
      "feature_name": "feature_250",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of long words (more than 7 characters) to total words\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 7)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014185370528700377,
        "mean_shap": -0.00020065256150789202,
        "std_shap": 0.0018311811511930582,
        "min_shap": -0.005071412487794275,
        "max_shap": 0.006497024556067646
      },
      "rank": 187
    },
    {
      "feature_index": 329,
      "feature_name": "feature_329",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of sentences to total words\"\n    sentences = re.split(r'[.!?]+', text)\n    word_count = len(text.split())\n    sentence_count = len([s for s in sentences if s.strip()])  # Ensuring non-empty sentences\n    return float(sentence_count) / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014169911380358066,
        "mean_shap": 6.335060072031328e-05,
        "std_shap": 0.0019264233010141132,
        "min_shap": -0.007240189093830896,
        "max_shap": 0.003190079129197779
      },
      "rank": 188
    },
    {
      "feature_index": 269,
      "feature_name": "feature_269",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of action verbs in the text\"\n    doc = nlp(text)\n    action_verb_count = sum(1 for token in doc if token.pos_ == 'VERB' and token.dep_ == 'ROOT')\n    return float(action_verb_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001394891118048351,
        "mean_shap": -0.00032494748314774297,
        "std_shap": 0.0017841759219654964,
        "min_shap": -0.003652562532661819,
        "max_shap": 0.007704331267342001
      },
      "rank": 189
    },
    {
      "feature_index": 254,
      "feature_name": "feature_254",
      "feature_code": "def feature(text: str) -> float:\n    'Weighted average word length considering stopwords'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    total_length = sum(len(token.text) for token in doc if token.is_alpha)\n    non_stopwords_count = sum(1 for token in doc if not token.is_stop)\n    if non_stopwords_count == 0:\n        return 0.0\n    return float(total_length) / non_stopwords_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013862638648729506,
        "mean_shap": -0.00014999912591149585,
        "std_shap": 0.001766442139272624,
        "min_shap": -0.004853477234906417,
        "max_shap": 0.005847580027265358
      },
      "rank": 190
    },
    {
      "feature_index": 280,
      "feature_name": "feature_280",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with more than 15 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 15)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013814358633676077,
        "mean_shap": 6.064798458312287e-05,
        "std_shap": 0.0018426942317359132,
        "min_shap": -0.0036129116154943373,
        "max_shap": 0.009083576998395276
      },
      "rank": 191
    },
    {
      "feature_index": 177,
      "feature_name": "feature_177",
      "feature_code": "def feature(text: str) -> float:\n    \"Entropy of word frequency distribution in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    frequency = Counter(words)\n    total_words = len(words)\n    entropy = -sum((count / total_words) * np.log2(count / total_words) for count in frequency.values())\n    return float(entropy)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013813571932654554,
        "mean_shap": 0.0002489929062112559,
        "std_shap": 0.0017898232028300562,
        "min_shap": -0.005069928126589628,
        "max_shap": 0.00646504639547859
      },
      "rank": 192
    },
    {
      "feature_index": 214,
      "feature_name": "feature_214",
      "feature_code": "def feature(text: str) -> float:\n    \"Named entity density in the text\"\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    return float(entity_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013798268826010916,
        "mean_shap": -0.00020050831778225666,
        "std_shap": 0.0020740810000895583,
        "min_shap": -0.004661649741807199,
        "max_shap": 0.010935922612750077
      },
      "rank": 193
    },
    {
      "feature_index": 135,
      "feature_name": "feature_135",
      "feature_code": "def feature(text: str) -> float:\n    'Compound sentence ratio (sentences with conjunctions)'\n    sentences = re.split(r'[.!?]+', text)\n    compound_count = sum(1 for s in sentences if 'and' in s or 'but' in s or 'or' in s)\n    if not sentences:\n        return 0.0\n    return float(compound_count) / len(sentences)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001377833764286406,
        "mean_shap": 0.00034340002109882554,
        "std_shap": 0.0025276965819584505,
        "min_shap": -0.004108988851172931,
        "max_shap": 0.0201783856588819
      },
      "rank": 194
    },
    {
      "feature_index": 172,
      "feature_name": "feature_172",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013765892472562752,
        "mean_shap": -6.552826962432189e-05,
        "std_shap": 0.0016142184514364656,
        "min_shap": -0.003782737346093864,
        "max_shap": 0.003468296002527957
      },
      "rank": 195
    },
    {
      "feature_index": 185,
      "feature_name": "feature_185",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001362940883255216,
        "mean_shap": 9.379179436569102e-05,
        "std_shap": 0.00165423670189089,
        "min_shap": -0.002481141826672375,
        "max_shap": 0.00499269612621937
      },
      "rank": 196
    },
    {
      "feature_index": 142,
      "feature_name": "feature_142",
      "feature_code": "def feature(text: str) -> float:\n    \"Diversity of parts of speech as a ratio\"\n    doc = nlp(text)\n    pos_counts = Counter(token.pos_ for token in doc)\n    pos_diversity = len(pos_counts) / len(doc) if len(doc) > 0 else 0.0\n    return pos_diversity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013228261938983705,
        "mean_shap": -0.00010689777845464642,
        "std_shap": 0.002159493346155846,
        "min_shap": -0.006391127255222769,
        "max_shap": 0.008647282212950207
      },
      "rank": 197
    },
    {
      "feature_index": 266,
      "feature_name": "feature_266",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words\"\n    sentences = text.split('.')\n    if not sentences:\n        return 0.0\n    lengths = [len(s.split()) for s in sentences if s.strip()]\n    return float(sum(lengths)) / len(lengths) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001321704994923186,
        "mean_shap": -0.00015259416733893203,
        "std_shap": 0.0020648252924481514,
        "min_shap": -0.009286197047166916,
        "max_shap": 0.0030749610615505925
      },
      "rank": 198
    },
    {
      "feature_index": 151,
      "feature_name": "feature_151",
      "feature_code": "def feature(text: str) -> float:\n    \"Calculates the ratio of long sentences (greater than 20 words) to total sentences.\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 20)\n    total_sentence_count = len(sentences)\n    return (long_sentence_count / total_sentence_count) if total_sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013183237992243802,
        "mean_shap": 4.8685301647887765e-05,
        "std_shap": 0.0017795721666203303,
        "min_shap": -0.010199445331700098,
        "max_shap": 0.005179141549662659
      },
      "rank": 199
    },
    {
      "feature_index": 159,
      "feature_name": "feature_159",
      "feature_code": "def feature(text: str) -> float:\n    \"Longest word length in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    longest_length = max(len(word) for word in words)\n    return float(longest_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013135120589762492,
        "mean_shap": 0.0004985166991725079,
        "std_shap": 0.0025202146478449984,
        "min_shap": -0.028426634663310284,
        "max_shap": 0.0021779658702904686
      },
      "rank": 200
    },
    {
      "feature_index": 85,
      "feature_name": "feature_85",
      "feature_code": "def feature(text: str) -> float:\n    \"Total number of syllables in the text\"\n    words = text.split()\n    total_syllables = sum(textstat.syllable_count(word) for word in words)\n    return float(total_syllables)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001287213880171743,
        "mean_shap": -2.561724914638804e-06,
        "std_shap": 0.002326451539295879,
        "min_shap": -0.006927341897028592,
        "max_shap": 0.009703479280520162
      },
      "rank": 201
    },
    {
      "feature_index": 170,
      "feature_name": "feature_170",
      "feature_code": "def feature(text: str) -> float:\n    'Longest word length in the text'\n    words = text.split()\n    longest_length = max((len(word) for word in words), default=0)\n    return float(longest_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012853978262477494,
        "mean_shap": 0.0006731632776638854,
        "std_shap": 0.0020575998691993837,
        "min_shap": -0.021132672099212515,
        "max_shap": 0.0024373654597274665
      },
      "rank": 202
    },
    {
      "feature_index": 162,
      "feature_name": "feature_162",
      "feature_code": "def feature(text: str) -> float:\n    'Maximum length of a word in the text'\n    words = text.split()\n    return float(max(len(word) for word in words)) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012842176823568451,
        "mean_shap": 0.0006012421717593021,
        "std_shap": 0.0025773987693255194,
        "min_shap": -0.028046673021517653,
        "max_shap": 0.0023665862010586102
      },
      "rank": 203
    },
    {
      "feature_index": 143,
      "feature_name": "feature_143",
      "feature_code": "def feature(text: str) -> float:\n    'Count of occurrences of the word \"I\"'\n    return float(text.lower().count('i'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001277941847280356,
        "mean_shap": -0.00017422498051318237,
        "std_shap": 0.0018742398279555767,
        "min_shap": -0.012510136466165004,
        "max_shap": 0.0037298876101352677
      },
      "rank": 204
    },
    {
      "feature_index": 146,
      "feature_name": "feature_146",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of the text using VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012452875260975985,
        "mean_shap": -0.00011783037462036072,
        "std_shap": 0.0016490914935045537,
        "min_shap": -0.0026977822943335308,
        "max_shap": 0.006747334043113418
      },
      "rank": 205
    },
    {
      "feature_index": 278,
      "feature_name": "feature_278",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score using VADER\"\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012293448726594018,
        "mean_shap": -0.00013069685932969122,
        "std_shap": 0.0017644754666138475,
        "min_shap": -0.002375163388973242,
        "max_shap": 0.008790144968326575
      },
      "rank": 206
    },
    {
      "feature_index": 193,
      "feature_name": "feature_193",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical richness score based on types vs tokens'\n    doc = nlp(text)\n    unique_tokens = len(set(token.text.lower() for token in doc if token.is_alpha))\n    token_count = len([token for token in doc if token.is_alpha])\n    return float(unique_tokens) / token_count if token_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012183861310976739,
        "mean_shap": 0.00019385437696825748,
        "std_shap": 0.001846365543755132,
        "min_shap": -0.008827900844875261,
        "max_shap": 0.0031530779220086086
      },
      "rank": 207
    },
    {
      "feature_index": 233,
      "feature_name": "feature_233",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001214187198801699,
        "mean_shap": 9.586231895510195e-05,
        "std_shap": 0.0015344493421399284,
        "min_shap": -0.0026156665077743425,
        "max_shap": 0.0046060797495437135
      },
      "rank": 208
    },
    {
      "feature_index": 242,
      "feature_name": "feature_242",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of long words (6 or more characters) to total words\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) >= 6)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012119971796869494,
        "mean_shap": -0.00012946909484116674,
        "std_shap": 0.0016541557039378577,
        "min_shap": -0.003125598125369215,
        "max_shap": 0.00871027322314386
      },
      "rank": 209
    },
    {
      "feature_index": 198,
      "feature_name": "feature_198",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001210656271601531,
        "mean_shap": -6.019574058101449e-05,
        "std_shap": 0.0014620633896542303,
        "min_shap": -0.003035562544643818,
        "max_shap": 0.003893866754573914
      },
      "rank": 210
    },
    {
      "feature_index": 166,
      "feature_name": "feature_166",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (three or more syllables) in the text\"\n    words = re.findall(r'\\w+', text)\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    return complex_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011862380340807769,
        "mean_shap": -0.00012430400821644865,
        "std_shap": 0.0017324056497026257,
        "min_shap": -0.009160937065859182,
        "max_shap": 0.006379167574098819
      },
      "rank": 211
    },
    {
      "feature_index": 187,
      "feature_name": "feature_187",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words in the text\"\n    words = text.split()\n    unique_words = set(word.lower() for word in words)\n    return float(len(unique_words)) / len(words) if words else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011248352143243424,
        "mean_shap": 0.0002246905481009759,
        "std_shap": 0.001397143032865818,
        "min_shap": -0.003234135258125315,
        "max_shap": 0.004233118293631352
      },
      "rank": 212
    },
    {
      "feature_index": 197,
      "feature_name": "feature_197",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (three or more syllables) in the text\"\n    words = re.findall(r'\\w+', text)\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    return complex_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011206511300041657,
        "mean_shap": -1.8525069777207852e-05,
        "std_shap": 0.0017096320344914073,
        "min_shap": -0.010322165038532017,
        "max_shap": 0.006079503855665582
      },
      "rank": 213
    },
    {
      "feature_index": 286,
      "feature_name": "feature_286",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of numeric tokens in the text\"\n    doc = nlp(text)\n    numeric_count = sum(1 for token in doc if token.like_num)\n    return float(numeric_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011190780190719726,
        "mean_shap": 0.0001279138176846882,
        "std_shap": 0.0016015430203392903,
        "min_shap": -0.0018076745997260147,
        "max_shap": 0.008131646783917604
      },
      "rank": 214
    },
    {
      "feature_index": 226,
      "feature_name": "feature_226",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of long words (7 or more characters) in the text\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) >= 7)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001114421385433888,
        "mean_shap": -6.168984865522926e-05,
        "std_shap": 0.0017601247260816224,
        "min_shap": -0.002468910819838871,
        "max_shap": 0.013875047969730747
      },
      "rank": 215
    },
    {
      "feature_index": 307,
      "feature_name": "feature_307",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique words in the text\"\n    words = set(text.lower().split())\n    return float(len(words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011030861842399094,
        "mean_shap": 4.270364572382144e-05,
        "std_shap": 0.0016168554092991835,
        "min_shap": -0.0014868595060821242,
        "max_shap": 0.00756357764378663
      },
      "rank": 216
    },
    {
      "feature_index": 140,
      "feature_name": "feature_140",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity calculated using the ratio of unique words to total words\"\n    words = text.split()\n    unique_words = len(set(word.lower() for word in words))\n    return float(unique_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011011877625916862,
        "mean_shap": 0.00010209264198365331,
        "std_shap": 0.001417311374211162,
        "min_shap": -0.005363601828418813,
        "max_shap": 0.003386619293669571
      },
      "rank": 217
    },
    {
      "feature_index": 194,
      "feature_name": "feature_194",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of words that are longer than 6 characters\"\n    words = text.split()\n    if not words:\n        return 0.0\n    long_words = sum(1 for word in words if len(word) > 6)\n    return float(long_words) / len(words)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010696666206851419,
        "mean_shap": -8.192067207684737e-05,
        "std_shap": 0.0016189645245538245,
        "min_shap": -0.0029761984483151877,
        "max_shap": 0.01075768465404865
      },
      "rank": 218
    },
    {
      "feature_index": 144,
      "feature_name": "feature_144",
      "feature_code": "def feature(text: str) -> float:\n    \"Polarity score indicating overall sentiment of the text\"\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010688811197158293,
        "mean_shap": -5.755075686239737e-05,
        "std_shap": 0.0014672723674433953,
        "min_shap": -0.0025909721287781253,
        "max_shap": 0.005765950220281643
      },
      "rank": 219
    },
    {
      "feature_index": 285,
      "feature_name": "feature_285",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of digits to total characters in the text\"\n    digit_count = sum(1 for c in text if c.isdigit())\n    return float(digit_count) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010627446747074251,
        "mean_shap": -0.0001871765640356027,
        "std_shap": 0.0018877527536929157,
        "min_shap": -0.0026504934905684017,
        "max_shap": 0.013267092526781118
      },
      "rank": 220
    },
    {
      "feature_index": 294,
      "feature_name": "feature_294",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    return float(sum(lengths)) / len(lengths) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010619751292216073,
        "mean_shap": 0.00020829555445833975,
        "std_shap": 0.0015160347388386928,
        "min_shap": -0.006476632230308404,
        "max_shap": 0.0032544311987727967
      },
      "rank": 221
    },
    {
      "feature_index": 184,
      "feature_name": "feature_184",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are longer than 10 characters\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 10)\n    return float(long_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001053090295452753,
        "mean_shap": -5.346716463152671e-05,
        "std_shap": 0.0017207971417788116,
        "min_shap": -0.006459484990603841,
        "max_shap": 0.006826497740176873
      },
      "rank": 222
    },
    {
      "feature_index": 244,
      "feature_name": "feature_244",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of syllables in the text\"\n    total_syllables = sum(textstat.syllable_count(word) for word in text.split())\n    return float(total_syllables)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010506637452468287,
        "mean_shap": 2.418027563498393e-05,
        "std_shap": 0.00202418106315875,
        "min_shap": -0.005105530335224845,
        "max_shap": 0.008884902265137275
      },
      "rank": 223
    },
    {
      "feature_index": 96,
      "feature_name": "feature_96",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words\"\n    sentences = text.split('.')\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001048371646479399,
        "mean_shap": 7.818839360981915e-05,
        "std_shap": 0.0014384250063525748,
        "min_shap": -0.005895429322101688,
        "max_shap": 0.002232554450435074
      },
      "rank": 224
    },
    {
      "feature_index": 136,
      "feature_name": "feature_136",
      "feature_code": "def feature(text: str) -> float:\n    \"Diversity of parts of speech as a ratio\"\n    doc = nlp(text)\n    pos_counts = Counter(token.pos_ for token in doc)\n    pos_diversity = len(pos_counts) / len(doc) if len(doc) > 0 else 0.0\n    return pos_diversity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010374115523447234,
        "mean_shap": 0.00010025413786364305,
        "std_shap": 0.0016625433949051538,
        "min_shap": -0.004028734242678128,
        "max_shap": 0.006547561833067577
      },
      "rank": 225
    },
    {
      "feature_index": 167,
      "feature_name": "feature_167",
      "feature_code": "def feature(text: str) -> float:\n    \"Measures the cohesiveness of the text based on repeated phrases (n-grams).\"\n    from collections import Counter\n    n_grams = [text[i:i+2] for i in range(len(text)-1)]\n    most_common_ngrams = Counter(n_grams).most_common()\n    return float(len(most_common_ngrams))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010279219625451896,
        "mean_shap": -0.0002226081340938571,
        "std_shap": 0.001464432828141219,
        "min_shap": -0.006158943121205184,
        "max_shap": 0.006038743848169604
      },
      "rank": 226
    },
    {
      "feature_index": 174,
      "feature_name": "feature_174",
      "feature_code": "def feature(text: str) -> float:\n    'Density of specific keywords (e.g. \"recommend\") per total words'\n    keyword = 'recommend'\n    keyword_count = text.lower().count(keyword)\n    total_words = len(text.split())\n    return (keyword_count / total_words) if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010199837589179753,
        "mean_shap": -0.00031010102070120157,
        "std_shap": 0.001399573014001279,
        "min_shap": -0.0017673317578407242,
        "max_shap": 0.007272480345447542
      },
      "rank": 227
    },
    {
      "feature_index": 163,
      "feature_name": "feature_163",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words in the text\"\n    words = text.split()\n    stopwords_count = sum(1 for word in words if nlp.vocab[word].is_stop)\n    if len(words) == 0:\n        return 0.0\n    return float(stopwords_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00098527603422427,
        "mean_shap": -0.00023311531195121344,
        "std_shap": 0.0012650749343471259,
        "min_shap": -0.006165177633434293,
        "max_shap": 0.002628082941874533
      },
      "rank": 228
    },
    {
      "feature_index": 176,
      "feature_name": "feature_176",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of words that are long (7 or more characters)\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) >= 7)\n    return long_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009806565039203153,
        "mean_shap": -8.186148768705336e-05,
        "std_shap": 0.001591126781810662,
        "min_shap": -0.0028069680115298014,
        "max_shap": 0.011765338220712133
      },
      "rank": 229
    },
    {
      "feature_index": 122,
      "feature_name": "feature_122",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009762833310090194,
        "mean_shap": 0.00033334018171656633,
        "std_shap": 0.0013011590904084725,
        "min_shap": -0.002546210624378153,
        "max_shap": 0.005839351956854918
      },
      "rank": 230
    },
    {
      "feature_index": 181,
      "feature_name": "feature_181",
      "feature_code": "def feature(text: str) -> float:\n    'Sentence length variability (standard deviation of sentence lengths)'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009759686067346845,
        "mean_shap": 6.2602067974076945e-06,
        "std_shap": 0.001226620638413049,
        "min_shap": -0.004067631480490033,
        "max_shap": 0.0031514926171936973
      },
      "rank": 231
    },
    {
      "feature_index": 171,
      "feature_name": "feature_171",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.strip()]\n    return float(statistics.pstdev(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009742718790435018,
        "mean_shap": 2.154814217672712e-05,
        "std_shap": 0.0012085961913568717,
        "min_shap": -0.004035662024377696,
        "max_shap": 0.0029299067235178883
      },
      "rank": 232
    },
    {
      "feature_index": 227,
      "feature_name": "feature_227",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    return float(statistics.pstdev(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009741319102199449,
        "mean_shap": -3.4869483425244335e-05,
        "std_shap": 0.0011882345249985488,
        "min_shap": -0.003768552223533375,
        "max_shap": 0.002618520730546372
      },
      "rank": 233
    },
    {
      "feature_index": 268,
      "feature_name": "feature_268",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000962114533763767,
        "mean_shap": -0.00024017954086369883,
        "std_shap": 0.0011868962976512947,
        "min_shap": -0.0019211905577051723,
        "max_shap": 0.004437207188522316
      },
      "rank": 234
    },
    {
      "feature_index": 291,
      "feature_name": "feature_291",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique named entities in the text\"\n    doc = nlp(text)\n    entity_count = len(set(ent.text for ent in doc.ents))\n    return float(entity_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009555618268707956,
        "mean_shap": 0.00031894325251861233,
        "std_shap": 0.0013483827262505382,
        "min_shap": -0.002661278024985954,
        "max_shap": 0.006194559873428853
      },
      "rank": 235
    },
    {
      "feature_index": 292,
      "feature_name": "feature_292",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of sentences to total words\"\n    word_count = len(text.split())\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    return float(sentence_count) / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009432061602053296,
        "mean_shap": -0.00016703878148548946,
        "std_shap": 0.0011796493841936435,
        "min_shap": -0.004634089737706534,
        "max_shap": 0.002304413226378923
      },
      "rank": 236
    },
    {
      "feature_index": 178,
      "feature_name": "feature_178",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    return float(statistics.pstdev(lengths)) if lengths else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000932265313379681,
        "mean_shap": 5.757835986176202e-05,
        "std_shap": 0.0012002890415182019,
        "min_shap": -0.0034707402627526386,
        "max_shap": 0.0029722119702709072
      },
      "rank": 237
    },
    {
      "feature_index": 160,
      "feature_name": "feature_160",
      "feature_code": "def feature(text: str) -> float:\n    'Complexity score based on sentence length variability'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if len(lengths) < 2:\n        return 0.0\n    return float(np.std(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009261175350577308,
        "mean_shap": -6.4515709090338335e-06,
        "std_shap": 0.0011419580689042872,
        "min_shap": -0.003428933970782889,
        "max_shap": 0.0028514591799831002
      },
      "rank": 238
    },
    {
      "feature_index": 274,
      "feature_name": "feature_274",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique named entities in the text\"\n    doc = nlp(text)\n    return float(len({ent.text for ent in doc.ents}))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009257608496600356,
        "mean_shap": 0.000310169674950044,
        "std_shap": 0.0013709164528298992,
        "min_shap": -0.0022264631059670968,
        "max_shap": 0.009111518292428799
      },
      "rank": 239
    },
    {
      "feature_index": 282,
      "feature_name": "feature_282",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009181684496164268,
        "mean_shap": -0.00014882737940221665,
        "std_shap": 0.0011796401754265362,
        "min_shap": -0.0022957537826222654,
        "max_shap": 0.00464833684498021
      },
      "rank": 240
    },
    {
      "feature_index": 157,
      "feature_name": "feature_157",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity score of the text using TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.subjectivity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000914142729488056,
        "mean_shap": 1.6964465112818986e-05,
        "std_shap": 0.0011589239596790767,
        "min_shap": -0.0037974548991996683,
        "max_shap": 0.0018264674525007667
      },
      "rank": 241
    },
    {
      "feature_index": 165,
      "feature_name": "feature_165",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths (in words)\"\n    sentences = re.split(r'[.!?]+', text)\n    sentence_lengths = [len(s.split()) for s in sentences if s.strip()]\n    return float(statistics.pstdev(sentence_lengths)) if sentence_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009017929092601007,
        "mean_shap": 2.565152644647493e-07,
        "std_shap": 0.0011283814147920851,
        "min_shap": -0.002963776719691346,
        "max_shap": 0.0031343076706145975
      },
      "rank": 242
    },
    {
      "feature_index": 248,
      "feature_name": "feature_248",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique punctuation marks used\"\n    unique_punctuation = set(c for c in text if c in string.punctuation)\n    return float(len(unique_punctuation))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000899279339062665,
        "mean_shap": 0.00031950732888291365,
        "std_shap": 0.0012204800621955262,
        "min_shap": -0.006429114796180956,
        "max_shap": 0.00245151134473339
      },
      "rank": 243
    },
    {
      "feature_index": 281,
      "feature_name": "feature_281",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique named entities in the text\"\n    doc = nlp(text)\n    unique_entities = {ent.text for ent in doc.ents}\n    return float(len(unique_entities))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008980167989220889,
        "mean_shap": 0.00019188179716383844,
        "std_shap": 0.0012369955554597518,
        "min_shap": -0.002693945371249252,
        "max_shap": 0.0065528982535420135
      },
      "rank": 244
    },
    {
      "feature_index": 247,
      "feature_name": "feature_247",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of different punctuation marks used in the text\"\n    punctuation_marks = set(c for c in text if c in string.punctuation)\n    return float(len(punctuation_marks))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008973305078143925,
        "mean_shap": 0.00022736960301408203,
        "std_shap": 0.0012787976049190004,
        "min_shap": -0.006752929529285056,
        "max_shap": 0.002527969230172919
      },
      "rank": 245
    },
    {
      "feature_index": 272,
      "feature_name": "feature_272",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique named entities in the text\"\n    doc = nlp(text)\n    unique_entities = set(ent.text for ent in doc.ents)\n    return float(len(unique_entities))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008969030537185816,
        "mean_shap": 0.00030176604182483415,
        "std_shap": 0.001232134280796248,
        "min_shap": -0.0022545621980712147,
        "max_shap": 0.005206983693137315
      },
      "rank": 246
    },
    {
      "feature_index": 255,
      "feature_name": "feature_255",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stop words to total words\"\n    words = text.split()\n    stop_word_count = sum(1 for word in words if word.lower() in nlp.Defaults.stop_words)\n    return float(stop_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008912137796415647,
        "mean_shap": -0.00024882305834441866,
        "std_shap": 0.0011167563117309362,
        "min_shap": -0.005028958355286657,
        "max_shap": 0.002542462067208514
      },
      "rank": 247
    },
    {
      "feature_index": 161,
      "feature_name": "feature_161",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment score from VADER sentiment analysis'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008878816409158898,
        "mean_shap": -0.00012403857765881196,
        "std_shap": 0.001180675737093648,
        "min_shap": -0.001845864223633311,
        "max_shap": 0.005129360296387659
      },
      "rank": 248
    },
    {
      "feature_index": 139,
      "feature_name": "feature_139",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score from VADER sentiment analysis\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000883205023282404,
        "mean_shap": -0.00010043045794323528,
        "std_shap": 0.0012510406676085645,
        "min_shap": -0.001980762750166878,
        "max_shap": 0.0063634342899052425
      },
      "rank": 249
    },
    {
      "feature_index": 169,
      "feature_name": "feature_169",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are stop words'\n    words = text.split()\n    if not words:\n        return 0.0\n    stopwords_count = sum(1 for word in words if word.lower() in nlp.Defaults.stop_words)\n    return stopwords_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008816325911590468,
        "mean_shap": -0.00011300890908572091,
        "std_shap": 0.0010896077609202995,
        "min_shap": -0.0037936374379176204,
        "max_shap": 0.0023730470795794217
      },
      "rank": 250
    },
    {
      "feature_index": 288,
      "feature_name": "feature_288",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of adjectives in the text\"\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008690377543183787,
        "mean_shap": -0.00011149858070811988,
        "std_shap": 0.0013041498939898681,
        "min_shap": -0.0063612456439757084,
        "max_shap": 0.0029566286340508225
      },
      "rank": 251
    },
    {
      "feature_index": 134,
      "feature_name": "feature_134",
      "feature_code": "def feature(text: str) -> float:\n    'Maximum sentence length measured in words'\n    sentences = re.split(r'[.!?]+', text)\n    max_length = max((len(s.split()) for s in sentences if s.split()), default=0)\n    return float(max_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008600772406631234,
        "mean_shap": 9.742545333299312e-05,
        "std_shap": 0.0011580408405983634,
        "min_shap": -0.004175849856105954,
        "max_shap": 0.0035706826281616632
      },
      "rank": 252
    },
    {
      "feature_index": 141,
      "feature_name": "feature_141",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of the text to gauge emotional content'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008546254282339539,
        "mean_shap": -0.00014479171803145273,
        "std_shap": 0.0011068001944028816,
        "min_shap": -0.001939171517872617,
        "max_shap": 0.004100977283846156
      },
      "rank": 253
    },
    {
      "feature_index": 153,
      "feature_name": "feature_153",
      "feature_code": "def feature(text: str) -> float:\n    'Sentiment score using VADER sentiment analysis'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008459903630346927,
        "mean_shap": -0.00019366781685344606,
        "std_shap": 0.0010679172446626671,
        "min_shap": -0.002123464226703878,
        "max_shap": 0.003940156097554238
      },
      "rank": 254
    },
    {
      "feature_index": 258,
      "feature_name": "feature_258",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008449665917586159,
        "mean_shap": 0.0002985660966264553,
        "std_shap": 0.001143114705712571,
        "min_shap": -0.002309357771050338,
        "max_shap": 0.005560935137982123
      },
      "rank": 255
    },
    {
      "feature_index": 284,
      "feature_name": "feature_284",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity from VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008433955828658874,
        "mean_shap": -0.00011129082578938358,
        "std_shap": 0.0011107176996609164,
        "min_shap": -0.0019039514481649829,
        "max_shap": 0.004119118316857291
      },
      "rank": 256
    },
    {
      "feature_index": 287,
      "feature_name": "feature_287",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of sentences to words in the text\"\n    word_count = len(text.split())\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    return float(sentence_count) / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008426329662911477,
        "mean_shap": -0.00017145863742123142,
        "std_shap": 0.001031429892781047,
        "min_shap": -0.003407278791993269,
        "max_shap": 0.002051816327913931
      },
      "rank": 257
    },
    {
      "feature_index": 252,
      "feature_name": "feature_252",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008396620949759621,
        "mean_shap": 0.00025134504158010013,
        "std_shap": 0.0011322328000379785,
        "min_shap": -0.00233341028018354,
        "max_shap": 0.004910590401697999
      },
      "rank": 258
    },
    {
      "feature_index": 150,
      "feature_name": "feature_150",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score from VADER sentiment analysis\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008340693360058243,
        "mean_shap": -0.00011966797604598509,
        "std_shap": 0.0011162093643634302,
        "min_shap": -0.0026262188966333865,
        "max_shap": 0.0044726523451871225
      },
      "rank": 259
    },
    {
      "feature_index": 328,
      "feature_name": "feature_328",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    return float(sum(1 for word in words if word[0].isupper()))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008267854072390858,
        "mean_shap": 9.549387725902932e-05,
        "std_shap": 0.001185392656425768,
        "min_shap": -0.0022414128644945215,
        "max_shap": 0.006331524338956825
      },
      "rank": 260
    },
    {
      "feature_index": 262,
      "feature_name": "feature_262",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008255172259535137,
        "mean_shap": -7.836878489671624e-05,
        "std_shap": 0.0010914954828936043,
        "min_shap": -0.0016562194365371335,
        "max_shap": 0.004546076991722009
      },
      "rank": 261
    },
    {
      "feature_index": 276,
      "feature_name": "feature_276",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008249358104736896,
        "mean_shap": -9.50787571606543e-05,
        "std_shap": 0.0010917019840456052,
        "min_shap": -0.001899653906326665,
        "max_shap": 0.004195161945471857
      },
      "rank": 262
    },
    {
      "feature_index": 273,
      "feature_name": "feature_273",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique named entities in the text'\n    doc = nlp(text)\n    return float(len(set(ent.text for ent in doc.ents)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007992441862727616,
        "mean_shap": 0.00022027506317674956,
        "std_shap": 0.0011638474823713965,
        "min_shap": -0.002116772132702374,
        "max_shap": 0.006567633004912163
      },
      "rank": 263
    },
    {
      "feature_index": 164,
      "feature_name": "feature_164",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are stop words'\n    words = text.split()\n    if not words:\n        return 0.0\n    stopwords_count = sum(1 for word in words if word.lower() in nlp.Defaults.stop_words)\n    return stopwords_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007973544923563987,
        "mean_shap": -0.0001917832173880928,
        "std_shap": 0.0010720640651839775,
        "min_shap": -0.0041832595410016896,
        "max_shap": 0.002139368461828424
      },
      "rank": 264
    },
    {
      "feature_index": 344,
      "feature_name": "feature_344",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words (more than two syllables) to total words\"\n    words = text.split()\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) > 2)\n    return float(complex_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007857842933966386,
        "mean_shap": -3.790627471880881e-05,
        "std_shap": 0.0010161808646291992,
        "min_shap": -0.003080145740026754,
        "max_shap": 0.0033663520442199844
      },
      "rank": 265
    },
    {
      "feature_index": 315,
      "feature_name": "feature_315",
      "feature_code": "def feature(text: str) -> float:\n    \"Complex word count (words with 3 or more syllables)\"\n    words = text.split()\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    return float(complex_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007842317375342816,
        "mean_shap": 6.502884132215212e-05,
        "std_shap": 0.0011758807432115544,
        "min_shap": -0.003975352609426857,
        "max_shap": 0.005613493536228471
      },
      "rank": 266
    },
    {
      "feature_index": 271,
      "feature_name": "feature_271",
      "feature_code": "def feature(text: str) -> float:\n    \"Max length of sentences in words\"\n    sentences = re.split(r'[.!?]+', text)\n    max_length = max(len(s.split()) for s in sentences if s.split()) if sentences else 0\n    return float(max_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007782652557342246,
        "mean_shap": 5.650578444872796e-05,
        "std_shap": 0.0010971181766742946,
        "min_shap": -0.0036457661893865194,
        "max_shap": 0.002899648529243303
      },
      "rank": 267
    },
    {
      "feature_index": 89,
      "feature_name": "feature_89",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of sentences to total words in the text\"\n    sentence_count = len(re.findall(r'[.!?]', text))\n    total_words = len(text.split())\n    return float(sentence_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007633351010348116,
        "mean_shap": -0.00011551410029657482,
        "std_shap": 0.0009339002095861488,
        "min_shap": -0.0037654496009057294,
        "max_shap": 0.0020496594850109707
      },
      "rank": 268
    },
    {
      "feature_index": 385,
      "feature_name": "feature_385",
      "feature_code": "def feature(text: str) -> float:\n    \"Average syllable count per word in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_counts = [textstat.syllable_count(word) for word in words]\n    return float(sum(syllable_counts)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007544921757919374,
        "mean_shap": -0.00016798814247178496,
        "std_shap": 0.000921875158009835,
        "min_shap": -0.0029419353798910636,
        "max_shap": 0.001862430339543825
      },
      "rank": 269
    },
    {
      "feature_index": 127,
      "feature_name": "feature_127",
      "feature_code": "def feature(text: str) -> float:\n    'Sentiment polarity score of the text'\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007391809457127681,
        "mean_shap": -9.439553159096381e-05,
        "std_shap": 0.001035891550789051,
        "min_shap": -0.0015958936299449906,
        "max_shap": 0.005427667547172072
      },
      "rank": 270
    },
    {
      "feature_index": 186,
      "feature_name": "feature_186",
      "feature_code": "def feature(text: str) -> float:\n    'Count of exclamatory sentences in the text'\n    exclam_sentences = text.count('!')\n    total_sentences = len(re.findall(r'[.!?]', text)) + (1 if exclam_sentences > 0 else 0)\n    return float(exclam_sentences) / total_sentences if total_sentences > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007345649344939407,
        "mean_shap": -0.0002501121104867847,
        "std_shap": 0.000859562690297896,
        "min_shap": -0.0020308743717785275,
        "max_shap": 0.003735241033653289
      },
      "rank": 271
    },
    {
      "feature_index": 267,
      "feature_name": "feature_267",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s.strip()]\n    return float(sum(lengths)) / len(lengths) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007329655077667683,
        "mean_shap": 0.00010537525569668011,
        "std_shap": 0.0009510869649301081,
        "min_shap": -0.0035813582391390824,
        "max_shap": 0.0017267102186682172
      },
      "rank": 272
    },
    {
      "feature_index": 289,
      "feature_name": "feature_289",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of adjectives to total words\"\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007312656512716146,
        "mean_shap": 0.00010239706172253713,
        "std_shap": 0.0009681909359485571,
        "min_shap": -0.0033850587190499388,
        "max_shap": 0.003707309001934818
      },
      "rank": 273
    },
    {
      "feature_index": 299,
      "feature_name": "feature_299",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of prepositions in the text\"\n    doc = nlp(text)\n    prep_count = sum(1 for token in doc if token.pos_ == 'ADP')\n    return float(prep_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007286591052123092,
        "mean_shap": 0.00039497247791392604,
        "std_shap": 0.000859183249153539,
        "min_shap": -0.004725257149788694,
        "max_shap": 0.0028674372776471583
      },
      "rank": 274
    },
    {
      "feature_index": 290,
      "feature_name": "feature_290",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of prepositions in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADP'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007204067794140226,
        "mean_shap": 0.00040252924302112514,
        "std_shap": 0.000819303009567083,
        "min_shap": -0.0032908641818607713,
        "max_shap": 0.0031634220520248726
      },
      "rank": 275
    },
    {
      "feature_index": 279,
      "feature_name": "feature_279",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007185400727407244,
        "mean_shap": -7.702275251258456e-05,
        "std_shap": 0.001013994497254618,
        "min_shap": -0.0017947314797276414,
        "max_shap": 0.005111851431293343
      },
      "rank": 276
    },
    {
      "feature_index": 251,
      "feature_name": "feature_251",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that start with a vowel\"\n    words = text.split()\n    vowel_count = sum(1 for word in words if word and word[0].lower() in 'aeiou')\n    return float(vowel_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007116003414744834,
        "mean_shap": -4.824888850903218e-05,
        "std_shap": 0.0009107647296388333,
        "min_shap": -0.0028553584829006285,
        "max_shap": 0.0028552778893252374
      },
      "rank": 277
    },
    {
      "feature_index": 147,
      "feature_name": "feature_147",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity score from TextBlob for the text\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return blob.sentiment.subjectivity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007043386826349775,
        "mean_shap": 4.3273049545992775e-06,
        "std_shap": 0.0009157530802374795,
        "min_shap": -0.0035312400243586515,
        "max_shap": 0.0018979176598336567
      },
      "rank": 278
    },
    {
      "feature_index": 386,
      "feature_name": "feature_386",
      "feature_code": "def feature(text: str) -> float:\n    \"Average syllables per word in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    total_syllables = sum(textstat.syllable_count(word) for word in words)\n    return float(total_syllables) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006965557329575556,
        "mean_shap": -6.147173030898624e-05,
        "std_shap": 0.0008744905500780152,
        "min_shap": -0.0023857663694037885,
        "max_shap": 0.0025860595078250064
      },
      "rank": 279
    },
    {
      "feature_index": 168,
      "feature_name": "feature_168",
      "feature_code": "def feature(text: str) -> float:\n    \"Measure of emotional tone using sentiment scores\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006961440051668385,
        "mean_shap": -7.432068233097573e-05,
        "std_shap": 0.0009500878595612348,
        "min_shap": -0.0019451555328402874,
        "max_shap": 0.004202701747931746
      },
      "rank": 280
    },
    {
      "feature_index": 313,
      "feature_name": "feature_313",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of stop words in the text\"\n    doc = nlp(text)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return float(stop_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006959333146764735,
        "mean_shap": -0.0002339888024186959,
        "std_shap": 0.0007999398832358063,
        "min_shap": -0.002050453962340311,
        "max_shap": 0.0020355006767576772
      },
      "rank": 281
    },
    {
      "feature_index": 314,
      "feature_name": "feature_314",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000693408287683353,
        "mean_shap": 2.4402468278890522e-05,
        "std_shap": 0.000979215734405713,
        "min_shap": -0.0022364235693977962,
        "max_shap": 0.004462943599562642
      },
      "rank": 282
    },
    {
      "feature_index": 296,
      "feature_name": "feature_296",
      "feature_code": "def feature(text: str) -> float:\n    \"Total character count in the text\"\n    return float(len(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006903112804474091,
        "mean_shap": -0.00010883673719641635,
        "std_shap": 0.0011828649603919765,
        "min_shap": -0.001953149649099797,
        "max_shap": 0.005398539447807905
      },
      "rank": 283
    },
    {
      "feature_index": 158,
      "feature_name": "feature_158",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006887360296107639,
        "mean_shap": 1.3351964548790416e-07,
        "std_shap": 0.001133113825687386,
        "min_shap": -0.004238169965823364,
        "max_shap": 0.00427996123307254
      },
      "rank": 284
    },
    {
      "feature_index": 293,
      "feature_name": "feature_293",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity from TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.subjectivity)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006870054391229946,
        "mean_shap": -4.599035129504142e-05,
        "std_shap": 0.0009375739141002897,
        "min_shap": -0.003869325414937358,
        "max_shap": 0.0019146188575982176
      },
      "rank": 285
    },
    {
      "feature_index": 308,
      "feature_name": "feature_308",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with exclamation marks\"\n    exclamation_count = text.count('!')\n    sentences = re.split(r'[.!?]+', text)\n    return float(exclamation_count) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006843046204031347,
        "mean_shap": -0.00015268350422911292,
        "std_shap": 0.0008432840936368677,
        "min_shap": -0.002193810040717497,
        "max_shap": 0.002742154824738778
      },
      "rank": 286
    },
    {
      "feature_index": 327,
      "feature_name": "feature_327",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words repeated more than once\"\n    words = text.split()\n    word_count = Counter(words)\n    repeat_count = sum(1 for count in word_count.values() if count > 1)\n    return float(repeat_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006834574303500881,
        "mean_shap": 0.00014981861671708237,
        "std_shap": 0.0008561372414449511,
        "min_shap": -0.0022422599059327675,
        "max_shap": 0.0026624262285920566
      },
      "rank": 287
    },
    {
      "feature_index": 394,
      "feature_name": "feature_394",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of different sentence lengths\"\n    sentences = re.split(r'[.!?]+', text)\n    sentence_lengths = set(len(s.split()) for s in sentences if s.strip())\n    return float(len(sentence_lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006811306244630086,
        "mean_shap": 1.5487012489086124e-06,
        "std_shap": 0.000925260212269163,
        "min_shap": -0.001582353227800697,
        "max_shap": 0.003606181692421151
      },
      "rank": 288
    },
    {
      "feature_index": 316,
      "feature_name": "feature_316",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006794314453406069,
        "mean_shap": 5.926648399282751e-05,
        "std_shap": 0.0009446112300028173,
        "min_shap": -0.0014836153419029762,
        "max_shap": 0.003978039384719922
      },
      "rank": 289
    },
    {
      "feature_index": 304,
      "feature_name": "feature_304",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in characters\"\n    doc = nlp(text)\n    if not doc.ents:\n        return 0.0\n    return float(sum(len(ent.text) for ent in doc.ents)) / len(doc.ents)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006768902135354143,
        "mean_shap": -7.94781912245469e-05,
        "std_shap": 0.0009924388719080906,
        "min_shap": -0.0026295403789714546,
        "max_shap": 0.004976932940438232
      },
      "rank": 290
    },
    {
      "feature_index": 260,
      "feature_name": "feature_260",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s]\n    return float(sum(lengths)) / len(lengths) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006743873884517771,
        "mean_shap": 9.983890017756422e-05,
        "std_shap": 0.0009535939765265567,
        "min_shap": -0.0041579868935252675,
        "max_shap": 0.002039026480725712
      },
      "rank": 291
    },
    {
      "feature_index": 137,
      "feature_name": "feature_137",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text\"\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006730921722913506,
        "mean_shap": -2.863471886997023e-05,
        "std_shap": 0.0009522027963200068,
        "min_shap": -0.0015561708858421227,
        "max_shap": 0.004264742932268323
      },
      "rank": 292
    },
    {
      "feature_index": 295,
      "feature_name": "feature_295",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity of the text using TextBlob\"\n    from textblob import TextBlob\n    sentiment = TextBlob(text).sentiment\n    return float(sentiment.subjectivity)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006725117387845794,
        "mean_shap": 4.046139496193469e-05,
        "std_shap": 0.0008738000494198885,
        "min_shap": -0.003689563587354999,
        "max_shap": 0.0021052020143637238
      },
      "rank": 293
    },
    {
      "feature_index": 318,
      "feature_name": "feature_318",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of characters in the text\"\n    return float(len(text.strip()))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006563936513940557,
        "mean_shap": -5.6036055017707706e-05,
        "std_shap": 0.0011800997796298578,
        "min_shap": -0.002778250546219352,
        "max_shap": 0.006452950565108092
      },
      "rank": 294
    },
    {
      "feature_index": 322,
      "feature_name": "feature_322",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of punctuation characters in the text\"\n    if len(text) == 0:\n        return 0.0\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return float(punctuation_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006548166563939084,
        "mean_shap": -0.0001754488183619546,
        "std_shap": 0.0010156791292416228,
        "min_shap": -0.004712549588325511,
        "max_shap": 0.0019487511597398228
      },
      "rank": 295
    },
    {
      "feature_index": 108,
      "feature_name": "feature_108",
      "feature_code": "def feature(text: str) -> float:\n    'Named entity count in the text'\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000654514793017996,
        "mean_shap": 0.00019914039187178534,
        "std_shap": 0.0009158167303325625,
        "min_shap": -0.0017185232261006066,
        "max_shap": 0.0036857534080725635
      },
      "rank": 296
    },
    {
      "feature_index": 390,
      "feature_name": "feature_390",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adjectives in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADJ'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006483147579542144,
        "mean_shap": -0.00012943310451109667,
        "std_shap": 0.0008496515467564008,
        "min_shap": -0.002024271973866693,
        "max_shap": 0.004037705891316487
      },
      "rank": 297
    },
    {
      "feature_index": 397,
      "feature_name": "feature_397",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adjectives in the text\"\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006399645422926168,
        "mean_shap": -0.00023592206167092724,
        "std_shap": 0.0007464030733633888,
        "min_shap": -0.0022011122478862266,
        "max_shap": 0.0027793537284366713
      },
      "rank": 298
    },
    {
      "feature_index": 361,
      "feature_name": "feature_361",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with more than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000638981844190665,
        "mean_shap": 5.1574077725241815e-05,
        "std_shap": 0.0008376802824707876,
        "min_shap": -0.0012226819659258811,
        "max_shap": 0.0038909121371895564
      },
      "rank": 299
    },
    {
      "feature_index": 330,
      "feature_name": "feature_330",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of punctuation characters to total characters'\n    if len(text) == 0:\n        return 0.0\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return float(punctuation_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006385588510515137,
        "mean_shap": -0.00017793195727028142,
        "std_shap": 0.0009842279609952702,
        "min_shap": -0.004598376601719886,
        "max_shap": 0.0021704183877432927
      },
      "rank": 300
    },
    {
      "feature_index": 138,
      "feature_name": "feature_138",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity score using TextBlob\"\n    blob = TextBlob(text)\n    return float(blob.sentiment.subjectivity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006368149838078906,
        "mean_shap": -5.401639320448427e-06,
        "std_shap": 0.000845227973805614,
        "min_shap": -0.0032251278438738496,
        "max_shap": 0.0016054991255823718
      },
      "rank": 301
    },
    {
      "feature_index": 334,
      "feature_name": "feature_334",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of words with emotional connotations\"\n    emotional_words = {'happy', 'sad', 'angry', 'excited', 'fear', 'love'}\n    words = text.lower().split()\n    return float(sum(1 for word in words if word in emotional_words)) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006349353049931212,
        "mean_shap": 0.00021700821341024703,
        "std_shap": 0.0009779709724436136,
        "min_shap": -0.006016437134829063,
        "max_shap": 0.0013084138255426729
      },
      "rank": 302
    },
    {
      "feature_index": 152,
      "feature_name": "feature_152",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences containing more than 15 words, indicating complexity\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentences = sum(1 for s in sentences if len(s.split()) > 15)\n    return float(long_sentences) / len(sentences) if len(sentences) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006310815709114616,
        "mean_shap": -4.604790772033722e-05,
        "std_shap": 0.0008027271706253774,
        "min_shap": -0.002331923851403361,
        "max_shap": 0.0021104224277129343
      },
      "rank": 303
    },
    {
      "feature_index": 366,
      "feature_name": "feature_366",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences in the text\"\n    doc = nlp(text)\n    return float(len(list(doc.sents)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006238385220955397,
        "mean_shap": -1.964332819650625e-05,
        "std_shap": 0.0007740560279000201,
        "min_shap": -0.0013674835003921854,
        "max_shap": 0.0027359504231986224
      },
      "rank": 304
    },
    {
      "feature_index": 393,
      "feature_name": "feature_393",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adjectives in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADJ'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006063505568468714,
        "mean_shap": -0.0002582892489025524,
        "std_shap": 0.0006893074700686205,
        "min_shap": -0.0017778400342113936,
        "max_shap": 0.0025242523869880634
      },
      "rank": 305
    },
    {
      "feature_index": 324,
      "feature_name": "feature_324",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation to total characters\"\n    punct_count = sum(1 for c in text if c in string.punctuation)\n    return float(punct_count) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006048902051638166,
        "mean_shap": -0.00011885221808565067,
        "std_shap": 0.0010182843342703028,
        "min_shap": -0.005804654133274509,
        "max_shap": 0.002264454206814013
      },
      "rank": 306
    },
    {
      "feature_index": 277,
      "feature_name": "feature_277",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences longer than 15 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 15)\n    return float(long_sentence_count) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006016018755116004,
        "mean_shap": -5.154361681387127e-05,
        "std_shap": 0.0007516228482579092,
        "min_shap": -0.0023254726438595857,
        "max_shap": 0.0018765627778010155
      },
      "rank": 307
    },
    {
      "feature_index": 346,
      "feature_name": "feature_346",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words to total words\"\n    complex_word_count = sum(1 for word in text.split() if textstat.syllable_count(word) > 2)\n    words = text.split()\n    if not words:\n        return 0.0\n    return float(complex_word_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005993391116321786,
        "mean_shap": -9.339753617906131e-05,
        "std_shap": 0.0007485632128371178,
        "min_shap": -0.0020531885918396806,
        "max_shap": 0.003955469840446787
      },
      "rank": 308
    },
    {
      "feature_index": 301,
      "feature_name": "feature_301",
      "feature_code": "def feature(text: str) -> float:\n    \"Average named entity length in characters\"\n    doc = nlp(text)\n    if not doc.ents:\n        return 0.0\n    return float(sum(len(ent.text) for ent in doc.ents)) / len(doc.ents)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005958587309660196,
        "mean_shap": -4.648284226628008e-05,
        "std_shap": 0.0008526501675257512,
        "min_shap": -0.0018649207465294057,
        "max_shap": 0.003674720346152962
      },
      "rank": 309
    },
    {
      "feature_index": 175,
      "feature_name": "feature_175",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adjectives in the text\"\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005927777472388609,
        "mean_shap": -0.00015960508370787906,
        "std_shap": 0.0007294057504033026,
        "min_shap": -0.0024674628404682507,
        "max_shap": 0.002199367188178458
      },
      "rank": 310
    },
    {
      "feature_index": 283,
      "feature_name": "feature_283",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s.strip()]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005922795914552942,
        "mean_shap": 0.00014462586760862777,
        "std_shap": 0.0008249570723084953,
        "min_shap": -0.00370679136886855,
        "max_shap": 0.001836104420742832
      },
      "rank": 311
    },
    {
      "feature_index": 332,
      "feature_name": "feature_332",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (3 or more syllables) to total words\"\n    words = text.split()\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    return float(complex_word_count) / len(words) if words else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005832135321922388,
        "mean_shap": -7.352028817046201e-05,
        "std_shap": 0.000709251917730114,
        "min_shap": -0.002297619959439858,
        "max_shap": 0.001538657162669704
      },
      "rank": 312
    },
    {
      "feature_index": 120,
      "feature_name": "feature_120",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005787627076397519,
        "mean_shap": 0.00016068413751265135,
        "std_shap": 0.0007706771229025189,
        "min_shap": -0.0017993833689403792,
        "max_shap": 0.004363101789610768
      },
      "rank": 313
    },
    {
      "feature_index": 297,
      "feature_name": "feature_297",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    total_word_count = len(doc)\n    return float(stopword_count) / total_word_count if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005766786934545261,
        "mean_shap": -3.297313917377212e-05,
        "std_shap": 0.0007464085337114284,
        "min_shap": -0.0015089897200926546,
        "max_shap": 0.002834700251177816
      },
      "rank": 314
    },
    {
      "feature_index": 303,
      "feature_name": "feature_303",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005701109910104458,
        "mean_shap": -4.876262575826954e-06,
        "std_shap": 0.00076793269060943,
        "min_shap": -0.0017838623114247143,
        "max_shap": 0.002966828931406034
      },
      "rank": 315
    },
    {
      "feature_index": 154,
      "feature_name": "feature_154",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives in the text'\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005687519874986129,
        "mean_shap": -0.00016872329779466817,
        "std_shap": 0.0008046438188827585,
        "min_shap": -0.0020276335813583096,
        "max_shap": 0.005384911166263637
      },
      "rank": 316
    },
    {
      "feature_index": 319,
      "feature_name": "feature_319",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in the text\"\n    doc = nlp(text)\n    lengths = [len(ent.text) for ent in doc.ents]\n    return float(sum(lengths) / len(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005656391175430841,
        "mean_shap": -9.116487205439606e-05,
        "std_shap": 0.0008405185617895438,
        "min_shap": -0.002031751465507203,
        "max_shap": 0.004010851993439759
      },
      "rank": 317
    },
    {
      "feature_index": 357,
      "feature_name": "feature_357",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with more than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count) / len(sentences) if sentences else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000565149659449897,
        "mean_shap": -0.0001512676219554004,
        "std_shap": 0.0007222978411906362,
        "min_shap": -0.0025723961834575883,
        "max_shap": 0.0021343525890245548
      },
      "rank": 318
    },
    {
      "feature_index": 263,
      "feature_name": "feature_263",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005597919924663627,
        "mean_shap": 0.00018081836532960515,
        "std_shap": 0.0007762161174833746,
        "min_shap": -0.0017492771718277524,
        "max_shap": 0.0036876152910272305
      },
      "rank": 319
    },
    {
      "feature_index": 343,
      "feature_name": "feature_343",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADV'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005592145541858231,
        "mean_shap": -1.0049384742854016e-05,
        "std_shap": 0.0007156720965333189,
        "min_shap": -0.0017390492659988038,
        "max_shap": 0.0027787865822483343
      },
      "rank": 320
    },
    {
      "feature_index": 337,
      "feature_name": "feature_337",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words (three syllables or more) in the text\"\n    complex_word_count = sum(1 for word in text.split() if textstat.syllable_count(word) >= 3)\n    return float(complex_word_count) / len(text.split()) if len(text.split()) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005506446976117839,
        "mean_shap": -9.88794035409252e-05,
        "std_shap": 0.0007231229654908752,
        "min_shap": -0.002746576802305421,
        "max_shap": 0.002412993034755037
      },
      "rank": 321
    },
    {
      "feature_index": 148,
      "feature_name": "feature_148",
      "feature_code": "def feature(text: str) -> float:\n    'Count of numeric characters in the text'\n    return float(sum(1 for c in text if c.isdigit()))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005476947589994092,
        "mean_shap": -7.548483079404676e-05,
        "std_shap": 0.0008571548620458576,
        "min_shap": -0.0013939857047163386,
        "max_shap": 0.004539132523454791
      },
      "rank": 322
    },
    {
      "feature_index": 396,
      "feature_name": "feature_396",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of verbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'VERB'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005441834572141452,
        "mean_shap": 5.9713962352328715e-05,
        "std_shap": 0.0008142707453286153,
        "min_shap": -0.001830448280026315,
        "max_shap": 0.003010419600998368
      },
      "rank": 323
    },
    {
      "feature_index": 321,
      "feature_name": "feature_321",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of words that are stop words\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005440891531986275,
        "mean_shap": -9.725312912787424e-05,
        "std_shap": 0.0006861544690948448,
        "min_shap": -0.0018182057918374969,
        "max_shap": 0.0023587250894707367
      },
      "rank": 324
    },
    {
      "feature_index": 391,
      "feature_name": "feature_391",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are verbs\"\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005406831556623512,
        "mean_shap": 6.555033595651586e-05,
        "std_shap": 0.0008511819930979267,
        "min_shap": -0.0014414516627652154,
        "max_shap": 0.004059132999803866
      },
      "rank": 325
    },
    {
      "feature_index": 312,
      "feature_name": "feature_312",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005353745531947322,
        "mean_shap": 7.605854572988171e-06,
        "std_shap": 0.0006752890546493419,
        "min_shap": -0.001189643530841165,
        "max_shap": 0.0021234653100796488
      },
      "rank": 326
    },
    {
      "feature_index": 130,
      "feature_name": "feature_130",
      "feature_code": "def feature(text: str) -> float:\n    'Count of negative sentiment words in the text'\n    scores = sia.polarity_scores(text)\n    return float(scores['neg'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005342653729335533,
        "mean_shap": -1.1007427256214884e-05,
        "std_shap": 0.000742410577962145,
        "min_shap": -0.0030799307797848736,
        "max_shap": 0.002344186237305268
      },
      "rank": 327
    },
    {
      "feature_index": 325,
      "feature_name": "feature_325",
      "feature_code": "def feature(text: str) -> float:\n    \"Density of complex words (words with more than 2 syllables)\"\n    complex_word_count = sum(1 for word in text.split() if textstat.syllable_count(word) > 2)\n    return float(complex_word_count) / len(text.split()) if text.split() else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000530783604519007,
        "mean_shap": -7.435765862119234e-05,
        "std_shap": 0.0006775208625234524,
        "min_shap": -0.001865323297004168,
        "max_shap": 0.002134983268902399
      },
      "rank": 328
    },
    {
      "feature_index": 246,
      "feature_name": "feature_246",
      "feature_code": "def feature(text: str) -> float:\n    \"Number of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000526843339749011,
        "mean_shap": 0.00011575564755095199,
        "std_shap": 0.0007367537705789621,
        "min_shap": -0.002083049031147084,
        "max_shap": 0.003980579197953825
      },
      "rank": 329
    },
    {
      "feature_index": 192,
      "feature_name": "feature_192",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005230960947880262,
        "mean_shap": 1.833345698244833e-05,
        "std_shap": 0.0007228302735979839,
        "min_shap": -0.0012146218017361364,
        "max_shap": 0.003993611249230485
      },
      "rank": 330
    },
    {
      "feature_index": 257,
      "feature_name": "feature_257",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of questions in the text\"\n    return float(text.count('?'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005205436930833551,
        "mean_shap": -4.2192874571134824e-05,
        "std_shap": 0.0007457663140084158,
        "min_shap": -0.0025060256643104034,
        "max_shap": 0.0032547830986688173
      },
      "rank": 331
    },
    {
      "feature_index": 384,
      "feature_name": "feature_384",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of verbs in the text\"\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005186476120023073,
        "mean_shap": 7.107495998122965e-05,
        "std_shap": 0.0007354446598649562,
        "min_shap": -0.0012040783419835011,
        "max_shap": 0.0034435532256970394
      },
      "rank": 332
    },
    {
      "feature_index": 323,
      "feature_name": "feature_323",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of negative sentiment words in the text using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['neg'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005148447903275284,
        "mean_shap": 4.1353778188494975e-05,
        "std_shap": 0.0007061252001389702,
        "min_shap": -0.0028205802449914603,
        "max_shap": 0.002724849041015501
      },
      "rank": 333
    },
    {
      "feature_index": 156,
      "feature_name": "feature_156",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives in the text'\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005135123215999369,
        "mean_shap": -0.00015537832118024393,
        "std_shap": 0.000697742880475963,
        "min_shap": -0.0017535839141034047,
        "max_shap": 0.003453356434705492
      },
      "rank": 334
    },
    {
      "feature_index": 326,
      "feature_name": "feature_326",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of negative sentiment words in the text using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['neg'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005038188267807839,
        "mean_shap": 2.1593628156428377e-05,
        "std_shap": 0.0006706689468513372,
        "min_shap": -0.0030860597275276713,
        "max_shap": 0.00168495252928835
      },
      "rank": 335
    },
    {
      "feature_index": 354,
      "feature_name": "feature_354",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of sentences with more than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00050365117185202,
        "mean_shap": -0.0001415509711940123,
        "std_shap": 0.0006103313784889302,
        "min_shap": -0.0019019014094712515,
        "max_shap": 0.0011745429404109269
      },
      "rank": 336
    },
    {
      "feature_index": 387,
      "feature_name": "feature_387",
      "feature_code": "def feature(text: str) -> float:\n    \"Average readability score using Flesch Reading Ease\"\n    try:\n        return float(textstat.flesch_reading_ease(text))\n    except Exception:\n        return 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005026038044465215,
        "mean_shap": -4.5905917638379544e-05,
        "std_shap": 0.0006207850303078067,
        "min_shap": -0.0020232056256490037,
        "max_shap": 0.0018373370443172726
      },
      "rank": 337
    },
    {
      "feature_index": 256,
      "feature_name": "feature_256",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of question marks in the text\"\n    return float(text.count('?'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000498548711170012,
        "mean_shap": 1.0022789441598731e-05,
        "std_shap": 0.0007076449143825078,
        "min_shap": -0.002311595441887517,
        "max_shap": 0.0037218887726046803
      },
      "rank": 338
    },
    {
      "feature_index": 302,
      "feature_name": "feature_302",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of stopwords to total words'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004961112077621777,
        "mean_shap": -8.238731890750543e-06,
        "std_shap": 0.0006925497161761814,
        "min_shap": -0.0014917542677151045,
        "max_shap": 0.003238668124784023
      },
      "rank": 339
    },
    {
      "feature_index": 317,
      "feature_name": "feature_317",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of negative words in the text using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['neg'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004956054951560794,
        "mean_shap": 9.890817943229828e-07,
        "std_shap": 0.0006883903359132285,
        "min_shap": -0.0032142062411073305,
        "max_shap": 0.0018435557109311262
      },
      "rank": 340
    },
    {
      "feature_index": 306,
      "feature_name": "feature_306",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000494433202158461,
        "mean_shap": 4.549542879355115e-05,
        "std_shap": 0.0006877708486408403,
        "min_shap": -0.0012515323299393786,
        "max_shap": 0.0029213976942831137
      },
      "rank": 341
    },
    {
      "feature_index": 298,
      "feature_name": "feature_298",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return (stopword_count / len(doc)) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004910077362028657,
        "mean_shap": -2.1100676355865237e-05,
        "std_shap": 0.0006436326352718301,
        "min_shap": -0.0012241220489420798,
        "max_shap": 0.0028190752010202294
      },
      "rank": 342
    },
    {
      "feature_index": 264,
      "feature_name": "feature_264",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00048714308348258125,
        "mean_shap": 1.2373237569337517e-05,
        "std_shap": 0.0007022193705425073,
        "min_shap": -0.003669942253951996,
        "max_shap": 0.0019443313712196701
      },
      "rank": 343
    },
    {
      "feature_index": 300,
      "feature_name": "feature_300",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of words that are stop words\"\n    doc = nlp(text)\n    total_words = len(doc)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return float(stop_word_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004833738265248084,
        "mean_shap": -2.548214911682068e-05,
        "std_shap": 0.0006837001736601749,
        "min_shap": -0.001126004199103251,
        "max_shap": 0.003562177071401532
      },
      "rank": 344
    },
    {
      "feature_index": 341,
      "feature_name": "feature_341",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of positive sentiment words in the text\"\n    positive_words = {'good', 'great', 'happy', 'positive', 'excellent', 'fantastic', 'wonderful'}\n    word_list = text.lower().split()\n    return float(sum(1 for word in word_list if word in positive_words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004831765427839638,
        "mean_shap": -3.988811709106435e-05,
        "std_shap": 0.0008048404815014725,
        "min_shap": -0.0021837751403725562,
        "max_shap": 0.006548695321574749
      },
      "rank": 345
    },
    {
      "feature_index": 310,
      "feature_name": "feature_310",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of non-stopwords in the text\"\n    doc = nlp(text)\n    non_stopword_count = sum(1 for token in doc if not token.is_stop)\n    return float(non_stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00048124510734646726,
        "mean_shap": 6.709812685799994e-05,
        "std_shap": 0.0006390651882372589,
        "min_shap": -0.000924636461654401,
        "max_shap": 0.0025046304161316507
      },
      "rank": 346
    },
    {
      "feature_index": 335,
      "feature_name": "feature_335",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADV'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00047021089550599454,
        "mean_shap": -4.7984500531463865e-05,
        "std_shap": 0.0006823170290459288,
        "min_shap": -0.001112789867145035,
        "max_shap": 0.003266824404555818
      },
      "rank": 347
    },
    {
      "feature_index": 342,
      "feature_name": "feature_342",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADV'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000465216387722283,
        "mean_shap": -5.5049122456367875e-05,
        "std_shap": 0.0006816138921050092,
        "min_shap": -0.0012753551897305463,
        "max_shap": 0.003998485480276568
      },
      "rank": 348
    },
    {
      "feature_index": 348,
      "feature_name": "feature_348",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADV'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000463627437335977,
        "mean_shap": 3.1468918821121405e-05,
        "std_shap": 0.0006661720065510264,
        "min_shap": -0.0009655594798100983,
        "max_shap": 0.0035525546899273478
      },
      "rank": 349
    },
    {
      "feature_index": 196,
      "feature_name": "feature_196",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives in the text'\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004620296686987495,
        "mean_shap": -0.000191649725505923,
        "std_shap": 0.0005582381210983402,
        "min_shap": -0.0016704129139688757,
        "max_shap": 0.002576013971919283
      },
      "rank": 350
    },
    {
      "feature_index": 360,
      "feature_name": "feature_360",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability score (Flesch Reading Ease)\"\n    return float(textstat.flesch_reading_ease(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004527974455419148,
        "mean_shap": -9.290977689491351e-05,
        "std_shap": 0.000541474226526269,
        "min_shap": -0.001628087760458327,
        "max_shap": 0.0011707080931413574
      },
      "rank": 351
    },
    {
      "feature_index": 311,
      "feature_name": "feature_311",
      "feature_code": "def feature(text: str) -> float:\n    \"Average named entity length in characters\"\n    doc = nlp(text)\n    if not doc.ents:\n        return 0.0\n    return float(sum(len(ent.text) for ent in doc.ents)) / len(doc.ents)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00044492860541763186,
        "mean_shap": -5.814482980068776e-05,
        "std_shap": 0.0006019148929400734,
        "min_shap": -0.001605058606175416,
        "max_shap": 0.002389017247881379
      },
      "rank": 352
    },
    {
      "feature_index": 356,
      "feature_name": "feature_356",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences that contain more than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentences = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentences) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00044276720905036106,
        "mean_shap": -0.00016090231128082372,
        "std_shap": 0.0005433767033288938,
        "min_shap": -0.0018392035359709718,
        "max_shap": 0.0012842630240956295
      },
      "rank": 353
    },
    {
      "feature_index": 270,
      "feature_name": "feature_270",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of sentences measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004419289580605298,
        "mean_shap": 4.642329512432562e-05,
        "std_shap": 0.0006906218469168398,
        "min_shap": -0.0038379374252560253,
        "max_shap": 0.0016721038693055768
      },
      "rank": 354
    },
    {
      "feature_index": 333,
      "feature_name": "feature_333",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADV'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00043818513434786227,
        "mean_shap": -2.3787510391712327e-05,
        "std_shap": 0.0005639452283013113,
        "min_shap": -0.0011943402940959153,
        "max_shap": 0.002187335136680083
      },
      "rank": 355
    },
    {
      "feature_index": 338,
      "feature_name": "feature_338",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'ADV'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004315705785280014,
        "mean_shap": 4.344340383225546e-05,
        "std_shap": 0.000618804695198899,
        "min_shap": -0.00119512841293987,
        "max_shap": 0.002790325870439172
      },
      "rank": 356
    },
    {
      "feature_index": 381,
      "feature_name": "feature_381",
      "feature_code": "def feature(text: str) -> float:\n    \"Number of sentences longer than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00043055899600317685,
        "mean_shap": -4.6846445367319337e-05,
        "std_shap": 0.0005472271108062369,
        "min_shap": -0.0014983011709508171,
        "max_shap": 0.0018702689427388508
      },
      "rank": 357
    },
    {
      "feature_index": 398,
      "feature_name": "feature_398",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of verbs in the text\"\n    doc = nlp(text)\n    return float(sum(1 for token in doc if token.pos_ == 'VERB'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004173286993572478,
        "mean_shap": -1.573293228865264e-05,
        "std_shap": 0.0005529716370464453,
        "min_shap": -0.001208209444257361,
        "max_shap": 0.0017372545992209558
      },
      "rank": 358
    },
    {
      "feature_index": 388,
      "feature_name": "feature_388",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique sentences in the text\"\n    sentences = re.split(r'[.!?]+', text)\n    unique_sentences = set(s.strip() for s in sentences if s.strip())\n    return float(len(unique_sentences))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004167652487432611,
        "mean_shap": -4.241884458309801e-05,
        "std_shap": 0.0005514781833019028,
        "min_shap": -0.0011668299910642518,
        "max_shap": 0.001894024566232367
      },
      "rank": 359
    },
    {
      "feature_index": 380,
      "feature_name": "feature_380",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of specific keywords in the text\"\n    keywords = ['important', 'critical', 'essential', 'vital']\n    keyword_count = sum(text.lower().count(keyword) for keyword in keywords)\n    return float(keyword_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00041133236589829423,
        "mean_shap": 2.4842250213633067e-05,
        "std_shap": 0.0008409648796444656,
        "min_shap": -0.0015000353802267139,
        "max_shap": 0.004916449053605391
      },
      "rank": 360
    },
    {
      "feature_index": 309,
      "feature_name": "feature_309",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004099288201895969,
        "mean_shap": 1.1059388704431176e-05,
        "std_shap": 0.0005498886958044459,
        "min_shap": -0.0011114633234614185,
        "max_shap": 0.0021219782667051047
      },
      "rank": 361
    },
    {
      "feature_index": 340,
      "feature_name": "feature_340",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (3+ syllables) in the text\"\n    words = text.split()\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) > 2)\n    return float(complex_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00039805645372883964,
        "mean_shap": -5.8884259216722035e-05,
        "std_shap": 0.0005297300052024924,
        "min_shap": -0.002148664754949197,
        "max_shap": 0.001890350497562047
      },
      "rank": 362
    },
    {
      "feature_index": 305,
      "feature_name": "feature_305",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of digits in the text\"\n    digit_count = sum(1 for c in text if c.isdigit())\n    return float(digit_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00038101742012477876,
        "mean_shap": -3.107041528783915e-05,
        "std_shap": 0.000632825267001508,
        "min_shap": -0.0009220943575207882,
        "max_shap": 0.003778369313332928
      },
      "rank": 363
    },
    {
      "feature_index": 349,
      "feature_name": "feature_349",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words containing digits\"\n    return float(sum(1 for word in text.split() if any(char.isdigit() for char in word)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003804997854088337,
        "mean_shap": -3.845074425792779e-05,
        "std_shap": 0.0006645847847947154,
        "min_shap": -0.0008620976311740009,
        "max_shap": 0.005446744548877322
      },
      "rank": 364
    },
    {
      "feature_index": 364,
      "feature_name": "feature_364",
      "feature_code": "def feature(text: str) -> float:\n    \"Counting the number of different parts of speech (POS) used\"\n    doc = nlp(text)\n    pos_tags = {token.pos_ for token in doc}\n    return float(len(pos_tags))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00036288161099398735,
        "mean_shap": 4.0881174607995154e-05,
        "std_shap": 0.0004811260333844574,
        "min_shap": -0.001275055722543022,
        "max_shap": 0.0019475569648291313
      },
      "rank": 365
    },
    {
      "feature_index": 383,
      "feature_name": "feature_383",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words with 3 or fewer characters\"\n    return float(sum(1 for word in text.split() if len(word) <= 3))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00036067408118273155,
        "mean_shap": 7.73883163099862e-05,
        "std_shap": 0.0004697996204941286,
        "min_shap": -0.0008913233959510153,
        "max_shap": 0.0018695022937963645
      },
      "rank": 366
    },
    {
      "feature_index": 389,
      "feature_name": "feature_389",
      "feature_code": "def feature(text: str) -> float:\n    \"Average readability score using Flesch Reading Ease\"\n    return float(textstat.flesch_reading_ease(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003520236441230292,
        "mean_shap": -9.743512391599314e-05,
        "std_shap": 0.0004440607312618533,
        "min_shap": -0.001445548859727836,
        "max_shap": 0.0011815571178400558
      },
      "rank": 367
    },
    {
      "feature_index": 362,
      "feature_name": "feature_362",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability score based on Flesch Reading Ease\"\n    return float(textstat.flesch_reading_ease(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003415760619534228,
        "mean_shap": -8.007581000683265e-05,
        "std_shap": 0.0004350719460258801,
        "min_shap": -0.0016829539801423007,
        "max_shap": 0.0014017186671056226
      },
      "rank": 368
    },
    {
      "feature_index": 183,
      "feature_name": "feature_183",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003374146405954618,
        "mean_shap": 1.6458211231593497e-05,
        "std_shap": 0.0005448960630602853,
        "min_shap": -0.0011304375409482356,
        "max_shap": 0.004150327192538001
      },
      "rank": 369
    },
    {
      "feature_index": 336,
      "feature_name": "feature_336",
      "feature_code": "def feature(text: str) -> float:\n    \"Unique character count in the text\"\n    return float(len(set(text)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00033458253087103847,
        "mean_shap": 0.0001689566250478599,
        "std_shap": 0.0004293737688607433,
        "min_shap": -0.001921898800264613,
        "max_shap": 0.0024136966064742965
      },
      "rank": 370
    },
    {
      "feature_index": 355,
      "feature_name": "feature_355",
      "feature_code": "def feature(text: str) -> float:\n    \"Flesch reading ease score for the text\"\n    import textstat\n    return float(textstat.flesch_reading_ease(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00032245121754701287,
        "mean_shap": -4.843304952884761e-05,
        "std_shap": 0.0003997504910195508,
        "min_shap": -0.001252573731751873,
        "max_shap": 0.0008763105348640677
      },
      "rank": 371
    },
    {
      "feature_index": 374,
      "feature_name": "feature_374",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words with 3 or fewer characters\"\n    short_word_count = sum(1 for word in text.split() if len(word) <= 3)\n    return float(short_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00031845250650820104,
        "mean_shap": 0.00015600400927104824,
        "std_shap": 0.00042829170922569117,
        "min_shap": -0.0008421484142663066,
        "max_shap": 0.0024200051229346597
      },
      "rank": 372
    },
    {
      "feature_index": 115,
      "feature_name": "feature_115",
      "feature_code": "def feature(text: str) -> float:\n    'Count of sentences with more than 10 words'\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000313612242440738,
        "mean_shap": -3.604219544691308e-05,
        "std_shap": 0.0003879835612329936,
        "min_shap": -0.0013156025504438827,
        "max_shap": 0.001128668571872153
      },
      "rank": 373
    },
    {
      "feature_index": 369,
      "feature_name": "feature_369",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with more than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003105826419585146,
        "mean_shap": -2.4243168994713973e-05,
        "std_shap": 0.0003976281970647999,
        "min_shap": -0.001091318127385087,
        "max_shap": 0.0015999839427736542
      },
      "rank": 374
    },
    {
      "feature_index": 377,
      "feature_name": "feature_377",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity score based on unique words to total words ratio\"\n    words = text.split()\n    unique_words = set(words)\n    return float(len(unique_words)) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00031041364335773807,
        "mean_shap": 1.6353837100176852e-05,
        "std_shap": 0.00040642581319248923,
        "min_shap": -0.001278630943179338,
        "max_shap": 0.0012846999327323272
      },
      "rank": 375
    },
    {
      "feature_index": 379,
      "feature_name": "feature_379",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of distinct parts of speech in the text\"\n    doc = nlp(text)\n    pos_tags = set(token.pos_ for token in doc)\n    return float(len(pos_tags))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00029657435227489184,
        "mean_shap": -4.820840141506279e-06,
        "std_shap": 0.00036648383552701896,
        "min_shap": -0.0008911002137595124,
        "max_shap": 0.0013893348275691314
      },
      "rank": 376
    },
    {
      "feature_index": 351,
      "feature_name": "feature_351",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of distinct characters in the text\"\n    return float(len(set(text)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002943068744739902,
        "mean_shap": 0.00021300437180987776,
        "std_shap": 0.0003186372267360184,
        "min_shap": -0.0007835901214407348,
        "max_shap": 0.0013894824783318854
      },
      "rank": 377
    },
    {
      "feature_index": 345,
      "feature_name": "feature_345",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique characters in the text\"\n    unique_chars = set(text)\n    return float(len(unique_chars))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028937517984943474,
        "mean_shap": 0.0001656990532278466,
        "std_shap": 0.00034677338121459714,
        "min_shap": -0.0012646276843034175,
        "max_shap": 0.0013103778499738916
      },
      "rank": 378
    },
    {
      "feature_index": 382,
      "feature_name": "feature_382",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of unique words to total words\"\n    words = text.split()\n    if not words:\n        return 0.0\n    unique_words = len(set(words))\n    return float(unique_words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028842712484941684,
        "mean_shap": -7.33478973949817e-05,
        "std_shap": 0.00035842139934555954,
        "min_shap": -0.001122337609998053,
        "max_shap": 0.0010755114766034344
      },
      "rank": 379
    },
    {
      "feature_index": 368,
      "feature_name": "feature_368",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words with fewer than 3 characters\"\n    words = text.split()\n    short_word_count = sum(1 for word in words if len(word) < 3)\n    return float(short_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002847576582257682,
        "mean_shap": -9.60453352084952e-05,
        "std_shap": 0.00037926211047346,
        "min_shap": -0.0014505745788468223,
        "max_shap": 0.000868947091836117
      },
      "rank": 380
    },
    {
      "feature_index": 375,
      "feature_name": "feature_375",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of words to distinct words (vocabulary richness)\"\n    words = text.split()\n    unique_words = len(set(words))\n    return float(len(words)) / unique_words if unique_words > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002815536737832831,
        "mean_shap": 1.1305160247836457e-05,
        "std_shap": 0.0003664558738819535,
        "min_shap": -0.0010937731815286891,
        "max_shap": 0.0011520459206929869
      },
      "rank": 381
    },
    {
      "feature_index": 359,
      "feature_name": "feature_359",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words\"\n    words = text.split()\n    unique_words = len(set(words))\n    total_words = len(words)\n    return float(unique_words) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002731650400107747,
        "mean_shap": -1.0138282649124766e-05,
        "std_shap": 0.00035120430848092525,
        "min_shap": -0.0009506514001369547,
        "max_shap": 0.0012155094347687653
      },
      "rank": 382
    },
    {
      "feature_index": 376,
      "feature_name": "feature_376",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words in the text\"\n    words = text.split()\n    unique_word_count = len(set(words))\n    return float(unique_word_count) / len(words) if len(words) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002686576728574851,
        "mean_shap": -8.566197701550381e-05,
        "std_shap": 0.0003288728550915911,
        "min_shap": -0.0014542745128655343,
        "max_shap": 0.0008910994259660095
      },
      "rank": 383
    },
    {
      "feature_index": 378,
      "feature_name": "feature_378",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity measured as the ratio of unique words to total words\"\n    words = text.split()\n    unique_words = len(set(words))\n    return float(unique_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00026832984736177893,
        "mean_shap": 2.3868115992541444e-05,
        "std_shap": 0.0003585784873483041,
        "min_shap": -0.0014180143055656255,
        "max_shap": 0.0009974493866728193
      },
      "rank": 384
    },
    {
      "feature_index": 367,
      "feature_name": "feature_367",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words with less than 3 characters\"\n    words = text.split()\n    short_word_count = sum(1 for word in words if len(word) < 3)\n    return float(short_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00026103964680368615,
        "mean_shap": -6.493939450105577e-05,
        "std_shap": 0.0003394618670999686,
        "min_shap": -0.0010388337790585625,
        "max_shap": 0.0012403052201426488
      },
      "rank": 385
    },
    {
      "feature_index": 370,
      "feature_name": "feature_370",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are less than 3 characters\"\n    words = text.split()\n    short_word_count = sum(1 for word in words if len(word) < 3)\n    return float(short_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000260768877946887,
        "mean_shap": -0.00013363437402796468,
        "std_shap": 0.00031250510395805747,
        "min_shap": -0.0009087829196912847,
        "max_shap": 0.0010616680074033557
      },
      "rank": 386
    },
    {
      "feature_index": 373,
      "feature_name": "feature_373",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical diversity using unique words'\n    words = text.split()\n    if not words:\n        return 0.0\n    unique_words = len(set(words))\n    return float(unique_words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00025826459759689744,
        "mean_shap": -8.313109323756961e-06,
        "std_shap": 0.00033052813200268533,
        "min_shap": -0.0008902322509052082,
        "max_shap": 0.0010932904514402613
      },
      "rank": 387
    },
    {
      "feature_index": 331,
      "feature_name": "feature_331",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of distinct characters in the text\"\n    unique_chars = set(text)\n    return float(len(unique_chars))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00025495858030382887,
        "mean_shap": 8.521922229425552e-05,
        "std_shap": 0.0003248842390841598,
        "min_shap": -0.0009379297755721851,
        "max_shap": 0.001485791817436569
      },
      "rank": 388
    },
    {
      "feature_index": 353,
      "feature_name": "feature_353",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words\"\n    words = text.split()\n    unique_words = len(set(words))\n    return float(unique_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002520773410852122,
        "mean_shap": -9.986234013158182e-06,
        "std_shap": 0.0003330348871888183,
        "min_shap": -0.0008935566908146821,
        "max_shap": 0.0011881961867650436
      },
      "rank": 389
    },
    {
      "feature_index": 352,
      "feature_name": "feature_352",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of different POS tags in the text\"\n    doc = nlp(text)\n    pos_counts = len(set(token.pos_ for token in doc))\n    return float(pos_counts)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002488128877806061,
        "mean_shap": -3.812139080164914e-05,
        "std_shap": 0.0003196950820726614,
        "min_shap": -0.001001868146904602,
        "max_shap": 0.0014826242752651842
      },
      "rank": 390
    },
    {
      "feature_index": 129,
      "feature_name": "feature_129",
      "feature_code": "def feature(text: str) -> float:\n    \"Total count of unique characters in the text\"\n    unique_chars = len(set(text))\n    return float(unique_chars)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024264966437023716,
        "mean_shap": 6.168662090802479e-05,
        "std_shap": 0.00039117993790332056,
        "min_shap": -0.0037142469285619747,
        "max_shap": 0.0011519338118995714
      },
      "rank": 391
    },
    {
      "feature_index": 339,
      "feature_name": "feature_339",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique characters in the text\"\n    unique_chars = set(text)\n    return float(len(unique_chars))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023181064865842522,
        "mean_shap": 0.00010840748228684624,
        "std_shap": 0.0003092766009493663,
        "min_shap": -0.0019313958227159282,
        "max_shap": 0.0009637406962246031
      },
      "rank": 392
    },
    {
      "feature_index": 372,
      "feature_name": "feature_372",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words with 2 or fewer characters\"\n    words = text.split()\n    short_word_count = sum(1 for word in words if len(word) <= 2)\n    return float(short_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022455154264322294,
        "mean_shap": 3.7342851204248914e-05,
        "std_shap": 0.0003004884840672862,
        "min_shap": -0.0007402342288267124,
        "max_shap": 0.0015494673864607023
      },
      "rank": 393
    },
    {
      "feature_index": 347,
      "feature_name": "feature_347",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique characters in the text\"\n    unique_chars = set(text)\n    return float(len(unique_chars))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019855661593425896,
        "mean_shap": 5.719417535173778e-05,
        "std_shap": 0.0002540596750825977,
        "min_shap": -0.0007771348859158731,
        "max_shap": 0.0007261793974537091
      },
      "rank": 394
    },
    {
      "feature_index": 363,
      "feature_name": "feature_363",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of unique words to total words\"\n    words = text.split()\n    unique_words = len(set(words))\n    return float(unique_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019704364390254613,
        "mean_shap": -4.529646865212804e-05,
        "std_shap": 0.00025613109999991773,
        "min_shap": -0.000999239874632331,
        "max_shap": 0.0009750020329605519
      },
      "rank": 395
    },
    {
      "feature_index": 371,
      "feature_name": "feature_371",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences with more than 20 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 20)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019257265314492919,
        "mean_shap": 3.1458151514140457e-05,
        "std_shap": 0.00034758081430545515,
        "min_shap": -0.002285360644924924,
        "max_shap": 0.0017033621446114078
      },
      "rank": 396
    },
    {
      "feature_index": 365,
      "feature_name": "feature_365",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words\"\n    words = text.split()\n    unique_word_count = len(set(words))\n    return float(unique_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001903556519391616,
        "mean_shap": -9.851018156853797e-06,
        "std_shap": 0.0002443891548288986,
        "min_shap": -0.0009705337067385421,
        "max_shap": 0.0008581114097971747
      },
      "rank": 397
    },
    {
      "feature_index": 395,
      "feature_name": "feature_395",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences containing more than 10 words\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018441473041078298,
        "mean_shap": 1.8747985940420057e-05,
        "std_shap": 0.0002451155356768802,
        "min_shap": -0.0008318061810406121,
        "max_shap": 0.0010145614100445317
      },
      "rank": 398
    },
    {
      "feature_index": 392,
      "feature_name": "feature_392",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity measured as unique words to total words ratio\"\n    words = text.split()\n    if not words:\n        return 0.0\n    unique_word_count = len(set(words))\n    return float(unique_word_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018410463837412273,
        "mean_shap": -3.104057215648637e-05,
        "std_shap": 0.0002448653486867149,
        "min_shap": -0.0009020645272001103,
        "max_shap": 0.000802039979947097
      },
      "rank": 399
    },
    {
      "feature_index": 399,
      "feature_name": "feature_399",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of long sentences (more than 20 words)\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 20)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00015327462986549817,
        "mean_shap": -3.459723188275865e-05,
        "std_shap": 0.0003308492051555247,
        "min_shap": -0.0027272181722815996,
        "max_shap": 0.0009791948948255642
      },
      "rank": 400
    }
  ],
  "shap_metadata": {
    "explainer_type": "TreeExplainer",
    "num_features": 400
  }
}