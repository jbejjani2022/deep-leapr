{
  "model_info": {
    "learner": "did3",
    "domain": "text_regression_rm_helpful",
    "model": "gpt-4o-mini",
    "api_level": "expert",
    "checkpoint_path": "results/models/did3__text_regression_rm_helpful__gpt-4o-mini.pkl",
    "features_path": "results/features/did3__text_regression_rm_helpful__gpt-4o-mini.json",
    "analysis_date": "2025-11-20T21:49:21.770838"
  },
  "dataset_info": {
    "split": "validation",
    "num_samples": 236,
    "feature_matrix_shape": [
      236,
      469
    ]
  },
  "feature_importance": [
    {
      "feature_index": 33,
      "feature_name": "feature_33",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of words spoken per turn in the text'\n    turns = text.split('Assistant:')\n    total_words = sum(len(turn.split()) for turn in turns)\n    return total_words / len(turns) if turns else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.2848112244885716,
        "mean_shap": 0.045453989120326596,
        "std_shap": 0.30810867012607857,
        "min_shap": -0.6226094576641131,
        "max_shap": 0.4305055827418952
      },
      "rank": 1
    },
    {
      "feature_index": 21,
      "feature_name": "feature_21",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique nouns in the text'\n    doc = nlp(text)\n    unique_nouns = {token.text.lower() for token in doc if token.pos_ == 'NOUN'}\n    return float(len(unique_nouns))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.14044235478972777,
        "mean_shap": 0.005772117341371615,
        "std_shap": 0.17090908368632723,
        "min_shap": -0.24800372931702772,
        "max_shap": 0.38309433812060595
      },
      "rank": 2
    },
    {
      "feature_index": 375,
      "feature_name": "feature_375",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of personal pronouns to total words in the text\"\n    doc = nlp(text)\n    personal_pronouns = sum(1 for token in doc if token.text.lower() in ['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'])\n    total_words = len([token for token in doc if token.is_alpha])\n    return personal_pronouns / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.08491554045723414,
        "mean_shap": 0.012367989543698929,
        "std_shap": 0.10240390526400314,
        "min_shap": -0.18238463655572829,
        "max_shap": 0.2872637651967473
      },
      "rank": 3
    },
    {
      "feature_index": 78,
      "feature_name": "feature_78",
      "feature_code": "def feature(text: str) -> float:\n    'Average word length in characters within questions.'\n    if text.strip().endswith('?'):\n        words = text.split()\n        return float(sum(len(word) for word in words)) / len(words) if words else 0.0\n    return 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.050030199426657256,
        "mean_shap": -7.819639609630148e-05,
        "std_shap": 0.07362995995712475,
        "min_shap": -0.34815598917232565,
        "max_shap": 0.07344649621640913
      },
      "rank": 4
    },
    {
      "feature_index": 172,
      "feature_name": "feature_172",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of occurrences of key thematic words divided by total words\"\n    thematic_words = ['human', 'social', 'pandemic', 'learn', 'people']\n    count = sum(text.lower().count(word) for word in thematic_words)\n    total_words = len(text.split())\n    return count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.04286151740679507,
        "mean_shap": 0.013475899433190693,
        "std_shap": 0.05127820021218793,
        "min_shap": -0.1351715710628497,
        "max_shap": 0.11329068808155716
      },
      "rank": 5
    },
    {
      "feature_index": 80,
      "feature_name": "feature_80",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of stopwords to total words in the text'\n    doc = nlp(text)\n    total_words = len([token for token in doc if token.is_alpha])\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.033140271078511904,
        "mean_shap": 0.0007970590274423835,
        "std_shap": 0.04067868519533322,
        "min_shap": -0.10443925792567715,
        "max_shap": 0.12643148500497206
      },
      "rank": 6
    },
    {
      "feature_index": 134,
      "feature_name": "feature_134",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0313881405268406,
        "mean_shap": 0.0011985180887762134,
        "std_shap": 0.04030288971399873,
        "min_shap": -0.05312588344819316,
        "max_shap": 0.10731202212656919
      },
      "rank": 7
    },
    {
      "feature_index": 426,
      "feature_name": "feature_426",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical density measuring ratio of content words to total words'\n    doc = nlp(text)\n    total_words = len(doc)\n    if total_words == 0:\n        return 0.0\n    content_word_count = sum(1 for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV'])\n    return float(content_word_count) / total_words\n",
      "shap_statistics": {
        "mean_abs_shap": 0.028862776937697458,
        "mean_shap": -0.0012611830644087406,
        "std_shap": 0.03759978790038543,
        "min_shap": -0.08788721899545762,
        "max_shap": 0.1466038860732375
      },
      "rank": 8
    },
    {
      "feature_index": 265,
      "feature_name": "feature_265",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of pronouns to total words in the text\"\n    doc = nlp(text)\n    pronoun_count = sum(1 for token in doc if token.pos_ == 'PRON')\n    return pronoun_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.028540042487923387,
        "mean_shap": 0.001250740502172541,
        "std_shap": 0.03308291209407935,
        "min_shap": -0.07776141424803291,
        "max_shap": 0.054729747963833626
      },
      "rank": 9
    },
    {
      "feature_index": 4,
      "feature_name": "feature_4",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.026869613478025904,
        "mean_shap": 0.000804712062354133,
        "std_shap": 0.0353592830535951,
        "min_shap": -0.04663772100321103,
        "max_shap": 0.09213393966907729
      },
      "rank": 10
    },
    {
      "feature_index": 27,
      "feature_name": "feature_27",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.024511467037166893,
        "mean_shap": 0.0002620181598364398,
        "std_shap": 0.031760917565818884,
        "min_shap": -0.040327237951344914,
        "max_shap": 0.0860476359485764
      },
      "rank": 11
    },
    {
      "feature_index": 406,
      "feature_name": "feature_406",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.024417934202802173,
        "mean_shap": 0.0003161148393743618,
        "std_shap": 0.031901083037933146,
        "min_shap": -0.03966443965386758,
        "max_shap": 0.08841294283899402
      },
      "rank": 12
    },
    {
      "feature_index": 2,
      "feature_name": "feature_2",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0240630559442166,
        "mean_shap": -0.00014155059330159982,
        "std_shap": 0.03234108605449568,
        "min_shap": -0.037720321692532456,
        "max_shap": 0.09171312775598232
      },
      "rank": 13
    },
    {
      "feature_index": 316,
      "feature_name": "feature_316",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words longer than 6 characters\"\n    long_words = sum(1 for word in text.split() if len(word) > 6)\n    return float(long_words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.023731008157847386,
        "mean_shap": 0.0022461199557344885,
        "std_shap": 0.02615324087549965,
        "min_shap": -0.051114849783876655,
        "max_shap": 0.053716783096643544
      },
      "rank": 14
    },
    {
      "feature_index": 433,
      "feature_name": "feature_433",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns in the text\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    return float(unique_nouns)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.02211278162876483,
        "mean_shap": 0.00017867756186155473,
        "std_shap": 0.029497019567755564,
        "min_shap": -0.03366927462608316,
        "max_shap": 0.0835470353186844
      },
      "rank": 15
    },
    {
      "feature_index": 281,
      "feature_name": "feature_281",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of words per question in the text'\n    questions = re.findall(r'\\?\\s*', text)\n    if not questions:\n        return 0.0\n    return float(len(text.split())) / len(questions)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.019958198042355824,
        "mean_shap": -0.0028895456937590145,
        "std_shap": 0.029533398501864115,
        "min_shap": -0.17793672101864486,
        "max_shap": 0.03923925993862542
      },
      "rank": 16
    },
    {
      "feature_index": 290,
      "feature_name": "feature_290",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of words per question in the text'\n    questions = re.findall(r'\\?\\s*', text)\n    if not questions:\n        return 0.0\n    return float(len(text.split())) / len(questions)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.019250232955631746,
        "mean_shap": -0.0031880156251442126,
        "std_shap": 0.027623622715035264,
        "min_shap": -0.1556979628034633,
        "max_shap": 0.03571291740991711
      },
      "rank": 17
    },
    {
      "feature_index": 86,
      "feature_name": "feature_86",
      "feature_code": "def feature(text: str) -> float:\n    'Presence of personal pronouns in the text (I, me, my)'\n    personal_pronouns = {'i', 'me', 'my', 'you', 'your'}\n    words = text.split()\n    if not words:\n        return 0.0\n    pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n    return float(pronoun_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.018558298089671955,
        "mean_shap": 0.0030288481907722723,
        "std_shap": 0.024198375970733935,
        "min_shap": -0.04677260174056538,
        "max_shap": 0.09656745611071162
      },
      "rank": 18
    },
    {
      "feature_index": 67,
      "feature_name": "feature_67",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of personal pronouns to total words indicating personal engagement'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    personal_pronouns = sum(1 for token in doc if token.tag_ in ['PRP', 'PRP$', 'WP', 'WP$'])\n    return personal_pronouns / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01855619040956212,
        "mean_shap": 0.001966044094289871,
        "std_shap": 0.02113552831781745,
        "min_shap": -0.03566585630727546,
        "max_shap": 0.05754785823875289
      },
      "rank": 19
    },
    {
      "feature_index": 321,
      "feature_name": "feature_321",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical density of the text'\n    doc = nlp(text)\n    total_tokens = len(doc)\n    content_tokens = sum(1 for token in doc if token.pos_ not in ['DET', 'ADP', 'CONJ', 'PRON', 'PUNCT', 'NUM'])\n    return content_tokens / total_tokens if total_tokens > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.018273990484177455,
        "mean_shap": -0.003633910116917112,
        "std_shap": 0.02226859454504251,
        "min_shap": -0.05535084711147734,
        "max_shap": 0.04661725672278442
      },
      "rank": 20
    },
    {
      "feature_index": 76,
      "feature_name": "feature_76",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of sentences to paragraphs in the text'\n    paragraphs = text.split('\\n')\n    sentence_count = sum(text.count(s) for s in ['.', '!', '?'])\n    return sentence_count / len(paragraphs) if len(paragraphs) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.017704692546487336,
        "mean_shap": -0.0004100778352663309,
        "std_shap": 0.023078068493559176,
        "min_shap": -0.08001893595869329,
        "max_shap": 0.048933204656774754
      },
      "rank": 21
    },
    {
      "feature_index": 119,
      "feature_name": "feature_119",
      "feature_code": "def feature(text: str) -> float:\n    'Count of words longer than six characters'\n    long_words = sum(1 for word in text.split() if len(word) > 6)\n    return float(long_words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.017681783823160355,
        "mean_shap": 0.002096027479798693,
        "std_shap": 0.020351642281415824,
        "min_shap": -0.049539048280502304,
        "max_shap": 0.04495939297885519
      },
      "rank": 22
    },
    {
      "feature_index": 128,
      "feature_name": "feature_128",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    stopword_count = sum(1 for token in nlp(text) if token.is_stop)\n    return float(stopword_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.017621807728586632,
        "mean_shap": -0.0009911580569160398,
        "std_shap": 0.021432753553379306,
        "min_shap": -0.03962595040235581,
        "max_shap": 0.07875119228620502
      },
      "rank": 23
    },
    {
      "feature_index": 324,
      "feature_name": "feature_324",
      "feature_code": "def feature(text: str) -> float:\n    'Presence of personal pronouns in the text (I, me, my)'\n    personal_pronouns = {'i', 'me', 'my', 'you', 'your'}\n    words = text.split()\n    if not words:\n        return 0.0\n    pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n    return float(pronoun_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01702918023610171,
        "mean_shap": 0.002869383074195358,
        "std_shap": 0.023126261023463445,
        "min_shap": -0.041865731118054555,
        "max_shap": 0.10024918744420876
      },
      "rank": 24
    },
    {
      "feature_index": 220,
      "feature_name": "feature_220",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation marks to total characters in the text\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    total_chars = len(text)\n    return float(punctuation_count) / total_chars if total_chars > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01610688936283359,
        "mean_shap": 0.002880630376279655,
        "std_shap": 0.022149000857263162,
        "min_shap": -0.07570468234492497,
        "max_shap": 0.05232241699752011
      },
      "rank": 25
    },
    {
      "feature_index": 206,
      "feature_name": "feature_206",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of non-stopwords to total words'\n    doc = nlp(text)\n    non_stopword_count = sum(1 for token in doc if not token.is_stop and token.is_alpha)\n    total_word_count = len(doc)\n    return float(non_stopword_count) / total_word_count if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.016066705948726546,
        "mean_shap": -0.0003148560754302947,
        "std_shap": 0.020803642560282457,
        "min_shap": -0.047223201819431876,
        "max_shap": 0.061702330841072614
      },
      "rank": 26
    },
    {
      "feature_index": 137,
      "feature_name": "feature_137",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text as a measure of complexity\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return punctuation_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.016028674516757616,
        "mean_shap": 0.0027135137219644833,
        "std_shap": 0.02202955223362919,
        "min_shap": -0.07502209574804854,
        "max_shap": 0.04932318076582699
      },
      "rank": 27
    },
    {
      "feature_index": 72,
      "feature_name": "feature_72",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text as a measure of complexity\"\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return punctuation_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.015134920139027957,
        "mean_shap": 0.002253103875317016,
        "std_shap": 0.0208119379862181,
        "min_shap": -0.06845612314311594,
        "max_shap": 0.04926694413988974
      },
      "rank": 28
    },
    {
      "feature_index": 148,
      "feature_name": "feature_148",
      "feature_code": "def feature(text: str) -> float:\n    'Number of nouns in the text'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.015067717817861499,
        "mean_shap": -0.0008805291751822364,
        "std_shap": 0.020742216773432694,
        "min_shap": -0.021498493669583587,
        "max_shap": 0.0745160298488352
      },
      "rank": 29
    },
    {
      "feature_index": 215,
      "feature_name": "feature_215",
      "feature_code": "def feature(text: str) -> float:\n    \"Unique stopword ratio in the text\"\n    doc = nlp(text)\n    stopwords = [token for token in doc if token.is_stop]\n    unique_stopwords = len(set(token.lemma_.lower() for token in stopwords))\n    return float(unique_stopwords) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.014609071372971988,
        "mean_shap": 0.003747934523035,
        "std_shap": 0.018204748358368633,
        "min_shap": -0.03342739447417007,
        "max_shap": 0.08115271559668992
      },
      "rank": 30
    },
    {
      "feature_index": 216,
      "feature_name": "feature_216",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all unique words in the text\"\n    words = set(text.split())\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.012957151399660237,
        "mean_shap": 0.0011657350945460383,
        "std_shap": 0.013961864655106454,
        "min_shap": -0.03349876361502618,
        "max_shap": 0.028043984653511052
      },
      "rank": 31
    },
    {
      "feature_index": 308,
      "feature_name": "feature_308",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of punctuation sequences in the text'\n    punctuation_sequences = re.findall(r'[{0}]+'.format(re.escape(string.punctuation)), text)\n    if not punctuation_sequences:\n        return 0.0\n    return float(sum(len(seq) for seq in punctuation_sequences)) / len(punctuation_sequences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.012459745342162378,
        "mean_shap": 0.002151843527613241,
        "std_shap": 0.020670842895842242,
        "min_shap": -0.09924037356016507,
        "max_shap": 0.02179832254255048
      },
      "rank": 32
    },
    {
      "feature_index": 105,
      "feature_name": "feature_105",
      "feature_code": "def feature(text: str) -> float:\n    'Average position of named entities in the text'\n    doc = nlp(text)\n    entity_positions = [ent.start_char for ent in doc.ents]\n    if not entity_positions:\n        return 0.0\n    return float(sum(entity_positions)) / len(entity_positions)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.011286600844422841,
        "mean_shap": 0.0032549723691605637,
        "std_shap": 0.013402294072643808,
        "min_shap": -0.023589415001045794,
        "max_shap": 0.038778631295341606
      },
      "rank": 33
    },
    {
      "feature_index": 300,
      "feature_name": "feature_300",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of specific parts of speech (e.g., nouns)\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.011269468823441502,
        "mean_shap": -0.00037396885278411093,
        "std_shap": 0.01665780489803601,
        "min_shap": -0.01757409784289266,
        "max_shap": 0.05798158608372364
      },
      "rank": 34
    },
    {
      "feature_index": 98,
      "feature_name": "feature_98",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of passages'\n    passage_sentiments = []\n    passages = text.split('\\n')\n    for passage in passages:\n        blob = TextBlob(passage)\n        passage_sentiments.append(blob.sentiment.polarity)\n    return float(statistics.mean(passage_sentiments)) if passage_sentiments else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.010766340132230167,
        "mean_shap": -0.0005311155686790215,
        "std_shap": 0.013207283834931149,
        "min_shap": -0.03461526919941863,
        "max_shap": 0.05527053975912202
      },
      "rank": 35
    },
    {
      "feature_index": 151,
      "feature_name": "feature_151",
      "feature_code": "def feature(text: str) -> float:\n    \"Weighted count of nouns considering their frequency in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.01074855580455357,
        "mean_shap": -0.000599842597701079,
        "std_shap": 0.015835833575935352,
        "min_shap": -0.017073319884434775,
        "max_shap": 0.05747589898811547
      },
      "rank": 36
    },
    {
      "feature_index": 224,
      "feature_name": "feature_224",
      "feature_code": "def feature(text: str) -> float:\n    'Personal pronoun ratio in the text'\n    doc = nlp(text)\n    personal_pronoun_count = sum(1 for token in doc if token.tag_ in ['PRP', 'PRP$'])\n    return personal_pronoun_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.009645386609646386,
        "mean_shap": -0.00014384210032913984,
        "std_shap": 0.012099808538923059,
        "min_shap": -0.01592033722059725,
        "max_shap": 0.04346581909636183
      },
      "rank": 37
    },
    {
      "feature_index": 398,
      "feature_name": "feature_398",
      "feature_code": "def feature(text: str) -> float:\n    \"Variance of sentiment scores in the text using VADER\"\n    scores = sia.polarity_scores(text)\n    return float(statistics.pstdev([scores['pos'], scores['neg'], scores['neu']])) if any([scores['pos'], scores['neg'], scores['neu']]) else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.009228693874569111,
        "mean_shap": -0.0001320592419278461,
        "std_shap": 0.010900728309461065,
        "min_shap": -0.03089107928424565,
        "max_shap": 0.020689571336352783
      },
      "rank": 38
    },
    {
      "feature_index": 310,
      "feature_name": "feature_310",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are longer than the average word length\"\n    words = text.split()\n    average_length = sum(len(word) for word in words) / len(words) if words else 0.0\n    long_words_count = sum(1 for word in words if len(word) > average_length)\n    return float(long_words_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008956402712327496,
        "mean_shap": 0.0011496764191354163,
        "std_shap": 0.013047951390800032,
        "min_shap": -0.06791813120771009,
        "max_shap": 0.03301945866753626
      },
      "rank": 39
    },
    {
      "feature_index": 456,
      "feature_name": "feature_456",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of complex words in the text (more than two syllables)\"\n    complexity_count = len(re.findall(r'\\b\\w*[aeiou]{1}\\w*[aeiou]{1}\\w*\\b', text))\n    word_count = len(text.split())\n    return (complexity_count / word_count) if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00895067867229775,
        "mean_shap": -0.0002802972389885515,
        "std_shap": 0.010573870917970114,
        "min_shap": -0.02608226785834413,
        "max_shap": 0.021479623739111754
      },
      "rank": 40
    },
    {
      "feature_index": 83,
      "feature_name": "feature_83",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of conjunctions to total words in the text'\n    doc = nlp(text)\n    conjunctions = sum(1 for token in doc if token.dep_ == 'cc')\n    total_words = len(doc)\n    return float(conjunctions) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008590363694587053,
        "mean_shap": -0.0016227343028156818,
        "std_shap": 0.010629668950833024,
        "min_shap": -0.02785101003345131,
        "max_shap": 0.03380571755053043
      },
      "rank": 41
    },
    {
      "feature_index": 269,
      "feature_name": "feature_269",
      "feature_code": "def feature(text: str) -> float:\n    'Count of sentences containing the word \"I\" or \"me\"'\n    sentences = re.split(r'[.!?]+', text)\n    i_count = sum(1 for s in sentences if ' I ' in s or ' me ' in s)\n    return float(i_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008495024414843374,
        "mean_shap": -0.0004515685814891405,
        "std_shap": 0.010399721684518536,
        "min_shap": -0.0254382685488874,
        "max_shap": 0.027163063179899676
      },
      "rank": 42
    },
    {
      "feature_index": 339,
      "feature_name": "feature_339",
      "feature_code": "def feature(text: str) -> float:\n    'Average character length between punctuations'\n    segments = re.split(r'[,.!?]', text)\n    lengths = [len(segment.strip()) for segment in segments if segment.strip()]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008374379434509213,
        "mean_shap": -0.000257938931875402,
        "std_shap": 0.010356625953118134,
        "min_shap": -0.01865704040274787,
        "max_shap": 0.030260600614198895
      },
      "rank": 43
    },
    {
      "feature_index": 452,
      "feature_name": "feature_452",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of the longest word length to average word length'\n    words = text.split()\n    if not words:\n        return 0.0\n    longest_word_length = max(len(word) for word in words)\n    average_word_length = sum(len(word) for word in words) / len(words)\n    return longest_word_length / average_word_length if average_word_length > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008319820442685,
        "mean_shap": 0.0002622259673345786,
        "std_shap": 0.012118560775561239,
        "min_shap": -0.07841294495581634,
        "max_shap": 0.019784924687024065
      },
      "rank": 44
    },
    {
      "feature_index": 47,
      "feature_name": "feature_47",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase letters to total letters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    total_letters = sum(1 for c in text if c.isalpha())\n    return (uppercase_count / total_letters) if total_letters else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.008163916466174223,
        "mean_shap": 0.0011669229318353618,
        "std_shap": 0.009696244820346379,
        "min_shap": -0.0145999307387531,
        "max_shap": 0.030035764657937974
      },
      "rank": 45
    },
    {
      "feature_index": 253,
      "feature_name": "feature_253",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of sentence fragments per sentence\"\n    sentences = re.split(r'[.!?]+', text)\n    fragments = sum(len(s.split(',')) - 1 for s in sentences)  # Count commas as fragments\n    avg_fragments = fragments / len(sentences) if sentences else 0.0\n    return avg_fragments\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007998521211807535,
        "mean_shap": 0.0007398828489935657,
        "std_shap": 0.009155726811287117,
        "min_shap": -0.01978209149156931,
        "max_shap": 0.01912709961991378
      },
      "rank": 46
    },
    {
      "feature_index": 103,
      "feature_name": "feature_103",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences with multiple clauses (using commas)\"\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    complex_sentences = sum(1 for s in sentences if s.count(',') > 1)\n    return float(complex_sentences) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007916773777966034,
        "mean_shap": 0.0008194676247988285,
        "std_shap": 0.009765824083242766,
        "min_shap": -0.02183289866659043,
        "max_shap": 0.05292745863022447
      },
      "rank": 47
    },
    {
      "feature_index": 31,
      "feature_name": "feature_31",
      "feature_code": "def feature(text: str) -> float:\n    'Sum of unique word lengths in the text'\n    words = set(text.split())\n    if not words:\n        return 0.0\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007897464378815937,
        "mean_shap": 0.0006932598237056759,
        "std_shap": 0.008655108500929511,
        "min_shap": -0.022501672771021223,
        "max_shap": 0.016609436477839894
      },
      "rank": 48
    },
    {
      "feature_index": 468,
      "feature_name": "feature_468",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of conjunctions to total words in the text\"\n    doc = nlp(text)\n    conjunction_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conjunction_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007460677545084638,
        "mean_shap": -0.00047107946569491303,
        "std_shap": 0.009494160667860938,
        "min_shap": -0.026573348771359414,
        "max_shap": 0.033687257527629993
      },
      "rank": 49
    },
    {
      "feature_index": 251,
      "feature_name": "feature_251",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words to total words, indicating lexical difficulty\"\n    words = text.split()\n    complex_words_count = sum(1 for word in words if len(word) > 2 and sum(c.isalpha() for c in word) > 2)\n    return float(complex_words_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0073071756699550775,
        "mean_shap": 0.0004570693488947182,
        "std_shap": 0.00861413976208246,
        "min_shap": -0.023752496450656406,
        "max_shap": 0.017163183902844097
      },
      "rank": 50
    },
    {
      "feature_index": 135,
      "feature_name": "feature_135",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of personal pronouns to indicate personal engagement\"\n    personal_pronouns = {'i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'}\n    words = text.lower().split()\n    personal_count = sum(1 for word in words if word in personal_pronouns)\n    return float(personal_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00729683945323784,
        "mean_shap": -0.0005772659029013794,
        "std_shap": 0.008710111521167609,
        "min_shap": -0.013842834899033558,
        "max_shap": 0.023492463572786795
      },
      "rank": 51
    },
    {
      "feature_index": 374,
      "feature_name": "feature_374",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of question marks to total punctuation in the text\"\n    question_marks = text.count('?')\n    total_punctuation = sum(1 for c in text if c in string.punctuation)\n    return question_marks / total_punctuation if total_punctuation > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007289083606048694,
        "mean_shap": -0.003673171165804577,
        "std_shap": 0.00848818120696464,
        "min_shap": -0.030050866573326816,
        "max_shap": 0.012502890556581141
      },
      "rank": 52
    },
    {
      "feature_index": 71,
      "feature_name": "feature_71",
      "feature_code": "def feature(text: str) -> float:\n    'Sentence complexity measured by average number of clauses (using conjunctions)'\n    clauses = text.count(',') + text.count('and') + text.count('but') + text.count(';')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count == 0:\n        return 0.0\n    return float(clauses) / sentence_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007078154246689834,
        "mean_shap": 0.0016913519793193092,
        "std_shap": 0.012111116082525575,
        "min_shap": -0.0821371488161417,
        "max_shap": 0.030039291559666136
      },
      "rank": 53
    },
    {
      "feature_index": 125,
      "feature_name": "feature_125",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase letters to total letters in the text'\n    upper_count = sum(1 for c in text if c.isupper())\n    total_count = sum(1 for c in text if c.isalpha())\n    if total_count == 0:\n        return 0.0\n    return upper_count / total_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.007025826108981014,
        "mean_shap": 0.0009557725366013929,
        "std_shap": 0.00839370710785441,
        "min_shap": -0.014458985349149113,
        "max_shap": 0.02456723390280659
      },
      "rank": 54
    },
    {
      "feature_index": 371,
      "feature_name": "feature_371",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of noun phrases to total words in the text\"\n    doc = nlp(text)\n    noun_phrases = sum(1 for chunk in doc.noun_chunks)\n    word_count = len(doc)\n    return noun_phrases / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006883128055065836,
        "mean_shap": -0.00039724327575450065,
        "std_shap": 0.008795642096513987,
        "min_shap": -0.025613912635477118,
        "max_shap": 0.02660935602149914
      },
      "rank": 55
    },
    {
      "feature_index": 394,
      "feature_name": "feature_394",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are stopwords in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    stopword_count = sum(1 for word in words if nlp(word)[0].is_stop)\n    return stopword_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0067238581508823345,
        "mean_shap": 4.7819128405451315e-05,
        "std_shap": 0.009512496056713825,
        "min_shap": -0.010038574416614439,
        "max_shap": 0.044664682770729734
      },
      "rank": 56
    },
    {
      "feature_index": 123,
      "feature_name": "feature_123",
      "feature_code": "def feature(text: str) -> float:\n    'Frequency of definite articles (\"the\") in the text'\n    return float(text.lower().count('the'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006709900555826357,
        "mean_shap": 0.00027191280014603203,
        "std_shap": 0.007443297048041518,
        "min_shap": -0.013968633828582518,
        "max_shap": 0.015644066847443915
      },
      "rank": 57
    },
    {
      "feature_index": 391,
      "feature_name": "feature_391",
      "feature_code": "def feature(text: str) -> float:\n    'Count of emotional words using VADER'\n    scores = sia.polarity_scores(text)\n    emotional_words_count = scores['pos'] + scores['neg']\n    return float(emotional_words_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006483458902951102,
        "mean_shap": -0.0001289523758782731,
        "std_shap": 0.007706287344561568,
        "min_shap": -0.017253263902237918,
        "max_shap": 0.01904778577360646
      },
      "rank": 58
    },
    {
      "feature_index": 237,
      "feature_name": "feature_237",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of unique verbs relative to total verbs'\n    doc = nlp(text)\n    total_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB'))\n    return unique_verbs / total_verbs if total_verbs > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0064593075713565914,
        "mean_shap": 0.00019577741037745636,
        "std_shap": 0.007287145762479102,
        "min_shap": -0.016251782427792502,
        "max_shap": 0.014606381809038841
      },
      "rank": 59
    },
    {
      "feature_index": 132,
      "feature_name": "feature_132",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of uppercase words in the text'\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return uppercase_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006332855270635055,
        "mean_shap": 0.00028903233173358683,
        "std_shap": 0.00786948508394075,
        "min_shap": -0.02430715283365309,
        "max_shap": 0.012891791708039991
      },
      "rank": 60
    },
    {
      "feature_index": 270,
      "feature_name": "feature_270",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words in the text (words with three or more syllables)\"\n    complex_words = len(re.findall(r'\\b\\w{3,}\\b', text))\n    total_words = len(text.split())\n    if total_words == 0:\n        return 0.0\n    return float(complex_words) / total_words\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006296588744159101,
        "mean_shap": 0.0007387894521663493,
        "std_shap": 0.007554113807390456,
        "min_shap": -0.029984188418707626,
        "max_shap": 0.01204426383985771
      },
      "rank": 61
    },
    {
      "feature_index": 91,
      "feature_name": "feature_91",
      "feature_code": "def feature(text: str) -> float:\n    'Punctuation-to-word ratio in the text'\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words_count = len(text.split())\n    if words_count == 0:\n        return 0.0\n    return float(punctuation_count) / words_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006086033804296286,
        "mean_shap": -0.00020852641864928508,
        "std_shap": 0.007446804006185605,
        "min_shap": -0.021989564099021718,
        "max_shap": 0.012189132948969563
      },
      "rank": 62
    },
    {
      "feature_index": 20,
      "feature_name": "feature_20",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words in the text\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.006036413029153606,
        "mean_shap": 0.000270932142947967,
        "std_shap": 0.0076945086858954525,
        "min_shap": -0.02471863275747695,
        "max_shap": 0.012541406981915288
      },
      "rank": 63
    },
    {
      "feature_index": 373,
      "feature_name": "feature_373",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase to total words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005976104769909397,
        "mean_shap": 0.00013195404208148342,
        "std_shap": 0.007311929041444633,
        "min_shap": -0.021222114030301723,
        "max_shap": 0.014824428710214586
      },
      "rank": 64
    },
    {
      "feature_index": 227,
      "feature_name": "feature_227",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation counts per sentence.\"\n    sentences = re.split(r'[.!?]+', text)\n    punctuation_counts = [sum(1 for c in s if c in string.punctuation) for s in sentences]\n    return float(sum(punctuation_counts)) / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005947010937426023,
        "mean_shap": -0.00012255530728037288,
        "std_shap": 0.007366083220465124,
        "min_shap": -0.023442739043408243,
        "max_shap": 0.00897945603842496
      },
      "rank": 65
    },
    {
      "feature_index": 155,
      "feature_name": "feature_155",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of uppercase words in the text'\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return uppercase_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00582687902593455,
        "mean_shap": 0.00016833080277335605,
        "std_shap": 0.0071358337021150305,
        "min_shap": -0.02231603560222383,
        "max_shap": 0.013215126979196605
      },
      "rank": 66
    },
    {
      "feature_index": 101,
      "feature_name": "feature_101",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of uppercase words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005744568395005893,
        "mean_shap": 0.00047893740738941525,
        "std_shap": 0.007076369947123778,
        "min_shap": -0.022811930927683043,
        "max_shap": 0.012570604128454
      },
      "rank": 67
    },
    {
      "feature_index": 63,
      "feature_name": "feature_63",
      "feature_code": "def feature(text: str) -> float:\n    'Punctuation-to-word ratio in the text'\n    punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words_count = len(text.split())\n    if words_count == 0:\n        return 0.0\n    return float(punctuation_count) / words_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00571805569421452,
        "mean_shap": -4.329427802245118e-05,
        "std_shap": 0.007296710661128241,
        "min_shap": -0.02543356473356765,
        "max_shap": 0.013189538842376197
      },
      "rank": 68
    },
    {
      "feature_index": 3,
      "feature_name": "feature_3",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words in the text\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005546706241782453,
        "mean_shap": 2.075861100111007e-05,
        "std_shap": 0.006723732934239713,
        "min_shap": -0.020296604294043466,
        "max_shap": 0.011891873797708839
      },
      "rank": 69
    },
    {
      "feature_index": 275,
      "feature_name": "feature_275",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of words that are adverbs in the text'\n    doc = nlp(text)\n    word_count = len(doc)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count) / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005416361922536572,
        "mean_shap": -0.000552820189768779,
        "std_shap": 0.00689912845239259,
        "min_shap": -0.01966429465753057,
        "max_shap": 0.01760004237058233
      },
      "rank": 70
    },
    {
      "feature_index": 15,
      "feature_name": "feature_15",
      "feature_code": "def feature(text: str) -> float:\n    \"Total length of text excluding punctuation\"\n    text_no_punct = re.sub(r'[{}]+'.format(string.punctuation), '', text)\n    return float(len(text_no_punct))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005386906080191327,
        "mean_shap": -0.0002813289133997762,
        "std_shap": 0.007044979253282414,
        "min_shap": -0.00658720126039358,
        "max_shap": 0.026536900977007182
      },
      "rank": 71
    },
    {
      "feature_index": 387,
      "feature_name": "feature_387",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words in the text\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    upper_count = sum(1 for word in words if word.isupper())\n    return float(upper_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00531746609497805,
        "mean_shap": 0.00032424278613109894,
        "std_shap": 0.00643742526254098,
        "min_shap": -0.0178473903783356,
        "max_shap": 0.010926575603816262
      },
      "rank": 72
    },
    {
      "feature_index": 6,
      "feature_name": "feature_6",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of uppercase words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return float(uppercase_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005257950750693018,
        "mean_shap": 0.00036771586377018034,
        "std_shap": 0.006435001585219536,
        "min_shap": -0.017956717603711068,
        "max_shap": 0.012704223396065487
      },
      "rank": 73
    },
    {
      "feature_index": 231,
      "feature_name": "feature_231",
      "feature_code": "def feature(text: str) -> float:\n    \"Cohesion measure based on repeated phrases (n-grams)\"\n    ngrams = {text[i:i+2] for i in range(len(text)-1)}  # bi-grams\n    return float(len(ngrams)) / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0052330594027139845,
        "mean_shap": 0.00043525883861352405,
        "std_shap": 0.00687537818974274,
        "min_shap": -0.015982245218197605,
        "max_shap": 0.01902336549941023
      },
      "rank": 74
    },
    {
      "feature_index": 38,
      "feature_name": "feature_38",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of uppercase words in the text'\n    words = text.split()\n    uppercase_count = sum(1 for word in words if word.isupper())\n    return uppercase_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0052266402453802685,
        "mean_shap": 0.00011295820143901217,
        "std_shap": 0.006587828844714462,
        "min_shap": -0.019469704673808937,
        "max_shap": 0.01415405399676619
      },
      "rank": 75
    },
    {
      "feature_index": 359,
      "feature_name": "feature_359",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of abstract nouns to total nouns in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    abstract_noun_count = sum(1 for token in doc if token.pos_ == 'NOUN' and token.dep_ in ['nsubj', 'dobj'])\n    return abstract_noun_count / noun_count if noun_count > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.005036794348525112,
        "mean_shap": -0.00034452172560841366,
        "std_shap": 0.00673175126727538,
        "min_shap": -0.018362426751653747,
        "max_shap": 0.026426615604887335
      },
      "rank": 76
    },
    {
      "feature_index": 382,
      "feature_name": "feature_382",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of uppercase words to total words\"\n    words = text.split()\n    upper_count = sum(1 for word in words if word.isupper())\n    return upper_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004704067313198848,
        "mean_shap": 0.00010382442012350859,
        "std_shap": 0.005857903912949363,
        "min_shap": -0.01790712884285018,
        "max_shap": 0.009964252022809539
      },
      "rank": 77
    },
    {
      "feature_index": 241,
      "feature_name": "feature_241",
      "feature_code": "def feature(text: str) -> float:\n    'Named entity type diversity as a ratio'\n    doc = nlp(text)\n    unique_entity_types = len(set(ent.label_ for ent in doc.ents))\n    return float(unique_entity_types) / (len(doc.ents) if len(doc.ents) > 0 else 1)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0046998238713242485,
        "mean_shap": 0.00045128807249815076,
        "std_shap": 0.005149576364150546,
        "min_shap": -0.011019911161784527,
        "max_shap": 0.009476213706968458
      },
      "rank": 78
    },
    {
      "feature_index": 393,
      "feature_name": "feature_393",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of questions in the text\"\n    doc = nlp(text)\n    question_count = sum(1 for sent in doc.sents if sent.text.strip().endswith('?'))\n    return float(question_count) / len(list(doc.sents)) if doc.sents else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004522653088660374,
        "mean_shap": -0.0008947106738382875,
        "std_shap": 0.005435226719264111,
        "min_shap": -0.01636641094683065,
        "max_shap": 0.008227168000954037
      },
      "rank": 79
    },
    {
      "feature_index": 102,
      "feature_name": "feature_102",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical diversity based on unique bigrams in the text'\n    doc = nlp(text)\n    bigrams = set((doc[i].text, doc[i+1].text) for i in range(len(doc)-1))\n    return float(len(bigrams)) / (len(doc) - 1) if len(doc) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004334096997299404,
        "mean_shap": 4.398741762589533e-05,
        "std_shap": 0.006385559535357035,
        "min_shap": -0.015288443841727284,
        "max_shap": 0.02956632780270836
      },
      "rank": 80
    },
    {
      "feature_index": 108,
      "feature_name": "feature_108",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical diversity based on character n-grams'\n    n_grams = set(text[i:i+2] for i in range(len(text)-1))\n    return float(len(n_grams)) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004296973349703607,
        "mean_shap": 0.00033164571359264763,
        "std_shap": 0.005659897718245549,
        "min_shap": -0.016852266682288827,
        "max_shap": 0.014156277718103348
      },
      "rank": 81
    },
    {
      "feature_index": 328,
      "feature_name": "feature_328",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of questions in the text based on question marks\"\n    question_count = text.count('?')\n    total_sentences = text.count('.') + text.count('!') + question_count\n    return float(question_count) / total_sentences if total_sentences > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004205702266644937,
        "mean_shap": -0.0010451971267894228,
        "std_shap": 0.004778790935691794,
        "min_shap": -0.014958882915020604,
        "max_shap": 0.007233996439420055
      },
      "rank": 82
    },
    {
      "feature_index": 440,
      "feature_name": "feature_440",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique named entities in the text divided by total words'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    unique_entities = len(set(ent.text for ent in doc.ents))\n    return unique_entities / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004145252389220631,
        "mean_shap": -0.0012087672218092814,
        "std_shap": 0.006182365371879395,
        "min_shap": -0.009354816676562235,
        "max_shap": 0.033557611960973066
      },
      "rank": 83
    },
    {
      "feature_index": 449,
      "feature_name": "feature_449",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences that contain modal verbs in the text\"\n    doc = nlp(text)\n    modal_verbs = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would'}\n    modal_sentences = sum(1 for sent in doc.sents if any(token.lemma_.lower() in modal_verbs for token in sent))\n    return float(modal_sentences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.004094400095511645,
        "mean_shap": -0.0003307866911267785,
        "std_shap": 0.0057088155653688055,
        "min_shap": -0.012861859128142689,
        "max_shap": 0.023617688663244134
      },
      "rank": 84
    },
    {
      "feature_index": 199,
      "feature_name": "feature_199",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words (longer than 3 letters) to total words in the text\"\n    words = text.split()\n    complex_word_count = sum(1 for word in words if len(word) > 3)\n    return float(complex_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003963861222987254,
        "mean_shap": -6.761223625753264e-05,
        "std_shap": 0.0052655557523018015,
        "min_shap": -0.019529993762919313,
        "max_shap": 0.010013440422696631
      },
      "rank": 85
    },
    {
      "feature_index": 313,
      "feature_name": "feature_313",
      "feature_code": "def feature(text: str) -> float:\n    \"Total number of syllables in the text\"\n    words = text.split()\n    total_syllables = sum(textstat.syllable_count(word) for word in words)\n    return float(total_syllables)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003938303213620799,
        "mean_shap": -8.024810903931849e-05,
        "std_shap": 0.00719713880509642,
        "min_shap": -0.01411497797188473,
        "max_shap": 0.03152980984130112
      },
      "rank": 86
    },
    {
      "feature_index": 152,
      "feature_name": "feature_152",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique capitalization forms in the text\"\n    words = text.split()\n    unique_capitalized = len(set(word for word in words if word[0].isupper()))\n    return float(unique_capitalized)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003925042616376214,
        "mean_shap": 0.0011938760973506195,
        "std_shap": 0.005832365534013287,
        "min_shap": -0.008378637739722763,
        "max_shap": 0.042664256329132066
      },
      "rank": 87
    },
    {
      "feature_index": 245,
      "feature_name": "feature_245",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of all lowercase words in the text\"\n    lowercase_count = sum(1 for word in text.split() if word.islower())\n    return float(lowercase_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003806795734484629,
        "mean_shap": 0.0004952376840876783,
        "std_shap": 0.004605822671590752,
        "min_shap": -0.00905983947158918,
        "max_shap": 0.01594307280839252
      },
      "rank": 88
    },
    {
      "feature_index": 183,
      "feature_name": "feature_183",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique adjectives in the text'\n    doc = nlp(text)\n    unique_adjectives = len(set(token.text for token in doc if token.pos_ == 'ADJ'))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003715839320628228,
        "mean_shap": -0.0006588145010716703,
        "std_shap": 0.0042396913467414084,
        "min_shap": -0.008366653722244384,
        "max_shap": 0.011166472503512128
      },
      "rank": 89
    },
    {
      "feature_index": 228,
      "feature_name": "feature_228",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of sentences to total words in the text\"\n    sentence_count = len(re.findall(r'[.!?]', text))\n    total_words = len(text.split())\n    return float(sentence_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003698427777724084,
        "mean_shap": -0.0005354775882584229,
        "std_shap": 0.004686516905690772,
        "min_shap": -0.022746496384457256,
        "max_shap": 0.006679913984786206
      },
      "rank": 90
    },
    {
      "feature_index": 322,
      "feature_name": "feature_322",
      "feature_code": "def feature(text: str) -> float:\n    'Count of phrases with modal verbs (e.g., can, should, might)'\n    modal_verbs = ['can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would']\n    return float(sum(1 for word in text.lower().split() if word in modal_verbs))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0036914354252236153,
        "mean_shap": -1.1592069643081532e-05,
        "std_shap": 0.004796920558100464,
        "min_shap": -0.008344447127630734,
        "max_shap": 0.01615441485926072
      },
      "rank": 91
    },
    {
      "feature_index": 184,
      "feature_name": "feature_184",
      "feature_code": "def feature(text: str) -> float:\n    \"Frequency of modal verbs in the text\"\n    doc = nlp(text)\n    modal_verbs = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for token in doc if token.text.lower() in modal_verbs)\n    return float(count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0035013076679727253,
        "mean_shap": -0.00044024930436316876,
        "std_shap": 0.005359075773764502,
        "min_shap": -0.008258152619907269,
        "max_shap": 0.029214750005869965
      },
      "rank": 92
    },
    {
      "feature_index": 154,
      "feature_name": "feature_154",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase to total characters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    if len(text) == 0:\n        return 0.0\n    return float(uppercase_count) / len(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0034842960111878267,
        "mean_shap": 0.00037652338066167723,
        "std_shap": 0.004015461270227514,
        "min_shap": -0.006860184824017121,
        "max_shap": 0.011687525224125764
      },
      "rank": 93
    },
    {
      "feature_index": 26,
      "feature_name": "feature_26",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of question marks to total sentences in the text\"\n    sentences = text.count('?') + text.count('.') + text.count('!')  \n    if sentences == 0:\n        return 0.0\n    return float(text.count('?')) / sentences\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003483272230950045,
        "mean_shap": -0.0008546227630026564,
        "std_shap": 0.003962278600318297,
        "min_shap": -0.012405044622234967,
        "max_shap": 0.007132557883355285
      },
      "rank": 94
    },
    {
      "feature_index": 205,
      "feature_name": "feature_205",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of negations in the text to assess negative framing'\n    neg_words = {'no', 'not', 'never', 'none', 'neither', 'nobody'}\n    doc = nlp(text)\n    neg_count = sum(1 for token in doc if token.text.lower() in neg_words)\n    return float(neg_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0033833005773739534,
        "mean_shap": -0.0003306381724129656,
        "std_shap": 0.003996353408346908,
        "min_shap": -0.014092118910674443,
        "max_shap": 0.004915115594308454
      },
      "rank": 95
    },
    {
      "feature_index": 464,
      "feature_name": "feature_464",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment variance in the text using TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    scores = [sentence.sentiment.polarity for sentence in blob.sentences]\n    return float(statistics.pstdev(scores)) if scores else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0033723513237507367,
        "mean_shap": -0.00017301631182297633,
        "std_shap": 0.004232143714136246,
        "min_shap": -0.009973646929179484,
        "max_shap": 0.012476844196532473
      },
      "rank": 96
    },
    {
      "feature_index": 353,
      "feature_name": "feature_353",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words\"\n    sentences = text.split('.')\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0033677250024668885,
        "mean_shap": -8.158554832564289e-05,
        "std_shap": 0.004820967005434033,
        "min_shap": -0.019288097494396653,
        "max_shap": 0.007568344941367935
      },
      "rank": 97
    },
    {
      "feature_index": 106,
      "feature_name": "feature_106",
      "feature_code": "def feature(text: str) -> float:\n    'Number of distinct words used in the text'\n    words = text.split()\n    unique_word_count = len(set(word.lower() for word in words if word.isalpha()))\n    return float(unique_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0032639817835119884,
        "mean_shap": 0.000170438709448044,
        "std_shap": 0.006603514442708561,
        "min_shap": -0.008597499208129591,
        "max_shap": 0.039323748419342906
      },
      "rank": 98
    },
    {
      "feature_index": 114,
      "feature_name": "feature_114",
      "feature_code": "def feature(text: str) -> float:\n    \"Density of rare words based on frequency (using a simple threshold)\"\n    rare_word_count = sum(1 for token in nlp(text) if token.is_alpha and len(token) > 6 and not token.is_stop)\n    total_word_count = len(text.split())\n    return (rare_word_count / total_word_count) if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0031922780405514873,
        "mean_shap": -0.00053927395030063,
        "std_shap": 0.004674090463243443,
        "min_shap": -0.008834807932087303,
        "max_shap": 0.02831017333215642
      },
      "rank": 99
    },
    {
      "feature_index": 285,
      "feature_name": "feature_285",
      "feature_code": "def feature(text: str) -> float:\n    'Count of dependent clauses in the text'\n    doc = nlp(text)\n    dependents = sum(1 for token in doc if token.dep_ in {'ccomp', 'acl'})\n    return float(dependents)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003120659147861495,
        "mean_shap": -0.0006340230206880207,
        "std_shap": 0.0037670784085180706,
        "min_shap": -0.012914854051749101,
        "max_shap": 0.0056843288197792885
      },
      "rank": 100
    },
    {
      "feature_index": 416,
      "feature_name": "feature_416",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are longer than 5 characters'\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 5)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0031134640825125804,
        "mean_shap": -0.00022301177303294645,
        "std_shap": 0.004147632404400448,
        "min_shap": -0.008479538731474532,
        "max_shap": 0.020941250441644393
      },
      "rank": 101
    },
    {
      "feature_index": 368,
      "feature_name": "feature_368",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of capital letters in the text'\n    capital_letters = sum(1 for c in text if c.isupper())\n    return float(capital_letters) / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0031047075812743875,
        "mean_shap": 0.00037452735770124865,
        "std_shap": 0.003757899078907348,
        "min_shap": -0.0057645197572106345,
        "max_shap": 0.013846103385579253
      },
      "rank": 102
    },
    {
      "feature_index": 238,
      "feature_name": "feature_238",
      "feature_code": "def feature(text: str) -> float:\n    'Average syllables per word in the text'\n    words = text.split()\n    syllable_count = sum(textstat.syllable_count(word) for word in words if word.isalpha())\n    return float(syllable_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.003034396957948863,
        "mean_shap": 0.0005373982019363552,
        "std_shap": 0.004627871277953027,
        "min_shap": -0.026665316598304782,
        "max_shap": 0.014365903827182959
      },
      "rank": 103
    },
    {
      "feature_index": 235,
      "feature_name": "feature_235",
      "feature_code": "def feature(text: str) -> float:\n    'Count of dependent clauses in the text'\n    doc = nlp(text)\n    dependents = sum(1 for token in doc if token.dep_ in {'ccomp', 'acl'})\n    return float(dependents)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0030124682812830883,
        "mean_shap": -0.0004159931939637813,
        "std_shap": 0.0035801906155413506,
        "min_shap": -0.009378328544116082,
        "max_shap": 0.006538584266311501
      },
      "rank": 104
    },
    {
      "feature_index": 395,
      "feature_name": "feature_395",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of words that are adjacent and match the same part of speech'\n    doc = nlp(text)\n    pos_pairs = sum(1 for token1, token2 in itertools.pairwise(doc) if token1.pos_ == token2.pos_)\n    return float(pos_pairs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0029404817412005476,
        "mean_shap": 0.0004977351313736538,
        "std_shap": 0.0034912941049392147,
        "min_shap": -0.011293039866581945,
        "max_shap": 0.008659833151506176
      },
      "rank": 105
    },
    {
      "feature_index": 218,
      "feature_name": "feature_218",
      "feature_code": "def feature(text: str) -> float:\n    'Count of conjunctions in the text'\n    doc = nlp(text)\n    conjunction_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conjunction_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002886705777425489,
        "mean_shap": -0.000621351152922885,
        "std_shap": 0.0031792486599088086,
        "min_shap": -0.006893109583964001,
        "max_shap": 0.006761690670274659
      },
      "rank": 106
    },
    {
      "feature_index": 279,
      "feature_name": "feature_279",
      "feature_code": "def feature(text: str) -> float:\n    \"Average character length of nouns in the text\"\n    doc = nlp(text)\n    noun_lengths = [len(token.text) for token in doc if token.pos_ == 'NOUN']\n    return float(sum(noun_lengths)) / len(noun_lengths) if noun_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027888325261622438,
        "mean_shap": 0.0004635862450237351,
        "std_shap": 0.0036162906824330357,
        "min_shap": -0.01392668271841119,
        "max_shap": 0.006110898869288249
      },
      "rank": 107
    },
    {
      "feature_index": 458,
      "feature_name": "feature_458",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of clauses per sentence'\n    sentences = re.split(r'[.!?]+', text)\n    clause_count = sum(len(re.findall(r',', s)) + 1 for s in sentences if s.strip())\n    return clause_count / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027504905975917443,
        "mean_shap": 0.0001929116318881939,
        "std_shap": 0.003330190603782048,
        "min_shap": -0.009781459897938261,
        "max_shap": 0.011454231665184928
      },
      "rank": 108
    },
    {
      "feature_index": 68,
      "feature_name": "feature_68",
      "feature_code": "def feature(text: str) -> float:\n    'Named entity count in the text'\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027463762794999237,
        "mean_shap": 0.0008378470470685768,
        "std_shap": 0.003815268942715617,
        "min_shap": -0.008001738491973723,
        "max_shap": 0.01896222790653787
      },
      "rank": 109
    },
    {
      "feature_index": 386,
      "feature_name": "feature_386",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of unique lemmas in text\"\n    doc = nlp(text)\n    unique_lemmas = set(token.lemma_.lower() for token in doc if token.is_alpha)\n    if not unique_lemmas:\n        return 0.0\n    return float(sum(len(lemma) for lemma in unique_lemmas)) / len(unique_lemmas)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0027209852610247036,
        "mean_shap": -0.0001791202644055123,
        "std_shap": 0.003933207640936085,
        "min_shap": -0.014402631552849767,
        "max_shap": 0.013060178101724766
      },
      "rank": 110
    },
    {
      "feature_index": 149,
      "feature_name": "feature_149",
      "feature_code": "def feature(text: str) -> float:\n    'Count of negation words in the text'\n    negation_words = {'not', 'no', 'never', 'nobody', 'nothing', 'nowhere'}\n    words = text.lower().split()\n    negation_count = sum(1 for word in words if word in negation_words)\n    return float(negation_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002661678521120871,
        "mean_shap": -0.0001875229971407683,
        "std_shap": 0.003105961734444534,
        "min_shap": -0.0104590925505182,
        "max_shap": 0.003559345363532499
      },
      "rank": 111
    },
    {
      "feature_index": 229,
      "feature_name": "feature_229",
      "feature_code": "def feature(text: str) -> float:\n    'Count of conjunctions in the text'\n    doc = nlp(text)\n    conjunction_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conjunction_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0026462051269308994,
        "mean_shap": -0.0005230056950564317,
        "std_shap": 0.0030483020262869333,
        "min_shap": -0.005976946869212624,
        "max_shap": 0.00641483831031561
      },
      "rank": 112
    },
    {
      "feature_index": 57,
      "feature_name": "feature_57",
      "feature_code": "def feature(text: str) -> float:\n    'Fraction of uppercase letters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    return uppercase_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0026199470374549364,
        "mean_shap": 0.00023084917200352802,
        "std_shap": 0.003241421551382759,
        "min_shap": -0.006019500142486621,
        "max_shap": 0.01201757962191208
      },
      "rank": 113
    },
    {
      "feature_index": 309,
      "feature_name": "feature_309",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are recognized named entities in the text\"\n    words = text.split()\n    doc = nlp(text)\n    entity_words = {ent.text.lower() for ent in doc.ents}\n    count_of_entity_words = sum(1 for word in words if word.lower() in entity_words)\n    return float(count_of_entity_words) / len(words) if len(words) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002600259815054991,
        "mean_shap": 0.0003049476742032141,
        "std_shap": 0.0034467242134793652,
        "min_shap": -0.00924806985087302,
        "max_shap": 0.016945507397522663
      },
      "rank": 114
    },
    {
      "feature_index": 263,
      "feature_name": "feature_263",
      "feature_code": "def feature(text: str) -> float:\n    \"Density of complex words (3+ syllables) in the text\"\n    complex_words = len([word for word in text.split() if textstat.syllable_count(word) > 3])\n    return float(complex_words) / len(text.split()) if len(text.split()) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002543679523327851,
        "mean_shap": 1.0001643357912385e-05,
        "std_shap": 0.003165615651770934,
        "min_shap": -0.011156005312370878,
        "max_shap": 0.006807909749715282
      },
      "rank": 115
    },
    {
      "feature_index": 287,
      "feature_name": "feature_287",
      "feature_code": "def feature(text: str) -> float:\n    'Count of sentences with more than 10 words'\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 10)\n    return float(long_sentence_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0025313114609444177,
        "mean_shap": -1.067633588461561e-05,
        "std_shap": 0.0031178205078515957,
        "min_shap": -0.007026180086635383,
        "max_shap": 0.011529388232185639
      },
      "rank": 116
    },
    {
      "feature_index": 138,
      "feature_name": "feature_138",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of nouns in the text\"\n    doc = nlp(text)\n    noun_lengths = [len(token.text) for token in doc if token.pos_ == 'NOUN']\n    return float(sum(noun_lengths)) / len(noun_lengths) if noun_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0025140494412178626,
        "mean_shap": 0.0004315741514087264,
        "std_shap": 0.0033867176476184767,
        "min_shap": -0.01583190882561257,
        "max_shap": 0.006057094542343096
      },
      "rank": 117
    },
    {
      "feature_index": 286,
      "feature_name": "feature_286",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of unique lemmas in text\"\n    doc = nlp(text)\n    unique_lemmas = set(token.lemma_.lower() for token in doc if token.is_alpha)\n    if not unique_lemmas:\n        return 0.0\n    return float(sum(len(lemma) for lemma in unique_lemmas)) / len(unique_lemmas)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002459760256392789,
        "mean_shap": -7.290697876659372e-05,
        "std_shap": 0.003514106635808258,
        "min_shap": -0.012957810596793282,
        "max_shap": 0.010204781080884505
      },
      "rank": 118
    },
    {
      "feature_index": 107,
      "feature_name": "feature_107",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of uppercase letters to total characters'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    total_count = len(text)\n    return float(uppercase_count) / total_count if total_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002361560018966673,
        "mean_shap": 0.0003277188251485873,
        "std_shap": 0.00291209657398978,
        "min_shap": -0.005435854469582575,
        "max_shap": 0.0105786912714022
      },
      "rank": 119
    },
    {
      "feature_index": 142,
      "feature_name": "feature_142",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of unique lemmas in text\"\n    doc = nlp(text)\n    unique_lemmas = set(token.lemma_.lower() for token in doc if token.is_alpha)\n    if not unique_lemmas:\n        return 0.0\n    return float(sum(len(lemma) for lemma in unique_lemmas)) / len(unique_lemmas)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022807694600445786,
        "mean_shap": -0.00014906345899049357,
        "std_shap": 0.0033259126327452745,
        "min_shap": -0.011343891421884189,
        "max_shap": 0.010779130393285739
      },
      "rank": 120
    },
    {
      "feature_index": 25,
      "feature_name": "feature_25",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022689210626410366,
        "mean_shap": 0.0007722333180805836,
        "std_shap": 0.0031390538854026324,
        "min_shap": -0.006160358843154119,
        "max_shap": 0.015247175331791709
      },
      "rank": 121
    },
    {
      "feature_index": 372,
      "feature_name": "feature_372",
      "feature_code": "def feature(text: str) -> float:\n    'Fraction of uppercase letters in the text'\n    uppercase_count = sum(1 for c in text if c.isupper())\n    return uppercase_count / len(text) if len(text) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022681375240054713,
        "mean_shap": 0.0001483605995053927,
        "std_shap": 0.002716953627564788,
        "min_shap": -0.005209473735432336,
        "max_shap": 0.008414312029787905
      },
      "rank": 122
    },
    {
      "feature_index": 12,
      "feature_name": "feature_12",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text\"\n    doc = nlp(text)\n    return float(len(doc.ents))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022599529825996054,
        "mean_shap": 0.0007670482629271686,
        "std_shap": 0.0029212717218199944,
        "min_shap": -0.005948121754481381,
        "max_shap": 0.01256530969255801
      },
      "rank": 123
    },
    {
      "feature_index": 459,
      "feature_name": "feature_459",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words, but only considering sentences longer than a given threshold\"\n    sentences = re.split(r'(?<=[.!?]) +', text)\n    lengths = [len(s.split()) for s in sentences if len(s.split()) > 5]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0022182341233747743,
        "mean_shap": 7.218304234177335e-05,
        "std_shap": 0.0030290959359494103,
        "min_shap": -0.013512777238512411,
        "max_shap": 0.006713361422072975
      },
      "rank": 124
    },
    {
      "feature_index": 124,
      "feature_name": "feature_124",
      "feature_code": "def feature(text: str) -> float:\n    'Count of coordinating conjunctions in the text'\n    doc = nlp(text)\n    conj_count = sum(1 for token in doc if token.pos_ == 'CCONJ')\n    return float(conj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002163889048039007,
        "mean_shap": -0.00039678016086333313,
        "std_shap": 0.0025372831955846925,
        "min_shap": -0.0064448644281667605,
        "max_shap": 0.0066939418182608386
      },
      "rank": 125
    },
    {
      "feature_index": 87,
      "feature_name": "feature_87",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique nouns divided by total words\"\n    doc = nlp(text)\n    unique_nouns = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'NOUN'))\n    total_words = len(doc)\n    return float(unique_nouns) / total_words if total_words > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0021176250290660084,
        "mean_shap": -0.00014192866167010498,
        "std_shap": 0.0028969295671423057,
        "min_shap": -0.0065389486459825116,
        "max_shap": 0.01323951719598964
      },
      "rank": 126
    },
    {
      "feature_index": 158,
      "feature_name": "feature_158",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of sentences that contain a question mark\"\n    sentence_count = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n    question_count = text.count('?')\n    return question_count / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002099126757375262,
        "mean_shap": -0.0012930133246492703,
        "std_shap": 0.003016567181110617,
        "min_shap": -0.01880849897566956,
        "max_shap": 0.003439512465631419
      },
      "rank": 127
    },
    {
      "feature_index": 438,
      "feature_name": "feature_438",
      "feature_code": "def feature(text: str) -> float:\n    'Sentiment polarity score of the text'\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0020962324236665237,
        "mean_shap": -0.00039703002873072433,
        "std_shap": 0.002689312872033232,
        "min_shap": -0.004625469723747366,
        "max_shap": 0.011380630094570245
      },
      "rank": 128
    },
    {
      "feature_index": 51,
      "feature_name": "feature_51",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of noun phrases to total sentences in the text\"\n    doc = nlp(text)\n    noun_phrases = sum(1 for chunk in doc.noun_chunks)\n    sentence_count = len(list(doc.sents))\n    return float(noun_phrases) / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.002095469742780306,
        "mean_shap": -0.00047037822675386154,
        "std_shap": 0.002554832300288695,
        "min_shap": -0.007198832511363803,
        "max_shap": 0.005084238459761132
      },
      "rank": 129
    },
    {
      "feature_index": 422,
      "feature_name": "feature_422",
      "feature_code": "def feature(text: str) -> float:\n    \"Total count of unique characters in the text\"\n    unique_chars = len(set(text))\n    return float(unique_chars)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019753039907642518,
        "mean_shap": 0.0011984941078186492,
        "std_shap": 0.0024835072456584647,
        "min_shap": -0.01866068447851045,
        "max_shap": 0.008025773825806305
      },
      "rank": 130
    },
    {
      "feature_index": 284,
      "feature_name": "feature_284",
      "feature_code": "def feature(text: str) -> float:\n    'Count of negative sentiment words in the text'\n    scores = sia.polarity_scores(text)\n    return float(scores['neg'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019601071862249114,
        "mean_shap": -1.962230523869538e-05,
        "std_shap": 0.002647096283953157,
        "min_shap": -0.009995078776132378,
        "max_shap": 0.007902607520839839
      },
      "rank": 131
    },
    {
      "feature_index": 159,
      "feature_name": "feature_159",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique adjectives in the text\"\n    doc = nlp(text)\n    unique_adjectives = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'ADJ' and token.is_alpha))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019218454851090948,
        "mean_shap": -0.000255980195750048,
        "std_shap": 0.002259609303374562,
        "min_shap": -0.0033628896352610304,
        "max_shap": 0.006349343649581186
      },
      "rank": 132
    },
    {
      "feature_index": 411,
      "feature_name": "feature_411",
      "feature_code": "def feature(text: str) -> float:\n    'Count of conjunctions in the text'\n    conjunctions = ['and', 'but', 'or', 'so', 'for', 'nor', 'yet']\n    count = sum(text.lower().count(conj) for conj in conjunctions)\n    return float(count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0019204227228164401,
        "mean_shap": -0.00030976642174454754,
        "std_shap": 0.002391469496827262,
        "min_shap": -0.006596459550287646,
        "max_shap": 0.010167242997552083
      },
      "rank": 133
    },
    {
      "feature_index": 110,
      "feature_name": "feature_110",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of nouns that are not stopwords'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN' and not token.is_stop)\n    total_noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return noun_count / total_noun_count if total_noun_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0017352914236468736,
        "mean_shap": -7.036823252044836e-05,
        "std_shap": 0.002612265218267007,
        "min_shap": -0.011205822347395093,
        "max_shap": 0.005173915064667635
      },
      "rank": 134
    },
    {
      "feature_index": 320,
      "feature_name": "feature_320",
      "feature_code": "def feature(text: str) -> float:\n    'Maximum sentence length measured in words'\n    sentences = re.split(r'[.!?]+', text)\n    max_length = max((len(s.split()) for s in sentences if s.split()), default=0)\n    return float(max_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0017244582016322644,
        "mean_shap": 0.00034399542093007366,
        "std_shap": 0.002325974434950358,
        "min_shap": -0.007972097435686313,
        "max_shap": 0.007129066678438759
      },
      "rank": 135
    },
    {
      "feature_index": 460,
      "feature_name": "feature_460",
      "feature_code": "def feature(text: str) -> float:\n    'Compound sentence ratio (sentences with conjunctions)'\n    sentences = re.split(r'[.!?]+', text)\n    compound_count = sum(1 for s in sentences if 'and' in s or 'but' in s or 'or' in s)\n    if not sentences:\n        return 0.0\n    return float(compound_count) / len(sentences)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0017199964367093283,
        "mean_shap": 0.0004149730495576854,
        "std_shap": 0.0028038884993137436,
        "min_shap": -0.005472387383587591,
        "max_shap": 0.020026843589541026
      },
      "rank": 136
    },
    {
      "feature_index": 442,
      "feature_name": "feature_442",
      "feature_code": "def feature(text: str) -> float:\n    \"Diversity of parts of speech as a ratio\"\n    doc = nlp(text)\n    pos_counts = Counter(token.pos_ for token in doc)\n    pos_diversity = len(pos_counts) / len(doc) if len(doc) > 0 else 0.0\n    return pos_diversity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0017154667125422725,
        "mean_shap": 6.013057595473853e-05,
        "std_shap": 0.0030047666141187713,
        "min_shap": -0.005842979784793397,
        "max_shap": 0.012140489519635778
      },
      "rank": 137
    },
    {
      "feature_index": 278,
      "feature_name": "feature_278",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity of the text\"\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001712802705636453,
        "mean_shap": -0.0002201730208775524,
        "std_shap": 0.0022660328669962377,
        "min_shap": -0.0037630106614235767,
        "max_shap": 0.00942427641449141
      },
      "rank": 138
    },
    {
      "feature_index": 377,
      "feature_name": "feature_377",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity score using TextBlob\"\n    blob = TextBlob(text)\n    return float(blob.sentiment.subjectivity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001682228811524491,
        "mean_shap": 0.0001090873819427154,
        "std_shap": 0.0022709157746378613,
        "min_shap": -0.009713216487552693,
        "max_shap": 0.0038897354834259447
      },
      "rank": 139
    },
    {
      "feature_index": 446,
      "feature_name": "feature_446",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score from VADER sentiment analysis\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016613989923855536,
        "mean_shap": -0.00018973910815759342,
        "std_shap": 0.002187446609455423,
        "min_shap": -0.0038957061598349895,
        "max_shap": 0.007803389704794222
      },
      "rank": 140
    },
    {
      "feature_index": 180,
      "feature_name": "feature_180",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity calculated using the ratio of unique words to total words\"\n    words = text.split()\n    unique_words = len(set(word.lower() for word in words))\n    return float(unique_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016546056290618779,
        "mean_shap": 0.00024826226937694606,
        "std_shap": 0.0020478687452563545,
        "min_shap": -0.009421166350146719,
        "max_shap": 0.004971899670036148
      },
      "rank": 141
    },
    {
      "feature_index": 174,
      "feature_name": "feature_174",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of the text to gauge emotional content'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016483333807198195,
        "mean_shap": -0.00013789466681240884,
        "std_shap": 0.0021616689232044177,
        "min_shap": -0.003324047266941871,
        "max_shap": 0.008328883501539803
      },
      "rank": 142
    },
    {
      "feature_index": 211,
      "feature_name": "feature_211",
      "feature_code": "def feature(text: str) -> float:\n    \"Diversity of parts of speech as a ratio\"\n    doc = nlp(text)\n    pos_counts = Counter(token.pos_ for token in doc)\n    pos_diversity = len(pos_counts) / len(doc) if len(doc) > 0 else 0.0\n    return pos_diversity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0016262425109138312,
        "mean_shap": -3.144311744200734e-05,
        "std_shap": 0.002672933351882256,
        "min_shap": -0.005518379928259603,
        "max_shap": 0.012584070707438243
      },
      "rank": 143
    },
    {
      "feature_index": 344,
      "feature_name": "feature_344",
      "feature_code": "def feature(text: str) -> float:\n    'Count of occurrences of the word \"I\"'\n    return float(text.lower().count('i'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015926946771860058,
        "mean_shap": -0.00026324239570129016,
        "std_shap": 0.002390475026048753,
        "min_shap": -0.016295301939738643,
        "max_shap": 0.00465086544619244
      },
      "rank": 144
    },
    {
      "feature_index": 295,
      "feature_name": "feature_295",
      "feature_code": "def feature(text: str) -> float:\n    \"Polarity score indicating overall sentiment of the text\"\n    scores = sia.polarity_scores(text)\n    return scores['compound']\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015912600268428788,
        "mean_shap": -0.00020252902392420538,
        "std_shap": 0.0021068689230397267,
        "min_shap": -0.003724932779868259,
        "max_shap": 0.00752946633843973
      },
      "rank": 145
    },
    {
      "feature_index": 415,
      "feature_name": "feature_415",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of syllables per word in the text\"\n    def syllable_count(word):\n        return len(re.findall(r'[aeiou]', word.lower()))\n    \n    words = text.split()\n    if not words:\n        return 0.0\n    return sum(syllable_count(word) for word in words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015840692268805794,
        "mean_shap": -0.0001059027555993303,
        "std_shap": 0.0020921429126992,
        "min_shap": -0.004830125935207102,
        "max_shap": 0.007742970609539012
      },
      "rank": 146
    },
    {
      "feature_index": 144,
      "feature_name": "feature_144",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of the text using VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015749029746016804,
        "mean_shap": -0.0002714020907164866,
        "std_shap": 0.0020580974065584454,
        "min_shap": -0.0034567496214085547,
        "max_shap": 0.007920629344135406
      },
      "rank": 147
    },
    {
      "feature_index": 261,
      "feature_name": "feature_261",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity score from TextBlob for the text\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return blob.sentiment.subjectivity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015743881924160656,
        "mean_shap": -0.00010233390331720992,
        "std_shap": 0.0021866609341925198,
        "min_shap": -0.010042069985278276,
        "max_shap": 0.003591262522945043
      },
      "rank": 148
    },
    {
      "feature_index": 37,
      "feature_name": "feature_37",
      "feature_code": "def feature(text: str) -> float:\n    'Count of numeric characters in the text'\n    return float(sum(1 for c in text if c.isdigit()))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015720151673698913,
        "mean_shap": -0.00016108584293799888,
        "std_shap": 0.0023727054329897444,
        "min_shap": -0.003708092827064713,
        "max_shap": 0.01152670971973207
      },
      "rank": 149
    },
    {
      "feature_index": 173,
      "feature_name": "feature_173",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of unique words in the text'\n    words = set(text.split())\n    if not words:\n        return 0.0\n    avg_length = sum(len(word) for word in words) / len(words)\n    return float(avg_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015701571188389846,
        "mean_shap": 0.00014996747444600373,
        "std_shap": 0.002099002103996807,
        "min_shap": -0.006874874625286894,
        "max_shap": 0.00673338322532551
      },
      "rank": 150
    },
    {
      "feature_index": 210,
      "feature_name": "feature_210",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score from VADER sentiment analysis\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015355383093747016,
        "mean_shap": -0.00024377687673465294,
        "std_shap": 0.0020405611297589516,
        "min_shap": -0.002905851502464693,
        "max_shap": 0.008582802912324456
      },
      "rank": 151
    },
    {
      "feature_index": 312,
      "feature_name": "feature_312",
      "feature_code": "def feature(text: str) -> float:\n    \"Calculates the ratio of long sentences (greater than 20 words) to total sentences.\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentence_count = sum(1 for s in sentences if len(s.split()) > 20)\n    total_sentence_count = len(sentences)\n    return (long_sentence_count / total_sentence_count) if total_sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015313439167681181,
        "mean_shap": -0.000179315076680514,
        "std_shap": 0.0020665320308390425,
        "min_shap": -0.009717331612137588,
        "max_shap": 0.004559124200924509
      },
      "rank": 152
    },
    {
      "feature_index": 417,
      "feature_name": "feature_417",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of sentences containing more than 15 words, indicating complexity\"\n    sentences = re.split(r'[.!?]+', text)\n    long_sentences = sum(1 for s in sentences if len(s.split()) > 15)\n    return float(long_sentences) / len(sentences) if len(sentences) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001525529547621788,
        "mean_shap": -0.0002499652089422746,
        "std_shap": 0.001896428389135618,
        "min_shap": -0.005535527476002066,
        "max_shap": 0.004394289028294991
      },
      "rank": 153
    },
    {
      "feature_index": 81,
      "feature_name": "feature_81",
      "feature_code": "def feature(text: str) -> float:\n    'Sentiment score using VADER sentiment analysis'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0015138908762676439,
        "mean_shap": -0.00018188547245156624,
        "std_shap": 0.0019549847836227196,
        "min_shap": -0.003518424163374178,
        "max_shap": 0.007025072078070837
      },
      "rank": 154
    },
    {
      "feature_index": 79,
      "feature_name": "feature_79",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives in the text'\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001492107970942912,
        "mean_shap": -0.0005275439807698961,
        "std_shap": 0.001782643689169247,
        "min_shap": -0.004659831029074143,
        "max_shap": 0.007335410700816752
      },
      "rank": 155
    },
    {
      "feature_index": 96,
      "feature_name": "feature_96",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of unique words in the text'\n    unique_words = set(text.split())\n    if not unique_words:\n        return 0.0\n    return float(sum(len(word) for word in unique_words)) / len(unique_words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014713451837954704,
        "mean_shap": 0.000263218201743679,
        "std_shap": 0.001896155673512466,
        "min_shap": -0.006190958943864086,
        "max_shap": 0.006125780227073932
      },
      "rank": 156
    },
    {
      "feature_index": 428,
      "feature_name": "feature_428",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives in the text'\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014343426228564668,
        "mean_shap": -0.00047213705392483765,
        "std_shap": 0.001927735940878856,
        "min_shap": -0.002997354097615462,
        "max_shap": 0.010392692297447064
      },
      "rank": 157
    },
    {
      "feature_index": 254,
      "feature_name": "feature_254",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment subjectivity score of the text using TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.subjectivity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014338310145328521,
        "mean_shap": 0.00012858422745631615,
        "std_shap": 0.0018859107851559088,
        "min_shap": -0.008661378374234294,
        "max_shap": 0.0031894966655083524
      },
      "rank": 158
    },
    {
      "feature_index": 0,
      "feature_name": "feature_0",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014327944663585586,
        "mean_shap": 2.843512420092114e-05,
        "std_shap": 0.0017778450062213103,
        "min_shap": -0.002927869800401842,
        "max_shap": 0.006080163358762192
      },
      "rank": 159
    },
    {
      "feature_index": 431,
      "feature_name": "feature_431",
      "feature_code": "def feature(text: str) -> float:\n    \"Longest word length in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    longest_length = max(len(word) for word in words)\n    return float(longest_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014215106309820719,
        "mean_shap": 0.0005770961491486596,
        "std_shap": 0.0026754136099720297,
        "min_shap": -0.025349734637859687,
        "max_shap": 0.002561405492025735
      },
      "rank": 160
    },
    {
      "feature_index": 84,
      "feature_name": "feature_84",
      "feature_code": "def feature(text: str) -> float:\n    'Complexity score based on sentence length variability'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if len(lengths) < 2:\n        return 0.0\n    return float(np.std(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0014012934515848711,
        "mean_shap": -5.955230435785186e-06,
        "std_shap": 0.0016766381287009252,
        "min_shap": -0.003818486459555358,
        "max_shap": 0.003694017156624365
      },
      "rank": 161
    },
    {
      "feature_index": 146,
      "feature_name": "feature_146",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment score from VADER sentiment analysis'\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013913170154619342,
        "mean_shap": -0.00022513444684013208,
        "std_shap": 0.001869261842215493,
        "min_shap": -0.002907007097540382,
        "max_shap": 0.009436162594371916
      },
      "rank": 162
    },
    {
      "feature_index": 399,
      "feature_name": "feature_399",
      "feature_code": "def feature(text: str) -> float:\n    'Maximum length of a word in the text'\n    words = text.split()\n    return float(max(len(word) for word in words)) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013731332602161827,
        "mean_shap": 0.0005062295144000725,
        "std_shap": 0.00242552653123106,
        "min_shap": -0.025057516750087,
        "max_shap": 0.00260946629183848
      },
      "rank": 163
    },
    {
      "feature_index": 293,
      "feature_name": "feature_293",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words in the text\"\n    words = text.split()\n    stopwords_count = sum(1 for word in words if nlp.vocab[word].is_stop)\n    if len(words) == 0:\n        return 0.0\n    return float(stopwords_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013335243695266547,
        "mean_shap": -0.00038515865410117435,
        "std_shap": 0.0016700922160200506,
        "min_shap": -0.006432085908515973,
        "max_shap": 0.0035242928169209892
      },
      "rank": 164
    },
    {
      "feature_index": 414,
      "feature_name": "feature_414",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are stop words'\n    words = text.split()\n    if not words:\n        return 0.0\n    stopwords_count = sum(1 for word in words if word.lower() in nlp.Defaults.stop_words)\n    return stopwords_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013263994973403857,
        "mean_shap": -0.0001971845338890548,
        "std_shap": 0.0017119686281321174,
        "min_shap": -0.00604718048809306,
        "max_shap": 0.004029088608445051
      },
      "rank": 165
    },
    {
      "feature_index": 268,
      "feature_name": "feature_268",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths (in words)\"\n    sentences = re.split(r'[.!?]+', text)\n    sentence_lengths = [len(s.split()) for s in sentences if s.strip()]\n    return float(statistics.pstdev(sentence_lengths)) if sentence_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013229102419043753,
        "mean_shap": 4.131295680387376e-05,
        "std_shap": 0.0016810219598498316,
        "min_shap": -0.0052105801706932115,
        "max_shap": 0.0039847890651729485
      },
      "rank": 166
    },
    {
      "feature_index": 342,
      "feature_name": "feature_342",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (three or more syllables) in the text\"\n    words = re.findall(r'\\w+', text)\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    return complex_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0013208052542919807,
        "mean_shap": -0.00013744486589804204,
        "std_shap": 0.0019864212136938716,
        "min_shap": -0.011549098303342364,
        "max_shap": 0.006291679986484902
      },
      "rank": 167
    },
    {
      "feature_index": 360,
      "feature_name": "feature_360",
      "feature_code": "def feature(text: str) -> float:\n    \"Measures the cohesiveness of the text based on repeated phrases (n-grams).\"\n    from collections import Counter\n    n_grams = [text[i:i+2] for i in range(len(text)-1)]\n    most_common_ngrams = Counter(n_grams).most_common()\n    return float(len(most_common_ngrams))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001298128415932286,
        "mean_shap": 0.0002328342264360924,
        "std_shap": 0.0018657064947549833,
        "min_shap": -0.0058360500850025985,
        "max_shap": 0.007507499418122892
      },
      "rank": 168
    },
    {
      "feature_index": 380,
      "feature_name": "feature_380",
      "feature_code": "def feature(text: str) -> float:\n    \"Measure of emotional tone using sentiment scores\"\n    scores = sia.polarity_scores(text)\n    return float(scores['compound'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001272654354049613,
        "mean_shap": -0.0001151449500984163,
        "std_shap": 0.0016640832428834599,
        "min_shap": -0.0024968283657649387,
        "max_shap": 0.008719828992435322
      },
      "rank": 169
    },
    {
      "feature_index": 64,
      "feature_name": "feature_64",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of words that are stop words'\n    words = text.split()\n    if not words:\n        return 0.0\n    stopwords_count = sum(1 for word in words if word.lower() in nlp.Defaults.stop_words)\n    return stopwords_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001260310742575985,
        "mean_shap": -0.00023174903756498222,
        "std_shap": 0.0015718937644081206,
        "min_shap": -0.005328578772440522,
        "max_shap": 0.004607812631644293
      },
      "rank": 170
    },
    {
      "feature_index": 430,
      "feature_name": "feature_430",
      "feature_code": "def feature(text: str) -> float:\n    'Longest word length in the text'\n    words = text.split()\n    longest_length = max((len(word) for word in words), default=0)\n    return float(longest_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012598279509550112,
        "mean_shap": 0.000696605898029114,
        "std_shap": 0.0021343104580531665,
        "min_shap": -0.022999188755823817,
        "max_shap": 0.002674075498051029
      },
      "rank": 171
    },
    {
      "feature_index": 418,
      "feature_name": "feature_418",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.strip()]\n    return float(statistics.pstdev(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012391258367642985,
        "mean_shap": 1.0872724283139459e-05,
        "std_shap": 0.0014932050020957414,
        "min_shap": -0.00382470974109619,
        "max_shap": 0.0032400666558969576
      },
      "rank": 172
    },
    {
      "feature_index": 133,
      "feature_name": "feature_133",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012343849055656416,
        "mean_shap": -3.144554928244132e-06,
        "std_shap": 0.0014936991720821372,
        "min_shap": -0.0037306832422802057,
        "max_shap": 0.004567321083588321
      },
      "rank": 173
    },
    {
      "feature_index": 40,
      "feature_name": "feature_40",
      "feature_code": "def feature(text: str) -> float:\n    \"Positive sentiment proportion from VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012342631250628429,
        "mean_shap": -0.00010844191055746982,
        "std_shap": 0.001471455820891033,
        "min_shap": -0.0036376455382720486,
        "max_shap": 0.0034349848265559685
      },
      "rank": 174
    },
    {
      "feature_index": 349,
      "feature_name": "feature_349",
      "feature_code": "def feature(text: str) -> float:\n    'Density of specific keywords (e.g. \"recommend\") per total words'\n    keyword = 'recommend'\n    keyword_count = text.lower().count(keyword)\n    total_words = len(text.split())\n    return (keyword_count / total_words) if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012313831508101911,
        "mean_shap": -0.00042447577714504694,
        "std_shap": 0.0016347528246373512,
        "min_shap": -0.00209359065861608,
        "max_shap": 0.008879147797568907
      },
      "rank": 175
    },
    {
      "feature_index": 214,
      "feature_name": "feature_214",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adjectives in the text\"\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012203697260434725,
        "mean_shap": -0.00041273336372815597,
        "std_shap": 0.0015179378393959746,
        "min_shap": -0.0034247365638501114,
        "max_shap": 0.0064432769835130466
      },
      "rank": 176
    },
    {
      "feature_index": 405,
      "feature_name": "feature_405",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of words that are long (7 or more characters)\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) >= 7)\n    return long_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0012196999620720738,
        "mean_shap": -0.00010295602109564374,
        "std_shap": 0.0018666714544227872,
        "min_shap": -0.0040849809466914945,
        "max_shap": 0.01200208855495188
      },
      "rank": 177
    },
    {
      "feature_index": 296,
      "feature_name": "feature_296",
      "feature_code": "def feature(text: str) -> float:\n    \"Entropy of word frequency distribution in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    frequency = Counter(words)\n    total_words = len(words)\n    entropy = -sum((count / total_words) * np.log2(count / total_words) for count in frequency.values())\n    return float(entropy)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001185207989130546,
        "mean_shap": 0.0002717508437153777,
        "std_shap": 0.0015905486797890339,
        "min_shap": -0.005593130376105904,
        "max_shap": 0.00632532217352467
      },
      "rank": 178
    },
    {
      "feature_index": 291,
      "feature_name": "feature_291",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of sentence lengths in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    return float(statistics.pstdev(lengths)) if lengths else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011747976214058421,
        "mean_shap": 1.7586017196068167e-06,
        "std_shap": 0.0014694398586639942,
        "min_shap": -0.004983036368206736,
        "max_shap": 0.00393359696036686
      },
      "rank": 179
    },
    {
      "feature_index": 116,
      "feature_name": "feature_116",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011721222188914937,
        "mean_shap": -5.484634167455918e-05,
        "std_shap": 0.0014576582918279905,
        "min_shap": -0.004035834442504188,
        "max_shap": 0.003376171539619825
      },
      "rank": 180
    },
    {
      "feature_index": 302,
      "feature_name": "feature_302",
      "feature_code": "def feature(text: str) -> float:\n    \"Positive sentiment proportion from VADER\"\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011701470381942332,
        "mean_shap": 5.258719004215981e-05,
        "std_shap": 0.0014165075616714496,
        "min_shap": -0.0035663834755392263,
        "max_shap": 0.0038236040841457463
      },
      "rank": 181
    },
    {
      "feature_index": 60,
      "feature_name": "feature_60",
      "feature_code": "def feature(text: str) -> float:\n    'Sentence length variability (standard deviation of sentence lengths)'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011510897892640882,
        "mean_shap": -4.9160447470471135e-06,
        "std_shap": 0.001425473302297669,
        "min_shap": -0.004277583650322645,
        "max_shap": 0.003785641110399508
      },
      "rank": 182
    },
    {
      "feature_index": 22,
      "feature_name": "feature_22",
      "feature_code": "def feature(text: str) -> float:\n    'Positive sentiment score using VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0011492963163760512,
        "mean_shap": -0.00011491368693777277,
        "std_shap": 0.0013853298859685418,
        "min_shap": -0.003903866939645451,
        "max_shap": 0.0029329482980954983
      },
      "rank": 183
    },
    {
      "feature_index": 311,
      "feature_name": "feature_311",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001133235271054315,
        "mean_shap": 1.581916227569441e-05,
        "std_shap": 0.0014945545494029848,
        "min_shap": -0.0028932781694412098,
        "max_shap": 0.008412792079397356
      },
      "rank": 184
    },
    {
      "feature_index": 77,
      "feature_name": "feature_77",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of words that are longer than 10 characters\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 10)\n    return float(long_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001124973001939131,
        "mean_shap": -6.935776140649994e-05,
        "std_shap": 0.0020657179523910574,
        "min_shap": -0.008627109545303395,
        "max_shap": 0.008073131718736038
      },
      "rank": 185
    },
    {
      "feature_index": 16,
      "feature_name": "feature_16",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010940234910461616,
        "mean_shap": 9.678641298450268e-06,
        "std_shap": 0.0013310304100451356,
        "min_shap": -0.0021945638001431614,
        "max_shap": 0.0049939381345365285
      },
      "rank": 186
    },
    {
      "feature_index": 421,
      "feature_name": "feature_421",
      "feature_code": "def feature(text: str) -> float:\n    'Count of exclamatory sentences in the text'\n    exclam_sentences = text.count('!')\n    total_sentences = len(re.findall(r'[.!?]', text)) + (1 if exclam_sentences > 0 else 0)\n    return float(exclam_sentences) / total_sentences if total_sentences > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010857189190583393,
        "mean_shap": -0.0003579650105046968,
        "std_shap": 0.0012806247759864875,
        "min_shap": -0.002905304416961283,
        "max_shap": 0.00442881509425285
      },
      "rank": 187
    },
    {
      "feature_index": 407,
      "feature_name": "feature_407",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words in the text\"\n    words = text.split()\n    unique_words = set(word.lower() for word in words)\n    return float(len(unique_words)) / len(words) if words else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010812994949178452,
        "mean_shap": 0.00014004811879071035,
        "std_shap": 0.001437938247142859,
        "min_shap": -0.00567834634672116,
        "max_shap": 0.004610604443372294
      },
      "rank": 188
    },
    {
      "feature_index": 54,
      "feature_name": "feature_54",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010659267801073752,
        "mean_shap": 0.00017705539041199187,
        "std_shap": 0.0013358394724165715,
        "min_shap": -0.002197873470865308,
        "max_shap": 0.00558056258331414
      },
      "rank": 189
    },
    {
      "feature_index": 282,
      "feature_name": "feature_282",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of adjectives to total words in the text'\n    doc = nlp(text)\n    total_words = len([token for token in doc if token.is_alpha])\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adjective_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010511885525335207,
        "mean_shap": -0.00029372078210536666,
        "std_shap": 0.0014071721259112338,
        "min_shap": -0.007049026579366599,
        "max_shap": 0.005505703963759953
      },
      "rank": 190
    },
    {
      "feature_index": 186,
      "feature_name": "feature_186",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths)) if len(lengths) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010485110518975228,
        "mean_shap": 0.00021691853503971422,
        "std_shap": 0.0020594175156823987,
        "min_shap": -0.016361199006098694,
        "max_shap": 0.0025614356971856945
      },
      "rank": 191
    },
    {
      "feature_index": 127,
      "feature_name": "feature_127",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of paragraphs in the text'\n    paragraphs = text.split('\\n')\n    return float(len(paragraphs))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010447503921552856,
        "mean_shap": -0.0003309298734940261,
        "std_shap": 0.001377870097791735,
        "min_shap": -0.0037018321449817405,
        "max_shap": 0.005164394279950425
      },
      "rank": 192
    },
    {
      "feature_index": 46,
      "feature_name": "feature_46",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010386796337155868,
        "mean_shap": 3.707016974115567e-05,
        "std_shap": 0.0015157324386708727,
        "min_shap": -0.002414039638893575,
        "max_shap": 0.006624580095623088
      },
      "rank": 193
    },
    {
      "feature_index": 435,
      "feature_name": "feature_435",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical richness score based on types vs tokens'\n    doc = nlp(text)\n    unique_tokens = len(set(token.text.lower() for token in doc if token.is_alpha))\n    token_count = len([token for token in doc if token.is_alpha])\n    return float(unique_tokens) / token_count if token_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010386514925000252,
        "mean_shap": 0.0001650052382308109,
        "std_shap": 0.0015610316288927073,
        "min_shap": -0.007418141579340899,
        "max_shap": 0.0028966055336182507
      },
      "rank": 194
    },
    {
      "feature_index": 130,
      "feature_name": "feature_130",
      "feature_code": "def feature(text: str) -> float:\n    \"Percentage of words that are longer than 6 characters\"\n    words = text.split()\n    if not words:\n        return 0.0\n    long_words = sum(1 for word in words if len(word) > 6)\n    return float(long_words) / len(words)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010300010373991708,
        "mean_shap": -4.622338654902412e-05,
        "std_shap": 0.0015646478090658102,
        "min_shap": -0.0031860143525809367,
        "max_shap": 0.010247668447821259
      },
      "rank": 195
    },
    {
      "feature_index": 74,
      "feature_name": "feature_74",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001029083676325382,
        "mean_shap": 0.00025228099950349306,
        "std_shap": 0.002067846399789443,
        "min_shap": -0.01883720935379013,
        "max_shap": 0.0018423990029596557
      },
      "rank": 196
    },
    {
      "feature_index": 267,
      "feature_name": "feature_267",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives in the text'\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010252674740093182,
        "mean_shap": -0.0003417365370379592,
        "std_shap": 0.001257123952969442,
        "min_shap": -0.0024824932100533095,
        "max_shap": 0.00595687783717501
      },
      "rank": 197
    },
    {
      "feature_index": 112,
      "feature_name": "feature_112",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (three or more syllables) in the text\"\n    words = re.findall(r'\\w+', text)\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    return complex_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00102450349398412,
        "mean_shap": -1.896548227052205e-05,
        "std_shap": 0.0015530215789378032,
        "min_shap": -0.007438040962622807,
        "max_shap": 0.006193342162758985
      },
      "rank": 198
    },
    {
      "feature_index": 397,
      "feature_name": "feature_397",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.001020219939456011,
        "mean_shap": -0.00012353567799883435,
        "std_shap": 0.0012097692883920276,
        "min_shap": -0.0031833915342513903,
        "max_shap": 0.0029101301654777816
      },
      "rank": 199
    },
    {
      "feature_index": 24,
      "feature_name": "feature_24",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010135739175203906,
        "mean_shap": -0.00011486903354303002,
        "std_shap": 0.0012129314924583926,
        "min_shap": -0.0031775083130460816,
        "max_shap": 0.0032656210092965026
      },
      "rank": 200
    },
    {
      "feature_index": 299,
      "feature_name": "feature_299",
      "feature_code": "def feature(text: str) -> float:\n    'Positive sentiment score using VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0010011155646362908,
        "mean_shap": -2.2114691523901288e-05,
        "std_shap": 0.0011949035660513943,
        "min_shap": -0.0024216407161707526,
        "max_shap": 0.0031141905859208264
      },
      "rank": 201
    },
    {
      "feature_index": 95,
      "feature_name": "feature_95",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009920033425418766,
        "mean_shap": -4.857285554511022e-05,
        "std_shap": 0.0011813267261436074,
        "min_shap": -0.0022671204201191687,
        "max_shap": 0.003870743108016552
      },
      "rank": 202
    },
    {
      "feature_index": 325,
      "feature_name": "feature_325",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of long words (greater than 6 characters) in the text'\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 6)\n    return long_word_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000991978802981779,
        "mean_shap": -5.2080671143006116e-05,
        "std_shap": 0.0014706313141746477,
        "min_shap": -0.004003314351037276,
        "max_shap": 0.009262378503116288
      },
      "rank": 203
    },
    {
      "feature_index": 432,
      "feature_name": "feature_432",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009901130465665687,
        "mean_shap": 0.0002964891889637154,
        "std_shap": 0.0017988315722975666,
        "min_shap": -0.01631391746100542,
        "max_shap": 0.0033793500501347997
      },
      "rank": 204
    },
    {
      "feature_index": 271,
      "feature_name": "feature_271",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of words in the text excluding stopwords'\n    doc = nlp(text)\n    words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n    if not words:\n        return 0.0\n    return float(sum(len(word) for word in words)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009859600190048983,
        "mean_shap": 3.88313439553751e-05,
        "std_shap": 0.0014035085109186873,
        "min_shap": -0.005159897566899304,
        "max_shap": 0.004933819703033138
      },
      "rank": 205
    },
    {
      "feature_index": 50,
      "feature_name": "feature_50",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009859271542704674,
        "mean_shap": 0.00022500631787688853,
        "std_shap": 0.0018788651229099513,
        "min_shap": -0.016226092892546614,
        "max_shap": 0.0028791444490932102
      },
      "rank": 206
    },
    {
      "feature_index": 413,
      "feature_name": "feature_413",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of frequencies of top 5 most common words\"\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    word_counts = Counter(words)\n    most_common = word_counts.most_common(5)\n    return float(sum(count for _, count in most_common))\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009786464813097992,
        "mean_shap": -2.809960523733301e-06,
        "std_shap": 0.0014324593434212431,
        "min_shap": -0.004251136707333193,
        "max_shap": 0.005197239336173293
      },
      "rank": 207
    },
    {
      "feature_index": 262,
      "feature_name": "feature_262",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009716708064215983,
        "mean_shap": -4.048821106204586e-06,
        "std_shap": 0.0012047615741383846,
        "min_shap": -0.0025818165580474024,
        "max_shap": 0.0033313466455500203
      },
      "rank": 208
    },
    {
      "feature_index": 332,
      "feature_name": "feature_332",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs in the text to assess action-oriented language\"\n    doc = nlp(text)\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB' and token.is_alpha))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009711026314146024,
        "mean_shap": 0.00011736420376723351,
        "std_shap": 0.00146050235265609,
        "min_shap": -0.0036261191989397897,
        "max_shap": 0.0061029210826041725
      },
      "rank": 209
    },
    {
      "feature_index": 389,
      "feature_name": "feature_389",
      "feature_code": "def feature(text: str) -> float:\n    'Positive sentiment score using VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000968976871193856,
        "mean_shap": -6.598260478283306e-05,
        "std_shap": 0.0012632250221188423,
        "min_shap": -0.0039005173673723895,
        "max_shap": 0.0035382918260548845
      },
      "rank": 210
    },
    {
      "feature_index": 434,
      "feature_name": "feature_434",
      "feature_code": "def feature(text: str) -> float:\n    'Count of quotes in the text'\n    quote_count = text.count('\"') // 2\n    return float(quote_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009638083458471361,
        "mean_shap": 0.00022722414655329144,
        "std_shap": 0.0017911412553240993,
        "min_shap": -0.010375578843777959,
        "max_shap": 0.0017242752368131172
      },
      "rank": 211
    },
    {
      "feature_index": 351,
      "feature_name": "feature_351",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of complex words (3 or more syllables) to total words in the text'\n    import textstat\n    total_words = len(text.split())\n    complex_words = sum(1 for word in text.split() if textstat.syllable_count(word) >= 3)\n    return float(complex_words) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009584372364443004,
        "mean_shap": -9.62265943567568e-05,
        "std_shap": 0.0011454881139680525,
        "min_shap": -0.003934576801413416,
        "max_shap": 0.003194409520847965
      },
      "rank": 212
    },
    {
      "feature_index": 49,
      "feature_name": "feature_49",
      "feature_code": "def feature(text: str) -> float:\n    'Sentiment score using TextBlob'\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009326358834699071,
        "mean_shap": -4.777563850054638e-05,
        "std_shap": 0.0011268680730743017,
        "min_shap": -0.002880676341844775,
        "max_shap": 0.0036824176312558214
      },
      "rank": 213
    },
    {
      "feature_index": 409,
      "feature_name": "feature_409",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of capitalized words to total words\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009206087333168152,
        "mean_shap": 1.807013528398762e-05,
        "std_shap": 0.0011500910650012728,
        "min_shap": -0.00196716170945529,
        "max_shap": 0.003943994589665372
      },
      "rank": 214
    },
    {
      "feature_index": 244,
      "feature_name": "feature_244",
      "feature_code": "def feature(text: str) -> float:\n    'Average positive sentiment score from VADER'\n    scores = sia.polarity_scores(text)\n    return float(scores['pos'])\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009046334833641215,
        "mean_shap": -4.052406305722859e-05,
        "std_shap": 0.0011069002020643297,
        "min_shap": -0.0024919368110230414,
        "max_shap": 0.003490138188335101
      },
      "rank": 215
    },
    {
      "feature_index": 437,
      "feature_name": "feature_437",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment score using TextBlob\"\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0009011404068714809,
        "mean_shap": -4.952623028040274e-05,
        "std_shap": 0.0011328320575734413,
        "min_shap": -0.0039319919141732294,
        "max_shap": 0.002799993007749471
      },
      "rank": 216
    },
    {
      "feature_index": 307,
      "feature_name": "feature_307",
      "feature_code": "def feature(text: str) -> float:\n    'Flesch Reading Ease score (higher is easier to read)'\n    import textstat\n    return textstat.flesch_reading_ease(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000898113991358215,
        "mean_shap": -0.00023371495698840298,
        "std_shap": 0.0010985757168539454,
        "min_shap": -0.003919330490188741,
        "max_shap": 0.0031634218604740896
      },
      "rank": 217
    },
    {
      "feature_index": 331,
      "feature_name": "feature_331",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all sentences in the text\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s]\n    return float(sum(lengths)) if lengths else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008964240685678746,
        "mean_shap": -0.000132688806288554,
        "std_shap": 0.0015705977951048391,
        "min_shap": -0.0029316710975742025,
        "max_shap": 0.006664743212960786
      },
      "rank": 218
    },
    {
      "feature_index": 14,
      "feature_name": "feature_14",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity using TextBlob'\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008898460923953039,
        "mean_shap": -4.1673773994433784e-05,
        "std_shap": 0.0010651215132748448,
        "min_shap": -0.003661654499354329,
        "max_shap": 0.0024471019784057975
      },
      "rank": 219
    },
    {
      "feature_index": 314,
      "feature_name": "feature_314",
      "feature_code": "def feature(text: str) -> float:\n    'Readability score using Flesch Reading Ease'\n    import textstat\n    readability = textstat.flesch_reading_ease(text)\n    return float(readability)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008872102355855483,
        "mean_shap": -0.0002712300026717943,
        "std_shap": 0.0010670424051851574,
        "min_shap": -0.004035244063731921,
        "max_shap": 0.0023871154705615316
      },
      "rank": 220
    },
    {
      "feature_index": 55,
      "feature_name": "feature_55",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of paragraphs in the text'\n    paragraphs = text.split('\\n')\n    return float(len(paragraphs))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008683657910759269,
        "mean_shap": -0.0002644282136723801,
        "std_shap": 0.0011190115390428265,
        "min_shap": -0.00359827360208809,
        "max_shap": 0.003275126246909123
      },
      "rank": 221
    },
    {
      "feature_index": 11,
      "feature_name": "feature_11",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008664393332463356,
        "mean_shap": 4.698036258466801e-05,
        "std_shap": 0.0010964093685773232,
        "min_shap": -0.0019217701654309605,
        "max_shap": 0.004475153425836297
      },
      "rank": 222
    },
    {
      "feature_index": 208,
      "feature_name": "feature_208",
      "feature_code": "def feature(text: str) -> float:\n    'Variability in word length measured by standard deviation'\n    words = text.split()\n    if not words:\n        return 0.0\n    word_lengths = [len(word) for word in words]\n    return float(statistics.pstdev(word_lengths)) if len(word_lengths) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000863862008443712,
        "mean_shap": 0.00020342442024338754,
        "std_shap": 0.0014359491076114874,
        "min_shap": -0.010938825501697813,
        "max_shap": 0.003193215706307015
      },
      "rank": 223
    },
    {
      "feature_index": 177,
      "feature_name": "feature_177",
      "feature_code": "def feature(text: str) -> float:\n    'Count of words containing numerical digits'\n    digit_word_count = sum(1 for word in text.split() if any(char.isdigit() for char in word))\n    return float(digit_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008559930611524582,
        "mean_shap": -7.666742537623564e-05,
        "std_shap": 0.001568623012043787,
        "min_shap": -0.001864892396517277,
        "max_shap": 0.011813183786683357
      },
      "rank": 224
    },
    {
      "feature_index": 73,
      "feature_name": "feature_73",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of adjectives to total words in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    doc = nlp(text)\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return adj_count / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008550017405101064,
        "mean_shap": -3.271915118124515e-05,
        "std_shap": 0.0011167689810192513,
        "min_shap": -0.003966936156555063,
        "max_shap": 0.003262259469269815
      },
      "rank": 225
    },
    {
      "feature_index": 257,
      "feature_name": "feature_257",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of complex words (greater than 2 syllables) in the text\"\n    complex_word_count = sum(1 for word in text.split() if textstat.syllable_count(word) > 2)\n    total_words = len(text.split())\n    return float(complex_word_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008495132038066022,
        "mean_shap": -1.3673133651307958e-05,
        "std_shap": 0.001118833916767654,
        "min_shap": -0.0032666277459330716,
        "max_shap": 0.0043683327871118696
      },
      "rank": 226
    },
    {
      "feature_index": 304,
      "feature_name": "feature_304",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique verbs in the text'\n    doc = nlp(text)\n    unique_verbs = len(set(token.text.lower() for token in doc if token.pos_ == 'VERB'))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000847884940529188,
        "mean_shap": 0.00024157687419292532,
        "std_shap": 0.00131166175762741,
        "min_shap": -0.0016632669415757684,
        "max_shap": 0.005123846243529645
      },
      "rank": 227
    },
    {
      "feature_index": 330,
      "feature_name": "feature_330",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008240000712771622,
        "mean_shap": -4.852033161816125e-05,
        "std_shap": 0.001039767052172527,
        "min_shap": -0.002473336614837855,
        "max_shap": 0.003984000068466006
      },
      "rank": 228
    },
    {
      "feature_index": 336,
      "feature_name": "feature_336",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of words in the text excluding stopwords'\n    doc = nlp(text)\n    words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n    if not words:\n        return 0.0\n    return float(sum(len(word) for word in words)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008194625581037105,
        "mean_shap": -1.388470055319535e-05,
        "std_shap": 0.0011509982539648451,
        "min_shap": -0.003883904583420872,
        "max_shap": 0.005257321997867059
      },
      "rank": 229
    },
    {
      "feature_index": 338,
      "feature_name": "feature_338",
      "feature_code": "def feature(text: str) -> float:\n    'Average polarity score derived from TextBlob sentiment analysis'\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008079193203354542,
        "mean_shap": -6.699241248324106e-06,
        "std_shap": 0.0009683611986152988,
        "min_shap": -0.0023703853217534525,
        "max_shap": 0.002912880538356744
      },
      "rank": 230
    },
    {
      "feature_index": 166,
      "feature_name": "feature_166",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0008046071712026579,
        "mean_shap": 0.00013829395289026436,
        "std_shap": 0.0015076832689121821,
        "min_shap": -0.012892986598774834,
        "max_shap": 0.0036685754703096375
      },
      "rank": 231
    },
    {
      "feature_index": 292,
      "feature_name": "feature_292",
      "feature_code": "def feature(text: str) -> float:\n    'Average polarity score using TextBlob for sentiment alignment'\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007998343916579034,
        "mean_shap": 7.068719397079664e-06,
        "std_shap": 0.0010101371062013708,
        "min_shap": -0.003051647397512676,
        "max_shap": 0.004303392001683793
      },
      "rank": 232
    },
    {
      "feature_index": 126,
      "feature_name": "feature_126",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of word lengths in the text\"\n    words = text.split()\n    lengths = [len(word) for word in words]\n    if not lengths:\n        return 0.0\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007993228859315067,
        "mean_shap": 0.00016083354937567712,
        "std_shap": 0.0015156850312666773,
        "min_shap": -0.010300224989732475,
        "max_shap": 0.0023825752411923977
      },
      "rank": 233
    },
    {
      "feature_index": 239,
      "feature_name": "feature_239",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of capitalized words to total words\"\n    words = text.split()\n    if len(words) == 0:\n        return 0.0\n    capitalized_count = sum(1 for word in words if word.istitle())\n    return float(capitalized_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007965952840547563,
        "mean_shap": -2.418090307523524e-07,
        "std_shap": 0.0010065208250992976,
        "min_shap": -0.0017092197446765235,
        "max_shap": 0.004352237481036515
      },
      "rank": 234
    },
    {
      "feature_index": 191,
      "feature_name": "feature_191",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity from TextBlob\"\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007963239362025942,
        "mean_shap": 1.2298760816206586e-05,
        "std_shap": 0.0010039150410675126,
        "min_shap": -0.0030630096076475904,
        "max_shap": 0.002962137043553933
      },
      "rank": 235
    },
    {
      "feature_index": 250,
      "feature_name": "feature_250",
      "feature_code": "def feature(text: str) -> float:\n    'Entropy of word frequency in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    freq_map = Counter(words)\n    probabilities = [freq / len(words) for freq in freq_map.values()]\n    return -sum(p * math.log2(p) for p in probabilities if p > 0)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007944370230921835,
        "mean_shap": -8.458836678650976e-05,
        "std_shap": 0.001066891219839026,
        "min_shap": -0.0034821944690108346,
        "max_shap": 0.004682083335448954
      },
      "rank": 236
    },
    {
      "feature_index": 53,
      "feature_name": "feature_53",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths)) if len(lengths) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007901461961396925,
        "mean_shap": 0.00022524455889196728,
        "std_shap": 0.0014513567722431047,
        "min_shap": -0.011981397450649332,
        "max_shap": 0.0022898700095928194
      },
      "rank": 237
    },
    {
      "feature_index": 379,
      "feature_name": "feature_379",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score from TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000789148765716562,
        "mean_shap": -4.7996843456739896e-05,
        "std_shap": 0.0009891481356438696,
        "min_shap": -0.002720078725324358,
        "max_shap": 0.003350093732525862
      },
      "rank": 238
    },
    {
      "feature_index": 334,
      "feature_name": "feature_334",
      "feature_code": "def feature(text: str) -> float:\n    'Density of named entities in the text'\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    return entity_count / len(doc) if len(doc) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007876348111067673,
        "mean_shap": -2.1291104867231797e-05,
        "std_shap": 0.0011772353680224125,
        "min_shap": -0.0020310366312031843,
        "max_shap": 0.006210060769109422
      },
      "rank": 239
    },
    {
      "feature_index": 463,
      "feature_name": "feature_463",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of verbs in the text\"\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007842440352248824,
        "mean_shap": 1.2386527239581861e-05,
        "std_shap": 0.0011752869216659493,
        "min_shap": -0.001717195137962206,
        "max_shap": 0.004624930775999131
      },
      "rank": 240
    },
    {
      "feature_index": 454,
      "feature_name": "feature_454",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of verbs in the text\"\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007783733933509123,
        "mean_shap": -2.8700217043740856e-05,
        "std_shap": 0.001105415889315612,
        "min_shap": -0.0023725490860027937,
        "max_shap": 0.0041622207468133285
      },
      "rank": 241
    },
    {
      "feature_index": 419,
      "feature_name": "feature_419",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of named entities per sentence'\n    doc = nlp(text)\n    sentences = list(doc.sents)\n    entity_count = len(doc.ents)\n    if not sentences:\n        return 0.0\n    return float(entity_count) / len(sentences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007753963213195393,
        "mean_shap": -0.00011651380376584543,
        "std_shap": 0.0011339148778080049,
        "min_shap": -0.0032817664194131186,
        "max_shap": 0.006613409775707746
      },
      "rank": 242
    },
    {
      "feature_index": 92,
      "feature_name": "feature_92",
      "feature_code": "def feature(text: str) -> float:\n    'Sum of lengths of all words in the text'\n    return float(sum(len(word) for word in text.split()))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007726871543236979,
        "mean_shap": -0.0001319985409069497,
        "std_shap": 0.0010789533767452052,
        "min_shap": -0.0022281592130584684,
        "max_shap": 0.004338178350108367
      },
      "rank": 243
    },
    {
      "feature_index": 350,
      "feature_name": "feature_350",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of complex words (3+ syllables) to total words in the text'\n    words = text.split()\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    total_words = len(words)\n    return float(complex_word_count) / total_words if total_words > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007679664203524562,
        "mean_shap": -6.60439763393754e-05,
        "std_shap": 0.0009649544505889619,
        "min_shap": -0.0025489900394320714,
        "max_shap": 0.003554813634126237
      },
      "rank": 244
    },
    {
      "feature_index": 62,
      "feature_name": "feature_62",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007554287896777393,
        "mean_shap": 0.00021262473225087115,
        "std_shap": 0.0015246763735838923,
        "min_shap": -0.014564570897202955,
        "max_shap": 0.0032107352988106175
      },
      "rank": 245
    },
    {
      "feature_index": 113,
      "feature_name": "feature_113",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text divided by the number of sentences\"\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    sentence_count = len(list(doc.sents))\n    return float(entity_count) / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007554190203629408,
        "mean_shap": -6.351052306758167e-05,
        "std_shap": 0.0012901104958363706,
        "min_shap": -0.002163412101378544,
        "max_shap": 0.008855732031160872
      },
      "rank": 246
    },
    {
      "feature_index": 345,
      "feature_name": "feature_345",
      "feature_code": "def feature(text: str) -> float:\n    \"Average polarity score from TextBlob for sentiment\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007505152416381345,
        "mean_shap": 2.455749583371334e-05,
        "std_shap": 0.0008832924274940309,
        "min_shap": -0.002268401009289871,
        "max_shap": 0.0025401785537905093
      },
      "rank": 247
    },
    {
      "feature_index": 225,
      "feature_name": "feature_225",
      "feature_code": "def feature(text: str) -> float:\n    \"Average depth of the dependency tree\"\n    def get_tree_depth(token):\n        if not list(token.children):\n            return 0\n        return 1 + max(get_tree_depth(child) for child in token.children)\n    \n    doc = nlp(text)\n    depths = [get_tree_depth(sent.root) for sent in doc.sents] if list(doc.sents) else []\n    return float(statistics.mean(depths)) if depths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007395829944845991,
        "mean_shap": -7.568901970190061e-06,
        "std_shap": 0.0009414520052819518,
        "min_shap": -0.0028258247343617158,
        "max_shap": 0.0026975690889858173
      },
      "rank": 248
    },
    {
      "feature_index": 264,
      "feature_name": "feature_264",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity of the text'\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007374946240283841,
        "mean_shap": 1.5215380179510695e-05,
        "std_shap": 0.0008929697364347329,
        "min_shap": -0.0022565578376302644,
        "max_shap": 0.0027729560448423343
      },
      "rank": 249
    },
    {
      "feature_index": 189,
      "feature_name": "feature_189",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique sentences to total sentences in the text\"\n    sentences = list(nlp(text).sents)\n    unique_sentences = len(set(sent.text.strip() for sent in sentences))\n    return unique_sentences / len(sentences) if sentences else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000732433748373999,
        "mean_shap": -2.2448596823084906e-05,
        "std_shap": 0.0011441452764325388,
        "min_shap": -0.003998446977222145,
        "max_shap": 0.005586445734927049
      },
      "rank": 250
    },
    {
      "feature_index": 402,
      "feature_name": "feature_402",
      "feature_code": "def feature(text: str) -> float:\n    'Average polarity score using TextBlob for sentiment alignment'\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007226775473559127,
        "mean_shap": -2.974093295752986e-05,
        "std_shap": 0.0008705831326658627,
        "min_shap": -0.0019648143454877873,
        "max_shap": 0.0023886443171477105
      },
      "rank": 251
    },
    {
      "feature_index": 1,
      "feature_name": "feature_1",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0007096881108629499,
        "mean_shap": -6.113281513525359e-05,
        "std_shap": 0.0009579824107415924,
        "min_shap": -0.0013817197154942797,
        "max_shap": 0.0037789644882641723
      },
      "rank": 252
    },
    {
      "feature_index": 233,
      "feature_name": "feature_233",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentiment polarity score from TextBlob\"\n    blob = TextBlob(text)\n    return blob.sentiment.polarity\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006938282453154336,
        "mean_shap": -1.293120295395212e-05,
        "std_shap": 0.0008267604179951765,
        "min_shap": -0.0020400288138670256,
        "max_shap": 0.002009956383271423
      },
      "rank": 253
    },
    {
      "feature_index": 160,
      "feature_name": "feature_160",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of complex words (three or more syllables) to total words\"\n    complex_word_count = sum(1 for word in text.split() if textstat.syllable_count(word) >= 3)\n    total_words = len(text.split())\n    return (complex_word_count / total_words) if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006922484691284412,
        "mean_shap": -9.35963255268171e-05,
        "std_shap": 0.0008568444397335734,
        "min_shap": -0.0023634528360413315,
        "max_shap": 0.0034086388720832336
      },
      "rank": 254
    },
    {
      "feature_index": 145,
      "feature_name": "feature_145",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of complex words (3+ syllables) to total words in the text'\n    words = text.split()\n    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n    total_words = len(words)\n    return float(complex_word_count) / total_words if total_words > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006905714696300486,
        "mean_shap": -0.00011741021992827807,
        "std_shap": 0.0008470971804627367,
        "min_shap": -0.002368118304871869,
        "max_shap": 0.002296136635534014
      },
      "rank": 255
    },
    {
      "feature_index": 168,
      "feature_name": "feature_168",
      "feature_code": "def feature(text: str) -> float:\n    'Sentence complexity based on average depth of dependency trees'\n    def get_tree_depth(token):\n        if not list(token.children):\n            return 0\n        return 1 + max(get_tree_depth(child) for child in token.children)\n    \n    doc = nlp(text)\n    depths = [get_tree_depth(sent.root) for sent in doc.sents if list(sent)]\n    return float(statistics.mean(depths)) if depths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006894568583278946,
        "mean_shap": 2.6203103216394795e-05,
        "std_shap": 0.00090071190920911,
        "min_shap": -0.002969949606289435,
        "max_shap": 0.0030675722977778854
      },
      "rank": 256
    },
    {
      "feature_index": 167,
      "feature_name": "feature_167",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentiment polarity score using TextBlob'\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006866954354393965,
        "mean_shap": -4.602513360633125e-05,
        "std_shap": 0.0008482997343891646,
        "min_shap": -0.0026070017617832045,
        "max_shap": 0.0023682357767666118
      },
      "rank": 257
    },
    {
      "feature_index": 294,
      "feature_name": "feature_294",
      "feature_code": "def feature(text: str) -> float:\n    \"Standard deviation of word lengths in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    mean_length = statistics.mean(lengths)\n    std_dev = statistics.pstdev(lengths)\n    return std_dev\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006823764456774818,
        "mean_shap": 0.0001801535354043842,
        "std_shap": 0.0011609003816659955,
        "min_shap": -0.008256488511218811,
        "max_shap": 0.002707507164497728
      },
      "rank": 258
    },
    {
      "feature_index": 157,
      "feature_name": "feature_157",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs in the text\"\n    doc = nlp(text)\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB' and not token.is_stop))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006797920223274179,
        "mean_shap": 0.0003048664498789769,
        "std_shap": 0.0008687656852247682,
        "min_shap": -0.0015225876478426558,
        "max_shap": 0.0037629517374224417
      },
      "rank": 259
    },
    {
      "feature_index": 8,
      "feature_name": "feature_8",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length in terms of characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s.strip()]\n    return float(sum(lengths)) / len(lengths) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006755536040253947,
        "mean_shap": 0.00011629480224345206,
        "std_shap": 0.0010321715836674522,
        "min_shap": -0.005160500067609536,
        "max_shap": 0.0023737126446425763
      },
      "rank": 260
    },
    {
      "feature_index": 104,
      "feature_name": "feature_104",
      "feature_code": "def feature(text: str) -> float:\n    'Standard deviation of word lengths in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006722614850774972,
        "mean_shap": 9.211196243939802e-05,
        "std_shap": 0.0015397631668591556,
        "min_shap": -0.017158223330533497,
        "max_shap": 0.001990555600220267
      },
      "rank": 261
    },
    {
      "feature_index": 44,
      "feature_name": "feature_44",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of unique words to total words in the text\"\n    words = text.split()\n    unique_words = len(set(words))\n    return float(unique_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006591469345798707,
        "mean_shap": -0.00011002357146520125,
        "std_shap": 0.0007636131319054346,
        "min_shap": -0.002132547876392007,
        "max_shap": 0.0018444809992071202
      },
      "rank": 262
    },
    {
      "feature_index": 258,
      "feature_name": "feature_258",
      "feature_code": "def feature(text: str) -> float:\n    'Count of words ending with \"ly\" (common for adverbs)'\n    return float(len(re.findall(r'\\b\\w+ly\\b', text)))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006448243482166072,
        "mean_shap": -8.924942070237924e-05,
        "std_shap": 0.0008671157214598039,
        "min_shap": -0.002088453809648685,
        "max_shap": 0.004050822543160924
      },
      "rank": 263
    },
    {
      "feature_index": 246,
      "feature_name": "feature_246",
      "feature_code": "def feature(text: str) -> float:\n    'Variability in word lengths (standard deviation)'\n    words = text.split()\n    if not words:\n        return 0.0\n    lengths = [len(word) for word in words]\n    return float(statistics.pstdev(lengths)) if len(lengths) > 1 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000633580536203308,
        "mean_shap": 0.00017164860882912922,
        "std_shap": 0.0011552136977595697,
        "min_shap": -0.00777202971512355,
        "max_shap": 0.0022343541903478955
      },
      "rank": 264
    },
    {
      "feature_index": 226,
      "feature_name": "feature_226",
      "feature_code": "def feature(text: str) -> float:\n    'Named entity density in the text'\n    doc = nlp(text)\n    return float(len(doc.ents)) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006300256181281008,
        "mean_shap": 1.5566382901071101e-06,
        "std_shap": 0.0008941584996459288,
        "min_shap": -0.001853665559625913,
        "max_shap": 0.004153406027260374
      },
      "rank": 265
    },
    {
      "feature_index": 165,
      "feature_name": "feature_165",
      "feature_code": "def feature(text: str) -> float:\n    'Frequency of named entities (per sentence)'\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    sentence_count = len(list(doc.sents))\n    return float(entity_count) / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000622581582470931,
        "mean_shap": 1.0851345694517667e-05,
        "std_shap": 0.0009536121634726904,
        "min_shap": -0.0033820272307366244,
        "max_shap": 0.0049389145817843845
      },
      "rank": 266
    },
    {
      "feature_index": 354,
      "feature_name": "feature_354",
      "feature_code": "def feature(text: str) -> float:\n    \"Sentiment polarity score of the text using TextBlob\"\n    from textblob import TextBlob\n    blob = TextBlob(text)\n    return float(blob.sentiment.polarity)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006211900177727992,
        "mean_shap": -1.8561920876363093e-05,
        "std_shap": 0.0007437227242934009,
        "min_shap": -0.0016930443432190837,
        "max_shap": 0.002358181806654077
      },
      "rank": 267
    },
    {
      "feature_index": 247,
      "feature_name": "feature_247",
      "feature_code": "def feature(text: str) -> float:\n    'Density of named entities in the text'\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    total_words = len(doc)\n    return float(entity_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006207717894330564,
        "mean_shap": -2.0978455855832575e-05,
        "std_shap": 0.0008522038977886024,
        "min_shap": -0.00186649755068763,
        "max_shap": 0.003464602602386896
      },
      "rank": 268
    },
    {
      "feature_index": 147,
      "feature_name": "feature_147",
      "feature_code": "def feature(text: str) -> float:\n    'Named entity ratio in the text'\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    total_tokens = len(doc)\n    return (entity_count / total_tokens) if total_tokens > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006090654457779555,
        "mean_shap": -0.00011160075840268153,
        "std_shap": 0.000930927738927176,
        "min_shap": -0.001991621523210064,
        "max_shap": 0.005127022282576128
      },
      "rank": 269
    },
    {
      "feature_index": 424,
      "feature_name": "feature_424",
      "feature_code": "def feature(text: str) -> float:\n    'Frequency of the most common noun in the text'\n    from collections import Counter\n    doc = nlp(text)\n    nouns = [token.text.lower() for token in doc if token.pos_ == 'NOUN']\n    if not nouns:\n        return 0.0\n    most_common_noun_count = Counter(nouns).most_common(1)[0][1]\n    return float(most_common_noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006042219807476819,
        "mean_shap": 7.909154425389535e-05,
        "std_shap": 0.000852194163641734,
        "min_shap": -0.0035677140904878477,
        "max_shap": 0.002821126661119572
      },
      "rank": 270
    },
    {
      "feature_index": 143,
      "feature_name": "feature_143",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of exclusive ingredients or references to food items in the text\"\n    food_keywords = ['ingredient', 'ingredients', 'food', 'dish', 'meal']\n    food_count = sum(text.lower().count(keyword) for keyword in food_keywords)\n    return float(food_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0006038948751338969,
        "mean_shap": 0.00029299456351930813,
        "std_shap": 0.0009406629294675886,
        "min_shap": -0.0058105336026603475,
        "max_shap": 0.002479267722661611
      },
      "rank": 271
    },
    {
      "feature_index": 61,
      "feature_name": "feature_61",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return verb_count / len(doc) if len(doc) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005977660901632108,
        "mean_shap": 4.158465831808976e-06,
        "std_shap": 0.0007493564080461156,
        "min_shap": -0.0015823243338248649,
        "max_shap": 0.0025947866430091177
      },
      "rank": 272
    },
    {
      "feature_index": 171,
      "feature_name": "feature_171",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of stopwords to total words in the text'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    total_count = len(doc)\n    return float(stopword_count) / total_count if total_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005731922998673985,
        "mean_shap": 8.168694248374195e-05,
        "std_shap": 0.0007798979732212418,
        "min_shap": -0.0012191788928803027,
        "max_shap": 0.0035235900317242877
      },
      "rank": 273
    },
    {
      "feature_index": 404,
      "feature_name": "feature_404",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of sentences in terms of characters'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s]\n    return float(sum(lengths)) / len(lengths) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005725440718013564,
        "mean_shap": 0.0001150698476316525,
        "std_shap": 0.00081548895562734,
        "min_shap": -0.004119863437819922,
        "max_shap": 0.0015118389302921163
      },
      "rank": 274
    },
    {
      "feature_index": 365,
      "feature_name": "feature_365",
      "feature_code": "def feature(text: str) -> float:\n    'Number of verbs used in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005660959011818424,
        "mean_shap": 7.908686284586219e-05,
        "std_shap": 0.0008047165684852744,
        "min_shap": -0.0015032827052016927,
        "max_shap": 0.0026325650874896704
      },
      "rank": 275
    },
    {
      "feature_index": 443,
      "feature_name": "feature_443",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of adverbs in the text\"\n    doc = nlp(text)\n    adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n    return float(adverb_count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000564547637949687,
        "mean_shap": -2.3458004834191568e-05,
        "std_shap": 0.00080657829701272,
        "min_shap": -0.0012363205117271706,
        "max_shap": 0.004191329125798755
      },
      "rank": 276
    },
    {
      "feature_index": 329,
      "feature_name": "feature_329",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of adjectives to nouns in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    if noun_count == 0:\n        return 0.0\n    return adj_count / noun_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000559300534462728,
        "mean_shap": 2.106525073483145e-05,
        "std_shap": 0.0007157988310775719,
        "min_shap": -0.003398429127910929,
        "max_shap": 0.002103805783211877
      },
      "rank": 277
    },
    {
      "feature_index": 423,
      "feature_name": "feature_423",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of unique terms against total words in the text\"\n    words = text.split()\n    unique_words = len(set(words))\n    return float(unique_words) / len(words) if len(words) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005515193531922763,
        "mean_shap": 2.423718141435449e-05,
        "std_shap": 0.0006784989394423996,
        "min_shap": -0.002109133539557194,
        "max_shap": 0.0016873917830019232
      },
      "rank": 278
    },
    {
      "feature_index": 355,
      "feature_name": "feature_355",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of adjectives to total nouns in the text'\n    doc = nlp(text)\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return adjective_count / noun_count if noun_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005491513467069709,
        "mean_shap": 0.00011358178435826529,
        "std_shap": 0.0006973951382177298,
        "min_shap": -0.0016250050144965547,
        "max_shap": 0.0022412083391690793
      },
      "rank": 279
    },
    {
      "feature_index": 35,
      "feature_name": "feature_35",
      "feature_code": "def feature(text: str) -> float:\n    'Density of named entities in the text'\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    total_tokens = len(doc)\n    return float(entity_count) / total_tokens if total_tokens > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005466272257864918,
        "mean_shap": -0.00010993917694643687,
        "std_shap": 0.0008369690060747141,
        "min_shap": -0.002223202939595454,
        "max_shap": 0.004248232634368402
      },
      "rank": 280
    },
    {
      "feature_index": 36,
      "feature_name": "feature_36",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of named entities in the text divided by the number of sentences\"\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    sentence_count = len(list(doc.sents))\n    return float(entity_count) / sentence_count if sentence_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005389022007373187,
        "mean_shap": -2.6675130984064284e-05,
        "std_shap": 0.0007684594236651496,
        "min_shap": -0.001957881563568982,
        "max_shap": 0.003923635207375516
      },
      "rank": 281
    },
    {
      "feature_index": 340,
      "feature_name": "feature_340",
      "feature_code": "def feature(text: str) -> float:\n    'Count of questions in the text'\n    question_count = text.count('?')\n    return float(question_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005265575963928639,
        "mean_shap": -2.9659139323315056e-05,
        "std_shap": 0.0007452664255759893,
        "min_shap": -0.002734105795956154,
        "max_shap": 0.002797791125948203
      },
      "rank": 282
    },
    {
      "feature_index": 181,
      "feature_name": "feature_181",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    total_words = len(doc)\n    return float(verb_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005179598506728663,
        "mean_shap": -6.536300539668752e-05,
        "std_shap": 0.0006577195965774598,
        "min_shap": -0.002135897212081743,
        "max_shap": 0.0021177674931594434
      },
      "rank": 283
    },
    {
      "feature_index": 204,
      "feature_name": "feature_204",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of adjectives to total nouns in the text'\n    doc = nlp(text)\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return adjective_count / noun_count if noun_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000515572930532897,
        "mean_shap": 6.270755670657946e-05,
        "std_shap": 0.0006802938180288381,
        "min_shap": -0.0031164214212795237,
        "max_shap": 0.002909696839291751
      },
      "rank": 284
    },
    {
      "feature_index": 94,
      "feature_name": "feature_94",
      "feature_code": "def feature(text: str) -> float:\n    'Frequency of the most common noun in the text'\n    from collections import Counter\n    doc = nlp(text)\n    nouns = [token.text.lower() for token in doc if token.pos_ == 'NOUN']\n    if not nouns:\n        return 0.0\n    most_common_noun_count = Counter(nouns).most_common(1)[0][1]\n    return float(most_common_noun_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005153518241194305,
        "mean_shap": 0.0001583909796175934,
        "std_shap": 0.0007085809182109692,
        "min_shap": -0.003024763776751705,
        "max_shap": 0.0019793805596584465
      },
      "rank": 285
    },
    {
      "feature_index": 19,
      "feature_name": "feature_19",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005153320921371812,
        "mean_shap": -6.045385122230775e-05,
        "std_shap": 0.0008337004854498518,
        "min_shap": -0.002662649383259452,
        "max_shap": 0.003434988115740063
      },
      "rank": 286
    },
    {
      "feature_index": 370,
      "feature_name": "feature_370",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of punctuation marks in the text\"\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return float(punctuation_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005087256664673029,
        "mean_shap": -0.00015551309378779104,
        "std_shap": 0.0006098425217540593,
        "min_shap": -0.0020991566177002425,
        "max_shap": 0.0013042495456837317
      },
      "rank": 287
    },
    {
      "feature_index": 45,
      "feature_name": "feature_45",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of unique words (types) in the text'\n    words = set(text.split())\n    return float(len(words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0005001509951586783,
        "mean_shap": 2.8645962022639076e-05,
        "std_shap": 0.0007081400601155344,
        "min_shap": -0.00126641329423626,
        "max_shap": 0.003086436776470558
      },
      "rank": 288
    },
    {
      "feature_index": 298,
      "feature_name": "feature_298",
      "feature_code": "def feature(text: str) -> float:\n    'Density of named entities in the text'\n    doc = nlp(text)\n    entity_count = len(doc.ents)\n    return entity_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004934156983135376,
        "mean_shap": -9.80185408863878e-05,
        "std_shap": 0.0007132857864513856,
        "min_shap": -0.0016773440421942133,
        "max_shap": 0.0037137708016168653
      },
      "rank": 289
    },
    {
      "feature_index": 364,
      "feature_name": "feature_364",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical density measured as the ratio of unique words to total words\"\n    words = text.split()\n    unique_word_count = len(set(words))\n    if not words:\n        return 0.0\n    return float(unique_word_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004810141103066261,
        "mean_shap": -3.9582791666231954e-05,
        "std_shap": 0.0006016736136365099,
        "min_shap": -0.0015075178626063304,
        "max_shap": 0.0017786851032076416
      },
      "rank": 290
    },
    {
      "feature_index": 203,
      "feature_name": "feature_203",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words)) if words else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004796346723806695,
        "mean_shap": 9.541295773231091e-06,
        "std_shap": 0.0008478887218257581,
        "min_shap": -0.0033174364268057045,
        "max_shap": 0.004003429390684808
      },
      "rank": 291
    },
    {
      "feature_index": 439,
      "feature_name": "feature_439",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of unique words to total words in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    unique_words = len(set(words))\n    return float(unique_words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004787039229022125,
        "mean_shap": -9.385943770370116e-05,
        "std_shap": 0.0005805951141173233,
        "min_shap": -0.0022165814575057473,
        "max_shap": 0.0012283564902947715
      },
      "rank": 292
    },
    {
      "feature_index": 396,
      "feature_name": "feature_396",
      "feature_code": "def feature(text: str) -> float:\n    \"Total number of punctuation marks in the text\"\n    punctuation_count = sum(1 for char in text if char in string.punctuation)\n    return float(punctuation_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004709247027642798,
        "mean_shap": -0.000275018225098022,
        "std_shap": 0.0005437707768573562,
        "min_shap": -0.001894036451330445,
        "max_shap": 0.002377932766558786
      },
      "rank": 293
    },
    {
      "feature_index": 357,
      "feature_name": "feature_357",
      "feature_code": "def feature(text: str) -> float:\n    'Count of direct questions in the text (ends with ?)'\n    return float(text.count('?'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004707401631865717,
        "mean_shap": 2.0768383714945494e-05,
        "std_shap": 0.0006942332107487781,
        "min_shap": -0.002740250694012666,
        "max_shap": 0.0029038314457798557
      },
      "rank": 294
    },
    {
      "feature_index": 90,
      "feature_name": "feature_90",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of tokens that are verbs in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return float(verb_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00046442831404399656,
        "mean_shap": 2.1450799057890743e-05,
        "std_shap": 0.0005826377624897563,
        "min_shap": -0.0025398670769312937,
        "max_shap": 0.001307762036848424
      },
      "rank": 295
    },
    {
      "feature_index": 178,
      "feature_name": "feature_178",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    total_words = len(doc)\n    return float(verb_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004639092378082823,
        "mean_shap": -3.60162214905811e-05,
        "std_shap": 0.0005619542545070327,
        "min_shap": -0.0017147484062147087,
        "max_shap": 0.0016779534555441038
      },
      "rank": 296
    },
    {
      "feature_index": 436,
      "feature_name": "feature_436",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of stop words to total words in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return stop_word_count / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004565619899210517,
        "mean_shap": -3.708461249512309e-05,
        "std_shap": 0.000600366971127187,
        "min_shap": -0.0013002373056063793,
        "max_shap": 0.002229451776861804
      },
      "rank": 297
    },
    {
      "feature_index": 207,
      "feature_name": "feature_207",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all sentences in the text\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    return float(sum(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00045617702582324355,
        "mean_shap": 8.897281410850212e-05,
        "std_shap": 0.0008088347401589758,
        "min_shap": -0.00394450686771496,
        "max_shap": 0.004048648217273693
      },
      "rank": 298
    },
    {
      "feature_index": 179,
      "feature_name": "feature_179",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of stopwords in the text'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return stopword_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004486347123120489,
        "mean_shap": -3.1578366735194976e-07,
        "std_shap": 0.0006006321728060209,
        "min_shap": -0.0012153851316080977,
        "max_shap": 0.002044613004204183
      },
      "rank": 299
    },
    {
      "feature_index": 5,
      "feature_name": "feature_5",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentence length measured in characters'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004484066356582469,
        "mean_shap": 8.213594466682828e-05,
        "std_shap": 0.0006280814253418834,
        "min_shap": -0.00299952259715901,
        "max_shap": 0.0016016326332913976
      },
      "rank": 300
    },
    {
      "feature_index": 453,
      "feature_name": "feature_453",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of stopwords in the text'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    total_count = len(doc)\n    return float(stopword_count) / total_count if total_count > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00044712748893444774,
        "mean_shap": 3.630478215633879e-05,
        "std_shap": 0.0006220172944482612,
        "min_shap": -0.0015056058243323315,
        "max_shap": 0.002721783916207025
      },
      "rank": 301
    },
    {
      "feature_index": 75,
      "feature_name": "feature_75",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    total_words = len(doc)\n    return float(verb_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00044706317882308404,
        "mean_shap": -2.6318849262594048e-05,
        "std_shap": 0.0006003438841844752,
        "min_shap": -0.0031136066620814124,
        "max_shap": 0.0016763137962237534
      },
      "rank": 302
    },
    {
      "feature_index": 306,
      "feature_name": "feature_306",
      "feature_code": "def feature(text: str) -> float:\n    'Count of punctuation marks per 100 words'\n    punctuation_count = sum(1 for char in text if char in string.punctuation)\n    word_count = len(text.split())\n    return (punctuation_count / word_count * 100) if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004400036475836975,
        "mean_shap": -0.0001036703139626275,
        "std_shap": 0.000541514977261931,
        "min_shap": -0.0020682665372713086,
        "max_shap": 0.0012838856641194198
      },
      "rank": 303
    },
    {
      "feature_index": 232,
      "feature_name": "feature_232",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical density based on non-stop words\"\n    doc = nlp(text)\n    non_stop_words = [token for token in doc if not token.is_stop]\n    lexical_density = len(non_stop_words) / len(doc) if len(doc) > 0 else 0.0\n    return lexical_density\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00043548084638015885,
        "mean_shap": 7.952779837618795e-06,
        "std_shap": 0.0006138874835785284,
        "min_shap": -0.0008148254536332993,
        "max_shap": 0.003041205955973497
      },
      "rank": 304
    },
    {
      "feature_index": 156,
      "feature_name": "feature_156",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00043546881643198126,
        "mean_shap": -0.00015538139192112208,
        "std_shap": 0.0005058786757005386,
        "min_shap": -0.0015744512400480933,
        "max_shap": 0.0016301192671596288
      },
      "rank": 305
    },
    {
      "feature_index": 18,
      "feature_name": "feature_18",
      "feature_code": "def feature(text: str) -> float:\n    'Lexical diversity score based on unique tokens'\n    doc = nlp(text)\n    total_tokens = len(doc)\n    unique_tokens = len(set(token.text.lower() for token in doc if token.is_alpha))\n    return float(unique_tokens) / total_tokens if total_tokens > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00043417906981339127,
        "mean_shap": 4.05548992676325e-05,
        "std_shap": 0.0005742676531033494,
        "min_shap": -0.0020936281382228097,
        "max_shap": 0.0017710876702526992
      },
      "rank": 306
    },
    {
      "feature_index": 363,
      "feature_name": "feature_363",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentence length measured in characters'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s) for s in sentences if s.strip()]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00043132002088247013,
        "mean_shap": 4.2842037658877184e-05,
        "std_shap": 0.000650712104078823,
        "min_shap": -0.0036496996543418474,
        "max_shap": 0.0012435359724281701
      },
      "rank": 307
    },
    {
      "feature_index": 140,
      "feature_name": "feature_140",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    total_words = len(doc)\n    return float(verb_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00042742302330973356,
        "mean_shap": -8.069530350215469e-06,
        "std_shap": 0.0005483312827756118,
        "min_shap": -0.0022046611080830153,
        "max_shap": 0.0017025448983893233
      },
      "rank": 308
    },
    {
      "feature_index": 136,
      "feature_name": "feature_136",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique words (tokens) in the text\"\n    words = set(text.split())\n    return float(len(words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00042573833001000673,
        "mean_shap": 5.770913972996394e-05,
        "std_shap": 0.0006558389310069436,
        "min_shap": -0.002212389986443038,
        "max_shap": 0.0031185769771092846
      },
      "rank": 309
    },
    {
      "feature_index": 378,
      "feature_name": "feature_378",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words)) if words else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004246288904659667,
        "mean_shap": -3.290916376777951e-05,
        "std_shap": 0.0007568612090991635,
        "min_shap": -0.002723910957923778,
        "max_shap": 0.0031939228034533747
      },
      "rank": 310
    },
    {
      "feature_index": 139,
      "feature_name": "feature_139",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all sentences in the text\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    return float(sum(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00042328801635138625,
        "mean_shap": -2.086971049634929e-05,
        "std_shap": 0.0006380176287262253,
        "min_shap": -0.002061041724630659,
        "max_shap": 0.002510325977453713
      },
      "rank": 311
    },
    {
      "feature_index": 109,
      "feature_name": "feature_109",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    total_words = len(doc)\n    return float(verb_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004208146621840848,
        "mean_shap": -2.974117879217615e-06,
        "std_shap": 0.0005345496233519209,
        "min_shap": -0.001070191683996149,
        "max_shap": 0.002008486394550296
      },
      "rank": 312
    },
    {
      "feature_index": 341,
      "feature_name": "feature_341",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of unique words to total words in the text\"\n    words = text.split()\n    unique_word_count = len(set(words))\n    if len(words) == 0:\n        return 0.0\n    return float(unique_word_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004206568916764659,
        "mean_shap": -4.9619964572533805e-05,
        "std_shap": 0.0005202048689865565,
        "min_shap": -0.0016581991490076961,
        "max_shap": 0.001487354661505118
      },
      "rank": 313
    },
    {
      "feature_index": 209,
      "feature_name": "feature_209",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return stopword_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004141843843372487,
        "mean_shap": -5.661359174105472e-05,
        "std_shap": 0.0005564694084119689,
        "min_shap": -0.0013776983306196752,
        "max_shap": 0.0022985324860580387
      },
      "rank": 314
    },
    {
      "feature_index": 348,
      "feature_name": "feature_348",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentence length measured in characters'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0004072012870274761,
        "mean_shap": 6.068234683652053e-05,
        "std_shap": 0.0005670263953026023,
        "min_shap": -0.0025173002847343586,
        "max_shap": 0.0015755182461539485
      },
      "rank": 315
    },
    {
      "feature_index": 444,
      "feature_name": "feature_444",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of sentences that contain a question mark\"\n    question_sentences = len(re.findall(r'\\?+', text))\n    return float(question_sentences)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00040269000402923846,
        "mean_shap": -1.2763459330901681e-05,
        "std_shap": 0.0005892514038213839,
        "min_shap": -0.0019201645680893406,
        "max_shap": 0.0030632800133349215
      },
      "rank": 316
    },
    {
      "feature_index": 39,
      "feature_name": "feature_39",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of stop words in the text'\n    doc = nlp(text)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return stop_word_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003978557943020066,
        "mean_shap": -2.2000660372433145e-05,
        "std_shap": 0.0004908745995091452,
        "min_shap": -0.0012172745374056044,
        "max_shap": 0.0014905853145339132
      },
      "rank": 317
    },
    {
      "feature_index": 219,
      "feature_name": "feature_219",
      "feature_code": "def feature(text: str) -> float:\n    \"Type-Token Ratio (TTR) for lexical diversity\"\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    unique_tokens = len(set(token.text.lower() for token in doc if token.is_alpha))\n    return float(unique_tokens) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00039648934061323063,
        "mean_shap": -5.658050052197498e-05,
        "std_shap": 0.0004891970737540851,
        "min_shap": -0.0014388091454047055,
        "max_shap": 0.0015011422231033778
      },
      "rank": 318
    },
    {
      "feature_index": 197,
      "feature_name": "feature_197",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of stop words to total words in the text'\n    doc = nlp(text)\n    total_words = len(doc)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return (stop_word_count / total_words) if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003901347147519972,
        "mean_shap": 3.480958065570529e-05,
        "std_shap": 0.0005343846057980116,
        "min_shap": -0.0012617430586502134,
        "max_shap": 0.001762337791858018
      },
      "rank": 319
    },
    {
      "feature_index": 111,
      "feature_name": "feature_111",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of punctuation marks in the text\"\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return float(punctuation_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003900310391242063,
        "mean_shap": -0.00025652255260578266,
        "std_shap": 0.00046601748944246745,
        "min_shap": -0.0019610903942909068,
        "max_shap": 0.001422937131958314
      },
      "rank": 320
    },
    {
      "feature_index": 333,
      "feature_name": "feature_333",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stop words to total words in the text\"\n    doc = nlp(text)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return stop_word_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00038951027808741315,
        "mean_shap": -9.63289308264338e-06,
        "std_shap": 0.0005329314976641675,
        "min_shap": -0.001027383421886284,
        "max_shap": 0.0022952283196862443
      },
      "rank": 321
    },
    {
      "feature_index": 41,
      "feature_name": "feature_41",
      "feature_code": "def feature(text: str) -> float:\n    'Count of stop words as a proportion of total words'\n    doc = nlp(text)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return stop_word_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003884347848959223,
        "mean_shap": 2.2272577525075536e-05,
        "std_shap": 0.000579527251333885,
        "min_shap": -0.001010169197804842,
        "max_shap": 0.0029595871312928152
      },
      "rank": 322
    },
    {
      "feature_index": 462,
      "feature_name": "feature_462",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of stopwords to total words to assess common language'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003878740177603342,
        "mean_shap": 5.0609409129749234e-05,
        "std_shap": 0.0005506814984252958,
        "min_shap": -0.0010063394193196172,
        "max_shap": 0.0024076144641289226
      },
      "rank": 323
    },
    {
      "feature_index": 410,
      "feature_name": "feature_410",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of unique words to total words in the text'\n    words = text.split()\n    unique_words = len(set(words))\n    if len(words) == 0:\n        return 0.0\n    return float(unique_words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00037889884988160066,
        "mean_shap": 5.720333640815162e-05,
        "std_shap": 0.00048130080504493365,
        "min_shap": -0.0012839802936757175,
        "max_shap": 0.0018868358268537742
      },
      "rank": 324
    },
    {
      "feature_index": 392,
      "feature_name": "feature_392",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003779543705946373,
        "mean_shap": 6.374491648157214e-05,
        "std_shap": 0.000545790878088068,
        "min_shap": -0.002435764386498044,
        "max_shap": 0.0017582259706422525
      },
      "rank": 325
    },
    {
      "feature_index": 141,
      "feature_name": "feature_141",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity measured by type-token ratio\"\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    unique_tokens = len(set(token.text.lower() for token in doc if token.is_alpha))\n    return float(unique_tokens) / len(doc)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003742453364336006,
        "mean_shap": 2.7981814342720662e-05,
        "std_shap": 0.0005049624965223475,
        "min_shap": -0.0014053968835217219,
        "max_shap": 0.0016517767911118856
      },
      "rank": 326
    },
    {
      "feature_index": 256,
      "feature_name": "feature_256",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    total_words = len(doc)\n    return float(verb_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00037092444485735035,
        "mean_shap": -4.917324111357891e-05,
        "std_shap": 0.00048604146192247243,
        "min_shap": -0.002194202655649138,
        "max_shap": 0.0011069319071164787
      },
      "rank": 327
    },
    {
      "feature_index": 196,
      "feature_name": "feature_196",
      "feature_code": "def feature(text: str) -> float:\n    'Density of adjectives in the text compared to overall word count'\n    doc = nlp(text)\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    total_word_count = len(doc)\n    return float(adjective_count) / total_word_count if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003654474771448772,
        "mean_shap": -3.324914198448279e-05,
        "std_shap": 0.0005281185112218719,
        "min_shap": -0.00221206242674872,
        "max_shap": 0.001152203018054633
      },
      "rank": 328
    },
    {
      "feature_index": 465,
      "feature_name": "feature_465",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return stopword_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003652441364120464,
        "mean_shap": -5.091741304013223e-05,
        "std_shap": 0.00047893635468461096,
        "min_shap": -0.0009687911160442613,
        "max_shap": 0.002051559605955136
      },
      "rank": 329
    },
    {
      "feature_index": 383,
      "feature_name": "feature_383",
      "feature_code": "def feature(text: str) -> float:\n    'Punctuation count normalized by total characters'\n    punctuation_count = sum(1 for char in text if char in string.punctuation)\n    total_characters = len(text)\n    return float(punctuation_count) / total_characters if total_characters > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000359463915447242,
        "mean_shap": -1.583880911166372e-05,
        "std_shap": 0.0004835377262344032,
        "min_shap": -0.0019166100792115145,
        "max_shap": 0.0014085119283371887
      },
      "rank": 330
    },
    {
      "feature_index": 85,
      "feature_name": "feature_85",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of sentences in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003577431690970282,
        "mean_shap": 9.74926385148553e-05,
        "std_shap": 0.0005155879074140365,
        "min_shap": -0.002845026181570951,
        "max_shap": 0.0016676647679793606
      },
      "rank": 331
    },
    {
      "feature_index": 366,
      "feature_name": "feature_366",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of sentences in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]\n    return float(sum(sentence_lengths)) / len(sentence_lengths) if sentence_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00035540088544514225,
        "mean_shap": 8.749952426533057e-05,
        "std_shap": 0.0005267040863733195,
        "min_shap": -0.0025247161384393682,
        "max_shap": 0.0014230302153270566
      },
      "rank": 332
    },
    {
      "feature_index": 236,
      "feature_name": "feature_236",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of stopwords to total words\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    total_words = len(doc)\n    return float(stopword_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00035455924034247564,
        "mean_shap": 1.534853598299117e-05,
        "std_shap": 0.0005212486039478354,
        "min_shap": -0.001185792037711342,
        "max_shap": 0.0025189634966164996
      },
      "rank": 333
    },
    {
      "feature_index": 58,
      "feature_name": "feature_58",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique adjectives in the text\"\n    doc = nlp(text)\n    unique_adjectives = {token.text.lower() for token in doc if token.pos_ == 'ADJ'}\n    return float(len(unique_adjectives))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003514806943637597,
        "mean_shap": -7.459477830822166e-05,
        "std_shap": 0.0004184348769823899,
        "min_shap": -0.0011767905123948237,
        "max_shap": 0.000998233029184297
      },
      "rank": 334
    },
    {
      "feature_index": 252,
      "feature_name": "feature_252",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of characters per named entity'\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return float(statistics.mean(entity_lengths)) if entity_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000350160820082659,
        "mean_shap": -4.261996367041548e-05,
        "std_shap": 0.0004920855811258422,
        "min_shap": -0.0012275049608437244,
        "max_shap": 0.002505042397532559
      },
      "rank": 335
    },
    {
      "feature_index": 303,
      "feature_name": "feature_303",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuations to words in the text\"\n    punctuation_count = sum(1 for char in text if char in string.punctuation)\n    word_count = len(text.split())\n    return float(punctuation_count) / word_count if word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00034889089022935283,
        "mean_shap": -1.0378239749038738e-05,
        "std_shap": 0.0005528555138564534,
        "min_shap": -0.0027176970223640354,
        "max_shap": 0.0012031876126759639
      },
      "rank": 336
    },
    {
      "feature_index": 192,
      "feature_name": "feature_192",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of punctuation marks to total characters'\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return punctuation_count / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003425623952788517,
        "mean_shap": -7.420043676203187e-05,
        "std_shap": 0.0004992672974259625,
        "min_shap": -0.0026978506359962764,
        "max_shap": 0.0009625174798192541
      },
      "rank": 337
    },
    {
      "feature_index": 118,
      "feature_name": "feature_118",
      "feature_code": "def feature(text: str) -> float:\n    'Average proportion of adjectives in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003415634015585972,
        "mean_shap": -4.7953205168082515e-06,
        "std_shap": 0.0005317854611011156,
        "min_shap": -0.0028602737036522155,
        "max_shap": 0.0015183121771037348
      },
      "rank": 338
    },
    {
      "feature_index": 259,
      "feature_name": "feature_259",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of stopwords in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00033977752579515214,
        "mean_shap": -5.061318291525998e-06,
        "std_shap": 0.00044817321628013644,
        "min_shap": -0.0008825550019302696,
        "max_shap": 0.0016565203060585727
      },
      "rank": 339
    },
    {
      "feature_index": 455,
      "feature_name": "feature_455",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of long words (greater than 7 characters) in the text\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 7)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00033907081011093864,
        "mean_shap": 2.1949362609300678e-06,
        "std_shap": 0.0004445608139704062,
        "min_shap": -0.0012221666644337452,
        "max_shap": 0.0018859657701623016
      },
      "rank": 340
    },
    {
      "feature_index": 201,
      "feature_name": "feature_201",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of nouns to total words in the text\"\n    doc = nlp(text)\n    total_words = len(doc)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return noun_count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003387118052373287,
        "mean_shap": -2.8468294150788944e-05,
        "std_shap": 0.00042931551318959406,
        "min_shap": -0.001462306407127388,
        "max_shap": 0.0010247534929838297
      },
      "rank": 341
    },
    {
      "feature_index": 89,
      "feature_name": "feature_89",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of verbs to total words in the text'\n    doc = nlp(text)\n    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n    return verb_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003368120253236895,
        "mean_shap": 8.366744687161034e-06,
        "std_shap": 0.0004597852168139838,
        "min_shap": -0.0017577811093521033,
        "max_shap": 0.0015241575187030314
      },
      "rank": 342
    },
    {
      "feature_index": 343,
      "feature_name": "feature_343",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of punctuation marks to total words in the text'\n    words = text.split()\n    punctuation_count = sum(1 for char in text if char in string.punctuation)\n    return punctuation_count / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00033505352885972265,
        "mean_shap": -9.055050397278221e-05,
        "std_shap": 0.0005065852650336563,
        "min_shap": -0.003630713342878651,
        "max_shap": 0.0015858707872168226
      },
      "rank": 343
    },
    {
      "feature_index": 162,
      "feature_name": "feature_162",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of stopwords in the text'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return stopword_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00033449222372130416,
        "mean_shap": 2.2330241186012884e-05,
        "std_shap": 0.00046040932658185585,
        "min_shap": -0.000925126391354692,
        "max_shap": 0.0023050228743676593
      },
      "rank": 344
    },
    {
      "feature_index": 9,
      "feature_name": "feature_9",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation marks to total characters in the text\"\n    punctuation_count = sum(1 for char in text if char in string.punctuation)\n    return float(punctuation_count) / len(text) if len(text) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003308359417870765,
        "mean_shap": -9.363274556835773e-05,
        "std_shap": 0.0005150831100611425,
        "min_shap": -0.002782705202932715,
        "max_shap": 0.000681576094531956
      },
      "rank": 345
    },
    {
      "feature_index": 176,
      "feature_name": "feature_176",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of the named entities in the text\"\n    doc = nlp(text)\n    if not doc.ents:\n        return 0.0\n    average_length = sum(len(ent.text) for ent in doc.ents) / len(doc.ents)\n    return float(average_length)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003276675235779928,
        "mean_shap": -4.4148822239190436e-05,
        "std_shap": 0.0004523755385921833,
        "min_shap": -0.001108315320493973,
        "max_shap": 0.0018741930882311005
      },
      "rank": 346
    },
    {
      "feature_index": 23,
      "feature_name": "feature_23",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of nouns to total words\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00032575649158972364,
        "mean_shap": -5.570620225112954e-06,
        "std_shap": 0.0004226546196472926,
        "min_shap": -0.0013382209920848145,
        "max_shap": 0.0014950757335248004
      },
      "rank": 347
    },
    {
      "feature_index": 273,
      "feature_name": "feature_273",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation to word ratio in the text\"\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    word_count = len(text.split())\n    if word_count == 0:\n        return 0.0\n    return float(punctuation_count) / word_count\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003242407166479186,
        "mean_shap": 4.586861766431749e-05,
        "std_shap": 0.00044175779675531855,
        "min_shap": -0.001717017097086455,
        "max_shap": 0.0020116690136201707
      },
      "rank": 348
    },
    {
      "feature_index": 65,
      "feature_name": "feature_65",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of nouns to total number of tokens in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003234537238363077,
        "mean_shap": -2.487959473137124e-05,
        "std_shap": 0.0004234941612177914,
        "min_shap": -0.0011716437155198734,
        "max_shap": 0.0015569282494493591
      },
      "rank": 349
    },
    {
      "feature_index": 213,
      "feature_name": "feature_213",
      "feature_code": "def feature(text: str) -> float:\n    'Count of specific parts of speech ratio (nouns to total words)'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    total_words = len(doc)\n    return noun_count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003224064183897339,
        "mean_shap": 1.1748997580783355e-05,
        "std_shap": 0.0003948307751600874,
        "min_shap": -0.0012327251449538884,
        "max_shap": 0.0009518967020410998
      },
      "rank": 350
    },
    {
      "feature_index": 249,
      "feature_name": "feature_249",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of stopwords in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00032169259355096727,
        "mean_shap": -3.388860867611044e-06,
        "std_shap": 0.000434573450538632,
        "min_shap": -0.0011682503826305041,
        "max_shap": 0.0015014354009754728
      },
      "rank": 351
    },
    {
      "feature_index": 323,
      "feature_name": "feature_323",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003143606394878994,
        "mean_shap": -1.8339457386329662e-05,
        "std_shap": 0.0004395314717608879,
        "min_shap": -0.0010513534681027843,
        "max_shap": 0.002074161823133743
      },
      "rank": 352
    },
    {
      "feature_index": 388,
      "feature_name": "feature_388",
      "feature_code": "def feature(text: str) -> float:\n    'Readability score using Flesch-Kincaid Grade level'\n    import textstat\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003134908848013039,
        "mean_shap": -5.783049828131565e-05,
        "std_shap": 0.00040504465541880985,
        "min_shap": -0.0019677876579405696,
        "max_shap": 0.0009361189420076424
      },
      "rank": 353
    },
    {
      "feature_index": 347,
      "feature_name": "feature_347",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of nouns to total words'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003125641266737465,
        "mean_shap": -2.391462437245229e-05,
        "std_shap": 0.00041910374355414083,
        "min_shap": -0.0016754904564034682,
        "max_shap": 0.0011561908059118998
      },
      "rank": 354
    },
    {
      "feature_index": 337,
      "feature_name": "feature_337",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return stopword_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003098657022967237,
        "mean_shap": -3.2552233808427773e-06,
        "std_shap": 0.0004081080431287564,
        "min_shap": -0.0007902286870630184,
        "max_shap": 0.0017158673802079682
      },
      "rank": 355
    },
    {
      "feature_index": 100,
      "feature_name": "feature_100",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of named entities in the text'\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return float(sum(entity_lengths)) / len(entity_lengths) if entity_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00030923510284291196,
        "mean_shap": -2.6250430280809055e-05,
        "std_shap": 0.0004361599324984044,
        "min_shap": -0.0009513312381925759,
        "max_shap": 0.001715749502729144
      },
      "rank": 356
    },
    {
      "feature_index": 288,
      "feature_name": "feature_288",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of long words (greater than 7 characters) in the text'\n    long_word_count = sum(1 for word in text.split() if len(word) > 7)\n    total_words = len(text.split())\n    if total_words == 0:\n        return 0.0\n    return float(long_word_count) / total_words\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00030844008109164045,
        "mean_shap": -1.8896769043452408e-05,
        "std_shap": 0.0004085503446238323,
        "min_shap": -0.001653322261489487,
        "max_shap": 0.0015216494182619122
      },
      "rank": 357
    },
    {
      "feature_index": 283,
      "feature_name": "feature_283",
      "feature_code": "def feature(text: str) -> float:\n    'Total number of unique words in the text'\n    words = text.split()\n    unique_words = set(words)\n    return float(len(unique_words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003061694231915893,
        "mean_shap": 3.885367602638213e-05,
        "std_shap": 0.00041424373920305116,
        "min_shap": -0.0008025984355551831,
        "max_shap": 0.0012975726732289783
      },
      "rank": 358
    },
    {
      "feature_index": 198,
      "feature_name": "feature_198",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of punctuation marks to total characters'\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return punctuation_count / len(text) if text else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003049659775608811,
        "mean_shap": -3.120779724943805e-05,
        "std_shap": 0.00041981669431825246,
        "min_shap": -0.0016984256741401171,
        "max_shap": 0.0011795822806830944
      },
      "rank": 359
    },
    {
      "feature_index": 346,
      "feature_name": "feature_346",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0003031777196136611,
        "mean_shap": -0.0001037305723697567,
        "std_shap": 0.0003905962306009905,
        "min_shap": -0.002255282784356197,
        "max_shap": 0.0010853661196531506
      },
      "rank": 360
    },
    {
      "feature_index": 358,
      "feature_name": "feature_358",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique adjectives in the text\"\n    doc = nlp(text)\n    unique_adj = len(set(token.text.lower() for token in doc if token.pos_ == 'ADJ'))\n    return float(unique_adj)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00030212926666971414,
        "mean_shap": -8.490193485373675e-05,
        "std_shap": 0.00039091485627917443,
        "min_shap": -0.002069314375285255,
        "max_shap": 0.0010418340047765868
      },
      "rank": 361
    },
    {
      "feature_index": 390,
      "feature_name": "feature_390",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in characters\"\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return sum(entity_lengths) / len(entity_lengths) if entity_lengths else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00030188503682698607,
        "mean_shap": -5.599859983771353e-05,
        "std_shap": 0.0004033272150481005,
        "min_shap": -0.0008485606624788484,
        "max_shap": 0.0019240039492314973
      },
      "rank": 362
    },
    {
      "feature_index": 425,
      "feature_name": "feature_425",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of words per sentence in the text\"\n    sentences = re.split(r'[.!?]+', text)\n    word_counts = [len(s.split()) for s in sentences if s.strip()]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / len(word_counts)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00029905699864486864,
        "mean_shap": 1.095127677153108e-05,
        "std_shap": 0.0004045690608883175,
        "min_shap": -0.0018651239467326426,
        "max_shap": 0.0009980023279006624
      },
      "rank": 363
    },
    {
      "feature_index": 230,
      "feature_name": "feature_230",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of long words (more than 7 letters) to total words in the text'\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 7)\n    return float(long_word_count) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002986419573334378,
        "mean_shap": -5.5716738010609764e-05,
        "std_shap": 0.0004001106582859515,
        "min_shap": -0.0013780178566066704,
        "max_shap": 0.0011858587904399794
      },
      "rank": 364
    },
    {
      "feature_index": 129,
      "feature_name": "feature_129",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique parts of speech used'\n    doc = nlp(text)\n    unique_pos_tags = len(set(token.pos_ for token in doc if token.pos_))\n    return float(unique_pos_tags)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002985455196038717,
        "mean_shap": -1.4782580189023537e-05,
        "std_shap": 0.00039461259444174054,
        "min_shap": -0.0011300519892433682,
        "max_shap": 0.0017803765824816573
      },
      "rank": 365
    },
    {
      "feature_index": 445,
      "feature_name": "feature_445",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique tokens\"\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    unique_tokens = len(set(token.text.lower() for token in doc if token.is_alpha))\n    return float(unique_tokens) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002960267014771219,
        "mean_shap": 2.0613851603506332e-05,
        "std_shap": 0.00036551456563023335,
        "min_shap": -0.0010595788304061395,
        "max_shap": 0.0010021518803225234
      },
      "rank": 366
    },
    {
      "feature_index": 451,
      "feature_name": "feature_451",
      "feature_code": "def feature(text: str) -> float:\n    \"Density of stopwords in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002957389878901336,
        "mean_shap": 2.712324024683088e-05,
        "std_shap": 0.0003894386926406781,
        "min_shap": -0.0007275613714003265,
        "max_shap": 0.0013907344229853792
      },
      "rank": 367
    },
    {
      "feature_index": 161,
      "feature_name": "feature_161",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of long words (greater than 7 characters) in the text\"\n    words = text.split()\n    long_word_count = sum(1 for word in words if len(word) > 7)\n    if len(words) == 0:\n        return 0.0\n    return float(long_word_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00029565569835700886,
        "mean_shap": -2.7519556233642905e-05,
        "std_shap": 0.00037809034569287703,
        "min_shap": -0.0008355689889463551,
        "max_shap": 0.0012083320570430009
      },
      "rank": 368
    },
    {
      "feature_index": 400,
      "feature_name": "feature_400",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of nouns to total words in the text, indicating substantiveness\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    total_words = len(doc)\n    return noun_count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00029309565982983356,
        "mean_shap": 1.612474232387452e-05,
        "std_shap": 0.0003739821208506142,
        "min_shap": -0.0010118030204222593,
        "max_shap": 0.0011189809399345069
      },
      "rank": 369
    },
    {
      "feature_index": 43,
      "feature_name": "feature_43",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002916261542049902,
        "mean_shap": 4.625052429405108e-05,
        "std_shap": 0.0004396203637484526,
        "min_shap": -0.0009473541694592027,
        "max_shap": 0.0035823983832276017
      },
      "rank": 370
    },
    {
      "feature_index": 408,
      "feature_name": "feature_408",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of long words (greater than 7 characters) in the text'\n    words = text.split()\n    long_words = sum(1 for word in words if len(word) > 7)\n    return float(long_words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002913313918946849,
        "mean_shap": -6.31768293610987e-05,
        "std_shap": 0.0003685223632883877,
        "min_shap": -0.0010468613617357066,
        "max_shap": 0.0009568778185131184
      },
      "rank": 371
    },
    {
      "feature_index": 457,
      "feature_name": "feature_457",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in the text\"\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return float(sum(entity_lengths)) / len(entity_lengths) if entity_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002912611567252481,
        "mean_shap": -4.4925763381468716e-05,
        "std_shap": 0.0004113217161190721,
        "min_shap": -0.0009642638186135678,
        "max_shap": 0.0022056218449428393
      },
      "rank": 372
    },
    {
      "feature_index": 200,
      "feature_name": "feature_200",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives relative to total words to measure descriptive richness'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return adj_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00029122906598622224,
        "mean_shap": -1.3956201467664395e-05,
        "std_shap": 0.00041448961131727386,
        "min_shap": -0.0019430459115188847,
        "max_shap": 0.0013449154682653953
      },
      "rank": 373
    },
    {
      "feature_index": 381,
      "feature_name": "feature_381",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of nouns in the text'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    total_words = len(doc)\n    return float(noun_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028908496401499836,
        "mean_shap": 4.863750136869383e-05,
        "std_shap": 0.0003584573293024341,
        "min_shap": -0.0009261240313420354,
        "max_shap": 0.0013946639407513494
      },
      "rank": 374
    },
    {
      "feature_index": 88,
      "feature_name": "feature_88",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of stopwords to total words in the text\"\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    total_word_count = len(doc)\n    return stopword_count / total_word_count if total_word_count > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028855126321950317,
        "mean_shap": 2.368754492640057e-06,
        "std_shap": 0.000412647154137497,
        "min_shap": -0.0006298644449308746,
        "max_shap": 0.0017034075606396646
      },
      "rank": 375
    },
    {
      "feature_index": 223,
      "feature_name": "feature_223",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique adjectives in the text'\n    doc = nlp(text)\n    unique_adjectives = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'ADJ'))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028749838829181837,
        "mean_shap": 5.486233115019334e-06,
        "std_shap": 0.00041601485977410145,
        "min_shap": -0.0009633003254999724,
        "max_shap": 0.001966235375861466
      },
      "rank": 376
    },
    {
      "feature_index": 369,
      "feature_name": "feature_369",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of punctuation to total characters in the text\"\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    total_length = len(text)\n    if total_length == 0:\n        return 0.0\n    return float(punctuation_count) / total_length\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028403867681259107,
        "mean_shap": -5.945306029835267e-05,
        "std_shap": 0.0004363576992135028,
        "min_shap": -0.0026625486278490204,
        "max_shap": 0.0009665895291558641
      },
      "rank": 377
    },
    {
      "feature_index": 169,
      "feature_name": "feature_169",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentence length in words, emphasizing longer sentences'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002838588022801226,
        "mean_shap": -5.0164967167428525e-05,
        "std_shap": 0.0004221912909752695,
        "min_shap": -0.0018828069695844523,
        "max_shap": 0.0009340081952997257
      },
      "rank": 378
    },
    {
      "feature_index": 401,
      "feature_name": "feature_401",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in characters\"\n    doc = nlp(text)\n    if not doc.ents:\n        return 0.0\n    return float(sum(len(ent.text) for ent in doc.ents)) / len(doc.ents)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028156300986446957,
        "mean_shap": -5.054708344639599e-05,
        "std_shap": 0.0004028885719976463,
        "min_shap": -0.0010978669318303766,
        "max_shap": 0.001736161787545456
      },
      "rank": 379
    },
    {
      "feature_index": 315,
      "feature_name": "feature_315",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of stop words in the text\"\n    doc = nlp(text)\n    stop_word_count = sum(1 for token in doc if token.is_stop)\n    return float(stop_word_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00028087773963226176,
        "mean_shap": 3.1395789506574815e-05,
        "std_shap": 0.00037426007841802474,
        "min_shap": -0.0010439517760031025,
        "max_shap": 0.0017085026012840733
      },
      "rank": 380
    },
    {
      "feature_index": 243,
      "feature_name": "feature_243",
      "feature_code": "def feature(text: str) -> float:\n    \"Word density of adjectives in the text\"\n    doc = nlp(text)\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    total_words = len(doc)\n    return float(adjective_count) / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00027247595215864934,
        "mean_shap": -6.107637425063829e-05,
        "std_shap": 0.00041804724639438076,
        "min_shap": -0.0020845319755583834,
        "max_shap": 0.0011643010141130518
      },
      "rank": 381
    },
    {
      "feature_index": 122,
      "feature_name": "feature_122",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00027082810703843907,
        "mean_shap": 9.639450385023295e-05,
        "std_shap": 0.0003881107645304407,
        "min_shap": -0.0006011929846148569,
        "max_shap": 0.002725553852075769
      },
      "rank": 382
    },
    {
      "feature_index": 217,
      "feature_name": "feature_217",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of syllables per word in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_counts = [textstat.syllable_count(word) for word in words]\n    return float(sum(syllable_counts)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00027063143319973007,
        "mean_shap": -2.3460525794120698e-05,
        "std_shap": 0.00035433773414724796,
        "min_shap": -0.0013991627251917365,
        "max_shap": 0.0009464646953545061
      },
      "rank": 383
    },
    {
      "feature_index": 362,
      "feature_name": "feature_362",
      "feature_code": "def feature(text: str) -> float:\n    'Average proportion of adjectives in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return float(adj_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00027028945253693317,
        "mean_shap": -5.1932434875780735e-05,
        "std_shap": 0.0003679569068455647,
        "min_shap": -0.001414565596239423,
        "max_shap": 0.0010338655653483965
      },
      "rank": 384
    },
    {
      "feature_index": 182,
      "feature_name": "feature_182",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of long words (greater than 7 characters) in the text'\n    long_word_count = sum(1 for word in text.split() if len(word) > 7)\n    total_words = len(text.split())\n    if total_words == 0:\n        return 0.0\n    return float(long_word_count) / total_words\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002693819660403727,
        "mean_shap": -4.957336530368082e-05,
        "std_shap": 0.00034308669658563074,
        "min_shap": -0.0008417980686212096,
        "max_shap": 0.001440194521876068
      },
      "rank": 385
    },
    {
      "feature_index": 56,
      "feature_name": "feature_56",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if not lengths:\n        return 0.0\n    return sum(lengths) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00026861643531377543,
        "mean_shap": 2.5828093823283402e-05,
        "std_shap": 0.000359230225313793,
        "min_shap": -0.0013556540228532755,
        "max_shap": 0.0009710884916065504
      },
      "rank": 386
    },
    {
      "feature_index": 193,
      "feature_name": "feature_193",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length measured in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00026836669915028695,
        "mean_shap": 1.3847553585104868e-05,
        "std_shap": 0.00033917302817964824,
        "min_shap": -0.0010320407555138582,
        "max_shap": 0.0009665308388546775
      },
      "rank": 387
    },
    {
      "feature_index": 93,
      "feature_name": "feature_93",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique POS tags in the text'\n    doc = nlp(text)\n    pos_tags = {token.pos_ for token in doc}\n    return float(len(pos_tags))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002682388293148986,
        "mean_shap": -7.484146084926018e-06,
        "std_shap": 0.00036767926847782525,
        "min_shap": -0.0009735222944611747,
        "max_shap": 0.001838772969870063
      },
      "rank": 388
    },
    {
      "feature_index": 52,
      "feature_name": "feature_52",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in the text\"\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return float(statistics.mean(entity_lengths)) if entity_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002669362794989747,
        "mean_shap": -3.570268737839209e-05,
        "std_shap": 0.00036386531649359777,
        "min_shap": -0.0008685319404162505,
        "max_shap": 0.0015498314140213515
      },
      "rank": 389
    },
    {
      "feature_index": 32,
      "feature_name": "feature_32",
      "feature_code": "def feature(text: str) -> float:\n    'Average sentence length in words'\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    return float(statistics.mean(lengths)) if lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002661119796387476,
        "mean_shap": -2.6041030898548372e-05,
        "std_shap": 0.00040181086220067116,
        "min_shap": -0.002001487333708787,
        "max_shap": 0.0007437608154175659
      },
      "rank": 390
    },
    {
      "feature_index": 385,
      "feature_name": "feature_385",
      "feature_code": "def feature(text: str) -> float:\n    \"Proportion of nouns to total words in the text\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    total_words = len(doc)\n    return noun_count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00026334474873883546,
        "mean_shap": -3.395124951478021e-05,
        "std_shap": 0.0003642789555810516,
        "min_shap": -0.001790527879773375,
        "max_shap": 0.0016415053450695362
      },
      "rank": 391
    },
    {
      "feature_index": 276,
      "feature_name": "feature_276",
      "feature_code": "def feature(text: str) -> float:\n    'Count of stopwords in the text'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return float(stopword_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002628748965941921,
        "mean_shap": -0.00010047028763623141,
        "std_shap": 0.0003361524588366689,
        "min_shap": -0.001382123685246467,
        "max_shap": 0.0007932236907570841
      },
      "rank": 392
    },
    {
      "feature_index": 317,
      "feature_name": "feature_317",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of nouns to total words in the text'\n    doc = nlp(text)\n    total_words = len(doc)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return (float(noun_count) / total_words) if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002578517969276559,
        "mean_shap": -3.29336978228289e-05,
        "std_shap": 0.0003515090468657733,
        "min_shap": -0.0011579539765706375,
        "max_shap": 0.0012463683926242227
      },
      "rank": 393
    },
    {
      "feature_index": 467,
      "feature_name": "feature_467",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique parts of speech in the text'\n    doc = nlp(text)\n    unique_pos = set(token.pos_ for token in doc)\n    return float(len(unique_pos))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002558011841367056,
        "mean_shap": 2.9600108234035803e-05,
        "std_shap": 0.00034810836386352023,
        "min_shap": -0.000982659287984564,
        "max_shap": 0.00125232914733011
      },
      "rank": 394
    },
    {
      "feature_index": 117,
      "feature_name": "feature_117",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of nouns to total words\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002554223557343221,
        "mean_shap": -3.534445860455862e-05,
        "std_shap": 0.00034385486978759074,
        "min_shap": -0.0015604359336482026,
        "max_shap": 0.0009925104187753736
      },
      "rank": 395
    },
    {
      "feature_index": 163,
      "feature_name": "feature_163",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of adjectives to total word count in the text'\n    doc = nlp(text)\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    total_words = len(doc)\n    return (adjective_count / total_words) if total_words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002546682584512206,
        "mean_shap": -4.150153661774723e-05,
        "std_shap": 0.0003529410564328216,
        "min_shap": -0.0016510482824445016,
        "max_shap": 0.0011563916638701625
      },
      "rank": 396
    },
    {
      "feature_index": 450,
      "feature_name": "feature_450",
      "feature_code": "def feature(text: str) -> float:\n    'Average syllables per word in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_counts = sum(textstat.syllable_count(word) for word in words)\n    return syllable_counts / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.000254452825119502,
        "mean_shap": -6.305523851652845e-05,
        "std_shap": 0.0003365122709055573,
        "min_shap": -0.001605338002697636,
        "max_shap": 0.0007715899884522266
      },
      "rank": 397
    },
    {
      "feature_index": 429,
      "feature_name": "feature_429",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of adjectives to total words in the text'\n    doc = nlp(text)\n    adjective_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    total_words = len(doc)\n    return adjective_count / total_words if total_words > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00025237084230608667,
        "mean_shap": 4.734557869083177e-05,
        "std_shap": 0.0003403458088833285,
        "min_shap": -0.00140358106095908,
        "max_shap": 0.0013773861691488394
      },
      "rank": 398
    },
    {
      "feature_index": 195,
      "feature_name": "feature_195",
      "feature_code": "def feature(text: str) -> float:\n    \"Total number of unique words in the text\"\n    words = text.split()\n    unique_words = set(words)\n    return float(len(unique_words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024986336374968664,
        "mean_shap": 7.941043998261551e-05,
        "std_shap": 0.00033100924761824747,
        "min_shap": -0.0005170096864664606,
        "max_shap": 0.0014922252188755204
      },
      "rank": 399
    },
    {
      "feature_index": 448,
      "feature_name": "feature_448",
      "feature_code": "def feature(text: str) -> float:\n    'Count of adjectives relative to total words to measure descriptive richness'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n    return adj_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024978083972174007,
        "mean_shap": 5.995688578165842e-05,
        "std_shap": 0.00032796233508729716,
        "min_shap": -0.0010858691500939147,
        "max_shap": 0.0010960022952908792
      },
      "rank": 400
    },
    {
      "feature_index": 301,
      "feature_name": "feature_301",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of syllables per word using Textstat\"\n    syllable_count = textstat.syllable_count(text)\n    word_count = len(text.split())\n    return float(syllable_count) / word_count if word_count > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024880632180377104,
        "mean_shap": -9.509975177441123e-06,
        "std_shap": 0.0003587357779649726,
        "min_shap": -0.0011940126328551877,
        "max_shap": 0.0021694592361792475
      },
      "rank": 401
    },
    {
      "feature_index": 187,
      "feature_name": "feature_187",
      "feature_code": "def feature(text: str) -> float:\n    \"Ratio of nouns to total words\"\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024597474574513577,
        "mean_shap": -1.773352294938123e-05,
        "std_shap": 0.00031924816213673395,
        "min_shap": -0.0010857474686931283,
        "max_shap": 0.0012031685599537183
      },
      "rank": 402
    },
    {
      "feature_index": 188,
      "feature_name": "feature_188",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of sentences measured in characters\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.strip()) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024521813323469206,
        "mean_shap": 2.931591954338417e-05,
        "std_shap": 0.00033595815696144636,
        "min_shap": -0.0015876509778625352,
        "max_shap": 0.0009556056678677365
      },
      "rank": 403
    },
    {
      "feature_index": 242,
      "feature_name": "feature_242",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of named entities in the text'\n    doc = nlp(text)\n    named_entities = [len(ent.text) for ent in doc.ents]\n    if not named_entities:\n        return 0.0\n    return float(statistics.mean(named_entities))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024457336770172583,
        "mean_shap": -1.5517824727422457e-05,
        "std_shap": 0.0003700605957959634,
        "min_shap": -0.0006421444245980256,
        "max_shap": 0.0021673376875338355
      },
      "rank": 404
    },
    {
      "feature_index": 42,
      "feature_name": "feature_42",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024371443811385958,
        "mean_shap": 8.2257844853326e-05,
        "std_shap": 0.00035144271634806674,
        "min_shap": -0.0007965323775034746,
        "max_shap": 0.002673118514320284
      },
      "rank": 405
    },
    {
      "feature_index": 327,
      "feature_name": "feature_327",
      "feature_code": "def feature(text: str) -> float:\n    'Readability score using Flesch-Kincaid index'\n    import textstat\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024235737764790637,
        "mean_shap": -0.00010466965294911686,
        "std_shap": 0.0003194425204359401,
        "min_shap": -0.0013137335840976436,
        "max_shap": 0.0006727316773213564
      },
      "rank": 406
    },
    {
      "feature_index": 10,
      "feature_name": "feature_10",
      "feature_code": "def feature(text: str) -> float:\n    'Readability score using Flesch-Kincaid Grade level'\n    import textstat\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024184666103836932,
        "mean_shap": -8.590421416994325e-05,
        "std_shap": 0.0003194233038928621,
        "min_shap": -0.0019817823826624665,
        "max_shap": 0.0005470457482110298
      },
      "rank": 407
    },
    {
      "feature_index": 69,
      "feature_name": "feature_69",
      "feature_code": "def feature(text: str) -> float:\n    \"Average number of words per sentence in the text\"\n    sentences = re.split(r'[.!?]+', text)\n    word_counts = [len(s.split()) for s in sentences if s.split()]\n    return sum(word_counts) / len(word_counts) if word_counts else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00024080583464242692,
        "mean_shap": -3.62411062947627e-05,
        "std_shap": 0.00037560312091666103,
        "min_shap": -0.002034007568764922,
        "max_shap": 0.000681655975477582
      },
      "rank": 408
    },
    {
      "feature_index": 153,
      "feature_name": "feature_153",
      "feature_code": "def feature(text: str) -> float:\n    'Average syllables per word'\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_counts = [textstat.syllable_count(word) for word in words]\n    return float(sum(syllable_counts)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023854797149566433,
        "mean_shap": -6.872031365503366e-05,
        "std_shap": 0.00031805937131737744,
        "min_shap": -0.0012748049124161119,
        "max_shap": 0.0010417001320394905
      },
      "rank": 409
    },
    {
      "feature_index": 29,
      "feature_name": "feature_29",
      "feature_code": "def feature(text: str) -> float:\n    \"Average word length in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words) / len(words)) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023851439817030004,
        "mean_shap": -9.752168148875954e-05,
        "std_shap": 0.0003661301615009482,
        "min_shap": -0.002293887514063753,
        "max_shap": 0.0010485937748532827
      },
      "rank": 410
    },
    {
      "feature_index": 384,
      "feature_name": "feature_384",
      "feature_code": "def feature(text: str) -> float:\n    \"Total number of unique words in the text\"\n    words = text.split()\n    unique_words = set(words)\n    return float(len(unique_words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023756405989876995,
        "mean_shap": 2.7691944450977145e-05,
        "std_shap": 0.00036269081875004,
        "min_shap": -0.0018770179176556424,
        "max_shap": 0.0011890066678937154
      },
      "rank": 411
    },
    {
      "feature_index": 66,
      "feature_name": "feature_66",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of named entities in the text\"\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return float(statistics.mean(entity_lengths)) if entity_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002356133212986522,
        "mean_shap": 1.8962009810831725e-05,
        "std_shap": 0.0003273768036446409,
        "min_shap": -0.0010466711948688792,
        "max_shap": 0.001266702529427787
      },
      "rank": 412
    },
    {
      "feature_index": 412,
      "feature_name": "feature_412",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of exclamation marks in the text, indicating emphasis or strong sentiment\"\n    return float(text.count('!'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023496198169320253,
        "mean_shap": -0.00012379202895827074,
        "std_shap": 0.0003219383031014445,
        "min_shap": -0.0008156626574292524,
        "max_shap": 0.0029894361282302925
      },
      "rank": 413
    },
    {
      "feature_index": 202,
      "feature_name": "feature_202",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique words in the text'\n    unique_words = set(text.split())\n    return float(len(unique_words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023456879293848743,
        "mean_shap": 0.00010026554915179644,
        "std_shap": 0.0003412629805371555,
        "min_shap": -0.001956845244260665,
        "max_shap": 0.0014162021365839979
      },
      "rank": 414
    },
    {
      "feature_index": 222,
      "feature_name": "feature_222",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of the named entities in the text'\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    if not entity_lengths:\n        return 0.0\n    return float(sum(entity_lengths)) / len(entity_lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00023268463501629092,
        "mean_shap": -1.3926762449629178e-05,
        "std_shap": 0.0003237554488743053,
        "min_shap": -0.0006741613972873918,
        "max_shap": 0.0014802272165198127
      },
      "rank": 415
    },
    {
      "feature_index": 17,
      "feature_name": "feature_17",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022984044657421352,
        "mean_shap": -1.5751750420435463e-05,
        "std_shap": 0.00029710129755529076,
        "min_shap": -0.0007986637860744426,
        "max_shap": 0.0009228131127793475
      },
      "rank": 416
    },
    {
      "feature_index": 30,
      "feature_name": "feature_30",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of punctuation to total characters in the text'\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    total_characters = len(text)\n    if total_characters == 0:\n        return 0.0\n    return float(punctuation_count) / total_characters\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022920400928922225,
        "mean_shap": -7.83468704956021e-05,
        "std_shap": 0.0003155274220645443,
        "min_shap": -0.0013280269953161413,
        "max_shap": 0.0008839199228510184
      },
      "rank": 417
    },
    {
      "feature_index": 185,
      "feature_name": "feature_185",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of exclamation marks in the text, indicating emphasis or strong sentiment\"\n    return float(text.count('!'))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022905996187230182,
        "mean_shap": -0.0001340158895664311,
        "std_shap": 0.000375896763172732,
        "min_shap": -0.0006418238120342738,
        "max_shap": 0.0042607422362179815
      },
      "rank": 418
    },
    {
      "feature_index": 150,
      "feature_name": "feature_150",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability score based on the Flesch-Kincaid grade level\"\n    import textstat\n    return textstat.flesch_kincaid_grade(text)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002276609351558298,
        "mean_shap": -4.751995728220827e-05,
        "std_shap": 0.0003201129180382803,
        "min_shap": -0.002319443040493621,
        "max_shap": 0.0008799153986325587
      },
      "rank": 419
    },
    {
      "feature_index": 420,
      "feature_name": "feature_420",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022761042326737804,
        "mean_shap": 6.999769999742435e-06,
        "std_shap": 0.00032925015914382264,
        "min_shap": -0.0015131607400790234,
        "max_shap": 0.0010805161416467472
      },
      "rank": 420
    },
    {
      "feature_index": 121,
      "feature_name": "feature_121",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022570180645408092,
        "mean_shap": -5.355363729921778e-05,
        "std_shap": 0.0002917050614507099,
        "min_shap": -0.0009716996747177749,
        "max_shap": 0.0005784752543837689
      },
      "rank": 421
    },
    {
      "feature_index": 194,
      "feature_name": "feature_194",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002249768764975894,
        "mean_shap": 4.271473191732817e-05,
        "std_shap": 0.0003289841939122807,
        "min_shap": -0.0007924127471102072,
        "max_shap": 0.002246410437898011
      },
      "rank": 422
    },
    {
      "feature_index": 7,
      "feature_name": "feature_7",
      "feature_code": "def feature(text: str) -> float:\n    \"Punctuation density in the text\"\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    return punctuation_count / len(text) if text else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022389193757423417,
        "mean_shap": -5.972992073857119e-05,
        "std_shap": 0.00034424577928360094,
        "min_shap": -0.002144584212813371,
        "max_shap": 0.0009162370066420308
      },
      "rank": 423
    },
    {
      "feature_index": 175,
      "feature_name": "feature_175",
      "feature_code": "def feature(text: str) -> float:\n    \"Flesch-Kincaid grade level score of the text\"\n    import textstat\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00022360682735699127,
        "mean_shap": -7.643865505637099e-05,
        "std_shap": 0.0002959951468813146,
        "min_shap": -0.0015442018663560137,
        "max_shap": 0.0009431760457543574
      },
      "rank": 424
    },
    {
      "feature_index": 447,
      "feature_name": "feature_447",
      "feature_code": "def feature(text: str) -> float:\n    'Average length of named entities in the text.'\n    doc = nlp(text)\n    entity_lengths = [len(ent.text) for ent in doc.ents]\n    return float(sum(entity_lengths) / len(entity_lengths)) if entity_lengths else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002218066569287962,
        "mean_shap": -3.960227900455854e-05,
        "std_shap": 0.00031697778547874457,
        "min_shap": -0.0008883073847087868,
        "max_shap": 0.0017743448416879634
      },
      "rank": 425
    },
    {
      "feature_index": 34,
      "feature_name": "feature_34",
      "feature_code": "def feature(text: str) -> float:\n    \"Average word character length in the text\"\n    words = text.split()\n    return sum(len(word) for word in words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021892569749555488,
        "mean_shap": -3.762738156327055e-05,
        "std_shap": 0.0003035387786956086,
        "min_shap": -0.0016576485386553205,
        "max_shap": 0.0008413252517107194
      },
      "rank": 426
    },
    {
      "feature_index": 212,
      "feature_name": "feature_212",
      "feature_code": "def feature(text: str) -> float:\n    'Readability score based on average syllables per word'\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_count = sum(textstat.syllable_count(word) for word in words)\n    return float(syllable_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002173126012716565,
        "mean_shap": -9.0398622281878e-06,
        "std_shap": 0.0002981779012116745,
        "min_shap": -0.0011503103570607062,
        "max_shap": 0.0008406141895544605
      },
      "rank": 427
    },
    {
      "feature_index": 367,
      "feature_name": "feature_367",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of syllables per word in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_counts = [textstat.syllable_count(word) for word in words]\n    return float(sum(syllable_counts)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021652302384497752,
        "mean_shap": -5.314951167054856e-05,
        "std_shap": 0.0002870956612875046,
        "min_shap": -0.0009666912556384594,
        "max_shap": 0.0008645413991812972
      },
      "rank": 428
    },
    {
      "feature_index": 234,
      "feature_name": "feature_234",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique words in the text\"\n    unique_words = set(text.split())\n    return float(len(unique_words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021624184721463896,
        "mean_shap": 3.283316341250251e-05,
        "std_shap": 0.00035983072771513956,
        "min_shap": -0.0006954442071815988,
        "max_shap": 0.0019707507499320985
      },
      "rank": 429
    },
    {
      "feature_index": 131,
      "feature_name": "feature_131",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs in the text\"\n    doc = nlp(text)\n    unique_verbs = {token.lemma_.lower() for token in doc if token.pos_ == 'VERB'}\n    return float(len(unique_verbs))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021546349783590945,
        "mean_shap": 9.265160616131273e-05,
        "std_shap": 0.00027113819962560894,
        "min_shap": -0.0007124287673645541,
        "max_shap": 0.0010406997662240603
      },
      "rank": 430
    },
    {
      "feature_index": 356,
      "feature_name": "feature_356",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability score based on the Flesch-Kincaid grade level\"\n    import textstat\n    return textstat.flesch_kincaid_grade(text)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021368922132025827,
        "mean_shap": -6.414938470851019e-05,
        "std_shap": 0.0003051562403172288,
        "min_shap": -0.0018179561513663191,
        "max_shap": 0.0008946945793144045
      },
      "rank": 431
    },
    {
      "feature_index": 274,
      "feature_name": "feature_274",
      "feature_code": "def feature(text: str) -> float:\n    'Count of unique adjectives in the text'\n    doc = nlp(text)\n    unique_adjectives = len(set(token.text.lower() for token in doc if token.pos_ == 'ADJ'))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002108959020208301,
        "mean_shap": 4.5312601935493714e-05,
        "std_shap": 0.00032432752256919637,
        "min_shap": -0.0006996573785841317,
        "max_shap": 0.001922204776212623
      },
      "rank": 432
    },
    {
      "feature_index": 248,
      "feature_name": "feature_248",
      "feature_code": "def feature(text: str) -> float:\n    \"Flesch-Kincaid grade level score of the text\"\n    import textstat\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021071305062551004,
        "mean_shap": 8.219743417173295e-07,
        "std_shap": 0.00027902430509003354,
        "min_shap": -0.0010972708058902413,
        "max_shap": 0.0007293586256795007
      },
      "rank": 433
    },
    {
      "feature_index": 13,
      "feature_name": "feature_13",
      "feature_code": "def feature(text: str) -> float:\n    'Percentage of stopwords in the text'\n    doc = nlp(text)\n    stopword_count = sum(1 for token in doc if token.is_stop)\n    return stopword_count / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00021068729620355817,
        "mean_shap": -1.9470501935914783e-05,
        "std_shap": 0.0003010430069122291,
        "min_shap": -0.0008242268933053746,
        "max_shap": 0.0013754684178306496
      },
      "rank": 434
    },
    {
      "feature_index": 164,
      "feature_name": "feature_164",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of capitalized words in the text\"\n    words = text.split()\n    capitalized_count = sum(1 for word in words if word[0].isupper())\n    return float(capitalized_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020852412612844362,
        "mean_shap": 7.577361888322187e-05,
        "std_shap": 0.00028729321237335235,
        "min_shap": -0.0010493987027128926,
        "max_shap": 0.001462466569491146
      },
      "rank": 435
    },
    {
      "feature_index": 48,
      "feature_name": "feature_48",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of characters per word in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    char_count = sum(len(word) for word in words)\n    return float(char_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020850637292802823,
        "mean_shap": 2.7151673626990226e-05,
        "std_shap": 0.00029071082691234373,
        "min_shap": -0.0014465361303041871,
        "max_shap": 0.0008432104744242112
      },
      "rank": 436
    },
    {
      "feature_index": 272,
      "feature_name": "feature_272",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020579038041872942,
        "mean_shap": -1.1175086750742371e-05,
        "std_shap": 0.0002520948739567163,
        "min_shap": -0.0007701223502641325,
        "max_shap": 0.0007779398947316276
      },
      "rank": 437
    },
    {
      "feature_index": 59,
      "feature_name": "feature_59",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of nouns to total number of tokens in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020576548268267655,
        "mean_shap": -1.1160893829120768e-05,
        "std_shap": 0.000270147615229015,
        "min_shap": -0.0009439803005531002,
        "max_shap": 0.0009042388487657065
      },
      "rank": 438
    },
    {
      "feature_index": 70,
      "feature_name": "feature_70",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0002055446677237417,
        "mean_shap": -1.687644174455497e-05,
        "std_shap": 0.00028175753477192384,
        "min_shap": -0.0008865878772447196,
        "max_shap": 0.0010102496751234042
      },
      "rank": 439
    },
    {
      "feature_index": 266,
      "feature_name": "feature_266",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability score using Flesch-Kincaid Grade Level\"\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020499429326564694,
        "mean_shap": -3.179362498630263e-05,
        "std_shap": 0.0003195930254989514,
        "min_shap": -0.0018109137109034808,
        "max_shap": 0.0009764760457431343
      },
      "rank": 440
    },
    {
      "feature_index": 466,
      "feature_name": "feature_466",
      "feature_code": "def feature(text: str) -> float:\n    \"Average syllables per word in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_counts = [textstat.syllable_count(word) for word in words]\n    return float(sum(syllable_counts)) / len(words) if len(words) > 0 else 0.0\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020451506414486957,
        "mean_shap": -5.862856785706622e-05,
        "std_shap": 0.00027399952132021104,
        "min_shap": -0.001165974432591406,
        "max_shap": 0.0008330185636261784
      },
      "rank": 441
    },
    {
      "feature_index": 297,
      "feature_name": "feature_297",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of words in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    return sum(len(word) for word in words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00020262454957979123,
        "mean_shap": 2.2086923938019895e-05,
        "std_shap": 0.00029376692323792854,
        "min_shap": -0.0014705829863585972,
        "max_shap": 0.00090683744512804
      },
      "rank": 442
    },
    {
      "feature_index": 305,
      "feature_name": "feature_305",
      "feature_code": "def feature(text: str) -> float:\n    'Proportion of nouns to total words in the text'\n    doc = nlp(text)\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc) if len(doc) > 0 else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019897574013992583,
        "mean_shap": -5.7506302793893963e-05,
        "std_shap": 0.00027582972908298225,
        "min_shap": -0.0014423813315989926,
        "max_shap": 0.0006517583526821572
      },
      "rank": 443
    },
    {
      "feature_index": 255,
      "feature_name": "feature_255",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019792336574753623,
        "mean_shap": -2.8484705442632996e-05,
        "std_shap": 0.0002624154330430809,
        "min_shap": -0.0010112872259122447,
        "max_shap": 0.000673502046680458
      },
      "rank": 444
    },
    {
      "feature_index": 319,
      "feature_name": "feature_319",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of punctuation to total characters in the text'\n    punctuation_count = sum(1 for c in text if c in string.punctuation)\n    total_characters = len(text)\n    if total_characters == 0:\n        return 0.0\n    return float(punctuation_count) / total_characters\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001971621986302924,
        "mean_shap": -2.6282600911093315e-05,
        "std_shap": 0.00029461178009911515,
        "min_shap": -0.001264951783569776,
        "max_shap": 0.0015950254866844624
      },
      "rank": 445
    },
    {
      "feature_index": 280,
      "feature_name": "feature_280",
      "feature_code": "def feature(text: str) -> float:\n    'Ratio of nouns to total number of tokens in the text'\n    doc = nlp(text)\n    if len(doc) == 0:\n        return 0.0\n    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n    return float(noun_count) / len(doc)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019530540099444067,
        "mean_shap": -2.731522844095223e-05,
        "std_shap": 0.00025344181226477733,
        "min_shap": -0.0009976559547067562,
        "max_shap": 0.0007425662194377268
      },
      "rank": 446
    },
    {
      "feature_index": 99,
      "feature_name": "feature_99",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019451713565039915,
        "mean_shap": -4.323320447648864e-05,
        "std_shap": 0.00025138453353870557,
        "min_shap": -0.0009446081999101642,
        "max_shap": 0.0006557999030068901
      },
      "rank": 447
    },
    {
      "feature_index": 361,
      "feature_name": "feature_361",
      "feature_code": "def feature(text: str) -> float:\n    \"Average sentence length in words\"\n    sentences = re.split(r'[.!?]+', text)\n    lengths = [len(s.split()) for s in sentences if s.split()]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001917137624515503,
        "mean_shap": 2.5771541412688785e-05,
        "std_shap": 0.0002636101503415922,
        "min_shap": -0.0008636692725724524,
        "max_shap": 0.0010928781757508074
      },
      "rank": 448
    },
    {
      "feature_index": 318,
      "feature_name": "feature_318",
      "feature_code": "def feature(text: str) -> float:\n    \"Average word length in the text\"\n    words = text.split()\n    average_length = sum(len(word) for word in words) / len(words) if words else 0.0\n    return average_length\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00019137808368729385,
        "mean_shap": -2.0942761862927045e-05,
        "std_shap": 0.0002648247909006465,
        "min_shap": -0.0009172092741297728,
        "max_shap": 0.0008490262813829802
      },
      "rank": 449
    },
    {
      "feature_index": 461,
      "feature_name": "feature_461",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique adjectives in the text\"\n    doc = nlp(text)\n    adjective_count = len(set(token.text.lower() for token in doc if token.pos_ == 'ADJ'))\n    return float(adjective_count)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018478190230464736,
        "mean_shap": -7.751372144345913e-05,
        "std_shap": 0.00023308427773941726,
        "min_shap": -0.0007640817880682106,
        "max_shap": 0.0007253461596378555
      },
      "rank": 450
    },
    {
      "feature_index": 260,
      "feature_name": "feature_260",
      "feature_code": "def feature(text: str) -> float:\n    \"Readability score using Flesch-Kincaid Grade Level\"\n    import textstat\n    return textstat.flesch_kincaid_grade(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001845961804405981,
        "mean_shap": -7.931449449999571e-05,
        "std_shap": 0.0002604236633819038,
        "min_shap": -0.0015169708771370675,
        "max_shap": 0.0010995783130768745
      },
      "rank": 451
    },
    {
      "feature_index": 115,
      "feature_name": "feature_115",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of characters per word in the text'\n    words = text.split()\n    if not words:\n        return 0.0\n    return sum(len(word) for word in words) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018297717194359827,
        "mean_shap": 2.5836970108237178e-05,
        "std_shap": 0.00025434515310767455,
        "min_shap": -0.0010371247067153728,
        "max_shap": 0.0009249379986610988
      },
      "rank": 452
    },
    {
      "feature_index": 352,
      "feature_name": "feature_352",
      "feature_code": "def feature(text: str) -> float:\n    'Average syllable count per word'\n    total_syllables = sum(textstat.syllable_count(word) for word in text.split())\n    return total_syllables / len(text.split()) if text.split() else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018145678888436618,
        "mean_shap": 2.2060525553650785e-05,
        "std_shap": 0.0002530182740766781,
        "min_shap": -0.0011995005437612587,
        "max_shap": 0.00089006688328316
      },
      "rank": 453
    },
    {
      "feature_index": 240,
      "feature_name": "feature_240",
      "feature_code": "def feature(text: str) -> float:\n    'Readability index of the text using Flesch-Kincaid'\n    import textstat\n    return textstat.flesch_kincaid_grade(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018075848999936535,
        "mean_shap": -5.629707903214857e-05,
        "std_shap": 0.0002723794176801465,
        "min_shap": -0.0021750161809764327,
        "max_shap": 0.0009062918184866702
      },
      "rank": 454
    },
    {
      "feature_index": 326,
      "feature_name": "feature_326",
      "feature_code": "def feature(text: str) -> float:\n    \"Average length of words in the text\"\n    words = text.split()\n    if not words:\n        return 0.0\n    return float(sum(len(word) for word in words)) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00018001755850110613,
        "mean_shap": 9.370795957425794e-06,
        "std_shap": 0.00025130852232786347,
        "min_shap": -0.001138136631841936,
        "max_shap": 0.0006942520892382892
      },
      "rank": 455
    },
    {
      "feature_index": 82,
      "feature_name": "feature_82",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs in the text\"\n    doc = nlp(text)\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB'))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00017929505520774067,
        "mean_shap": -7.76758770767529e-06,
        "std_shap": 0.00028311306857701954,
        "min_shap": -0.0005211339229362383,
        "max_shap": 0.0013956939667356637
      },
      "rank": 456
    },
    {
      "feature_index": 403,
      "feature_name": "feature_403",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs used in the text\"\n    doc = nlp(text)\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB'))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00017739398264321444,
        "mean_shap": -9.548847718457579e-06,
        "std_shap": 0.00022877355356606353,
        "min_shap": -0.0007601288041190475,
        "max_shap": 0.0008369461648038196
      },
      "rank": 457
    },
    {
      "feature_index": 28,
      "feature_name": "feature_28",
      "feature_code": "def feature(text: str) -> float:\n    \"Average word character length in the text\"\n    words = text.split()\n    return sum(len(word) for word in words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00016065626984470587,
        "mean_shap": 1.1459965401297242e-05,
        "std_shap": 0.0002437428374879789,
        "min_shap": -0.0015885062857932208,
        "max_shap": 0.0008901397905870782
      },
      "rank": 458
    },
    {
      "feature_index": 97,
      "feature_name": "feature_97",
      "feature_code": "def feature(text: str) -> float:\n    \"Lexical diversity based on unique lemmas\"\n    doc = nlp(text)\n    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n    unique_lemmas = len(set(lemmas))\n    return float(unique_lemmas) / len(lemmas) if lemmas else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00015955806569131019,
        "mean_shap": -7.285669383811363e-05,
        "std_shap": 0.00019967310646252647,
        "min_shap": -0.0008735949136811098,
        "max_shap": 0.0005473528048276965
      },
      "rank": 459
    },
    {
      "feature_index": 277,
      "feature_name": "feature_277",
      "feature_code": "def feature(text: str) -> float:\n    \"Average word character length in the text\"\n    words = text.split()\n    return sum(len(word) for word in words) / len(words) if words else 0.0\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001562717724882417,
        "mean_shap": 1.5135038754020912e-05,
        "std_shap": 0.00023209995839134408,
        "min_shap": -0.0011429815044587317,
        "max_shap": 0.000944960389130864
      },
      "rank": 460
    },
    {
      "feature_index": 441,
      "feature_name": "feature_441",
      "feature_code": "def feature(text: str) -> float:\n    'Average number of syllables per word'\n    words = text.split()\n    if not words:\n        return 0.0\n    syllable_count = sum(textstat.syllable_count(word) for word in words)\n    return float(syllable_count) / len(words)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00015459608854169192,
        "mean_shap": 4.616153614082227e-05,
        "std_shap": 0.00021035538574445655,
        "min_shap": -0.000716684719420106,
        "max_shap": 0.0008435898108576407
      },
      "rank": 461
    },
    {
      "feature_index": 427,
      "feature_name": "feature_427",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique adjectives in the text\"\n    doc = nlp(text)\n    unique_adjectives = len(set(token.text.lower() for token in doc if token.pos_ == 'ADJ'))\n    return float(unique_adjectives)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00015388013313895538,
        "mean_shap": -1.5398692791289655e-05,
        "std_shap": 0.00023978135517209117,
        "min_shap": -0.001212947454979377,
        "max_shap": 0.0010613509275569917
      },
      "rank": 462
    },
    {
      "feature_index": 120,
      "feature_name": "feature_120",
      "feature_code": "def feature(text: str) -> float:\n    \"Flesch-Kincaid grade level score of the text\"\n    import textstat\n    return float(textstat.flesch_kincaid_grade(text))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00014770970290824774,
        "mean_shap": -3.469088059956749e-05,
        "std_shap": 0.00020677820501238063,
        "min_shap": -0.0010057401587058507,
        "max_shap": 0.0006704386922173275
      },
      "rank": 463
    },
    {
      "feature_index": 289,
      "feature_name": "feature_289",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs used in the text\"\n    doc = nlp(text)\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB'))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00014093308657117177,
        "mean_shap": 3.768967996878267e-05,
        "std_shap": 0.00022146002978690264,
        "min_shap": -0.0005825042527282391,
        "max_shap": 0.0013888869887081696
      },
      "rank": 464
    },
    {
      "feature_index": 335,
      "feature_name": "feature_335",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of unique verbs used in the text\"\n    doc = nlp(text)\n    unique_verbs = len(set(token.lemma_.lower() for token in doc if token.pos_ == 'VERB'))\n    return float(unique_verbs)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001403528861597895,
        "mean_shap": 1.925258689994251e-05,
        "std_shap": 0.00019302266369552087,
        "min_shap": -0.00045896643826725267,
        "max_shap": 0.000833991163101228
      },
      "rank": 465
    },
    {
      "feature_index": 170,
      "feature_name": "feature_170",
      "feature_code": "def feature(text: str) -> float:\n    'Count of specific keywords relevant to the domain (e.g., movie, food, dance)'\n    keywords = {'dance', 'movie', 'food', 'wedding', 'supply'}\n    count = sum(1 for word in text.split() if word.lower() in keywords)\n    return float(count)\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001324165097461469,
        "mean_shap": 4.546273772018698e-05,
        "std_shap": 0.00024673091182570204,
        "min_shap": -0.0017775935206395328,
        "max_shap": 0.00033671723187072384
      },
      "rank": 466
    },
    {
      "feature_index": 376,
      "feature_name": "feature_376",
      "feature_code": "def feature(text: str) -> float:\n    \"Sum of lengths of all words in the text\"\n    words = text.split()\n    return float(sum(len(word) for word in words))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0001265601592423508,
        "mean_shap": 1.1310172024785326e-05,
        "std_shap": 0.00020157155041531594,
        "min_shap": -0.001270124098740456,
        "max_shap": 0.000734313803897346
      },
      "rank": 467
    },
    {
      "feature_index": 190,
      "feature_name": "feature_190",
      "feature_code": "def feature(text: str) -> float:\n    'Readability index of the text using Flesch-Kincaid'\n    import textstat\n    return textstat.flesch_kincaid_grade(text)\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.00011322231894966321,
        "mean_shap": -2.0565329223222818e-05,
        "std_shap": 0.00016268696914999224,
        "min_shap": -0.0007992295571650607,
        "max_shap": 0.000715909065668242
      },
      "rank": 468
    },
    {
      "feature_index": 221,
      "feature_name": "feature_221",
      "feature_code": "def feature(text: str) -> float:\n    \"Count of thematic keywords (related to specific topics) in the text\"\n    keywords = {'invest', 'garden', 'crypto', 'mountain', 'house', 'squat'}\n    return float(sum(1 for word in text.split() if word.lower() in keywords))\n\n",
      "shap_statistics": {
        "mean_abs_shap": 0.0,
        "mean_shap": 0.0,
        "std_shap": 0.0,
        "min_shap": 0.0,
        "max_shap": 0.0
      },
      "rank": 469
    }
  ],
  "shap_metadata": {
    "explainer_type": "TreeExplainer",
    "num_features": 469
  }
}